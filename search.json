[
  {
    "objectID": "before_server.html",
    "href": "before_server.html",
    "title": "Connect to the server",
    "section": "",
    "text": "For this course, we will be working, for the most part, on a remote server.\nHere, you will access the server for the first time. You will need a terminal, your user name and password (which you will have gotten from Sam).",
    "crumbs": [
      "Home",
      "Before the course",
      "Connect to the server"
    ]
  },
  {
    "objectID": "before_server.html#remote-access-via-bash-over-ssh-in-the-consoleterminal",
    "href": "before_server.html#remote-access-via-bash-over-ssh-in-the-consoleterminal",
    "title": "Connect to the server",
    "section": "Remote access via bash over ssh in the console/terminal",
    "text": "Remote access via bash over ssh in the console/terminal\nBefore embarking on this short exercise, check out Chapter 4, “Working with Remote Machines” of the “Bioinformatics Data Skills” course book.\nStart with opening a terminal on your PC. This application is natively available on Linux and MacOs, while on Windows you will open the Ubuntu program you installed in the previous step. Now it is time to use the ssh command to connect to the server. Run the following command substituting “xxxxxxxx” with your account username, and “server-name” with the name of the server Sam has given you (if you haven’t gotten this information, please contact Sam, or me, via canvas):\nssh xxxxxxxx@server-name.se\n\nthe first time you run this command, the terminal will ask you if the address is trusted, type yes and enter;\nthen enter your srver password; BE CAREFUL, you have few attempts, after which your account will be blocked for a while!\n\nSuccessfully completing this login procedure will connect your terminal to the “login node” of the cluster (you will be greeted by some system information and a prophetic cow).\nThis location is the home folder of your account. Now you can practice your bash skills to explore the structure of the directories and available files.",
    "crumbs": [
      "Home",
      "Before the course",
      "Connect to the server"
    ]
  },
  {
    "objectID": "before_quarto_and_git.html",
    "href": "before_quarto_and_git.html",
    "title": "Quarto Homepage on Github",
    "section": "",
    "text": "Creating a website in Quarto\nPublish the quarto homepage on GitHub via GitHub Actions.",
    "crumbs": [
      "Home",
      "Before the course",
      "Quarto Homepage on Github"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This course is a hosted by the Swedish Agricultural University’s Bioinformatics Infrastructure (SLUBI).\nIn this course we hope to give you information on how to use reproducible bioinformatics pipelines, report results in a streamlined manner, and implement the system in your own research.\nFeedback is appreciated!\n(This is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites.)"
  },
  {
    "objectID": "before_quarto.html",
    "href": "before_quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "When analysing data, it is good practice to work as reproducibleas possible. Part of that is not only developing and executing code (for example in a R session), but to save and comment the code. As always, this documentation step is crucial to understand the code, especially when sharing code with others, or getting back to it at a later point in time.\nLiterate programming combines code and documentation in the same document: the documentation is in plain text, and the code is wrapped in so called chunks, that are executable from within the document.\nThese notebooks that allow for literate programming come in different flavors, for example jupyter notebooksand marimo for Python applications, Rmarkdown for R code. Its successor, quarto can be used to integrate a variety of coding languanges. In this course, we will introduce you to quarto.",
    "crumbs": [
      "Home",
      "Before the course",
      "Quarto"
    ]
  },
  {
    "objectID": "before_quarto.html#literate-programming",
    "href": "before_quarto.html#literate-programming",
    "title": "Quarto",
    "section": "",
    "text": "When analysing data, it is good practice to work as reproducibleas possible. Part of that is not only developing and executing code (for example in a R session), but to save and comment the code. As always, this documentation step is crucial to understand the code, especially when sharing code with others, or getting back to it at a later point in time.\nLiterate programming combines code and documentation in the same document: the documentation is in plain text, and the code is wrapped in so called chunks, that are executable from within the document.\nThese notebooks that allow for literate programming come in different flavors, for example jupyter notebooksand marimo for Python applications, Rmarkdown for R code. Its successor, quarto can be used to integrate a variety of coding languanges. In this course, we will introduce you to quarto.",
    "crumbs": [
      "Home",
      "Before the course",
      "Quarto"
    ]
  },
  {
    "objectID": "before_quarto.html#quarto",
    "href": "before_quarto.html#quarto",
    "title": "Quarto",
    "section": "Quarto",
    "text": "Quarto\n\nAn open-source scientific and technical publishing system.\n\nMeaning that you can use your favorite text editor (such as VScode (o; ) to write documents in plain text markdown.\nQuarto chunks, or code cells, are executable from within the document and can be written in a variety of different languages - such as python, R, Julia, bash, Observable, and more.\nYour document can then be rendered into a variety of different output formats: html, pdf, MS Word, Markdown, and more.\nWith this you can make presentations, dashboards, homepages (like this one), reports, books, manuscripts, and more.\n\n\n\n\n\n\nTo do for you\n\n\n\nInstall Quarto, and follow the tutorial for VScode.\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has a very good documentation, that you can access on their homepage -&gt; Guide, or by searching the web for “Quarto, feature you are looking for”.",
    "crumbs": [
      "Home",
      "Before the course",
      "Quarto"
    ]
  },
  {
    "objectID": "course_environments.html",
    "href": "course_environments.html",
    "title": "Introduction to Pixi",
    "section": "",
    "text": "Different operating systems bring different challenges to bioinformatics. Windows, for instance, doesn’t really support most bioinformatics tools, or your computer might run a different version of a specific tool than mine. One way to solve this, and make research more reproducible, is through the use of environments. Bioinformatics environments specify the tools neded for the task at hand, and environment managers install these tools with all their dependencies. There are many different kinds of environment managers, and for this course we are going to use Pixi.\nThere are, of course, other ways to solve tool access and compatibility issues, such as running virtual machines. In our experience, however, environments are a bit easier to manage and are more portable across different systems and users.\n\n\n\nPixi landing page\n\n\nAs you can see, you can run Pixi on all major operating systems, and you can include various platforms in your Pixi environment.\n\n\n\n\n\n\nTo do for you\n\n\n\nLog in to the course server and follow along:\n\n\n\n\nInstalling Pixi is really easy and described thoroughly here with separate installation guides for Windows and Mac/Linux.\n\n\n\n\n\n\nTip\n\n\n\nTo source your shell, you need to source the startup files, in Linux it’s the ~/.bashrc file, in Mac it’s the ~/.zshrc file."
  },
  {
    "objectID": "course_environments.html#introduction",
    "href": "course_environments.html#introduction",
    "title": "Introduction to Pixi",
    "section": "",
    "text": "Different operating systems bring different challenges to bioinformatics. Windows, for instance, doesn’t really support most bioinformatics tools, or your computer might run a different version of a specific tool than mine. One way to solve this, and make research more reproducible, is through the use of environments. Bioinformatics environments specify the tools neded for the task at hand, and environment managers install these tools with all their dependencies. There are many different kinds of environment managers, and for this course we are going to use Pixi.\nThere are, of course, other ways to solve tool access and compatibility issues, such as running virtual machines. In our experience, however, environments are a bit easier to manage and are more portable across different systems and users.\n\n\n\nPixi landing page\n\n\nAs you can see, you can run Pixi on all major operating systems, and you can include various platforms in your Pixi environment.\n\n\n\n\n\n\nTo do for you\n\n\n\nLog in to the course server and follow along:\n\n\n\n\nInstalling Pixi is really easy and described thoroughly here with separate installation guides for Windows and Mac/Linux.\n\n\n\n\n\n\nTip\n\n\n\nTo source your shell, you need to source the startup files, in Linux it’s the ~/.bashrc file, in Mac it’s the ~/.zshrc file."
  },
  {
    "objectID": "course_environments.html#setting-up-an-environment",
    "href": "course_environments.html#setting-up-an-environment",
    "title": "Introduction to Pixi",
    "section": "Setting Up An Environment",
    "text": "Setting Up An Environment\nYou should create separate environments for each project you run, just to keep things tidy. To create an environment, you have to specify a name for your environment. You can include different platforms/operating systems in your environment, for example if you want to develop your code on a Windows system, and later use the same code on many samples on a large Linux cluster. You can also include different vetted sources for your tools, the so called channels.\nHere, we will create a project called name_pixi_training (please use your own name to avoid creating multiple environments with the same name). We are adding the conda-forge and bioconda channels with the -c flag.\npixi init name_pixi_training -c conda-forge -c bioconda\nPixi will create a folder named name_pixi_training with a a file pixi.toml. Let’s have a look at that file!\n\n\n\n\n\n\nHow to get to the file\n\n\n\n\n\nHere is the code for changing directories, listing files, and viewing the contents of a file:\ncd name_sida_training\nls\nless pixi.toml\nTo exit the less view, press q for quit.\n\n\n\n\npixi.toml\nThe .toml file give your information about your project. Let’s have a look at one I made one my computer before adding any dependencies to my environment. How is it different from the one you’ve made on HPC2N?\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"amrei_pixi_training\"\nplatforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\n \n\n\n\n\n\n\nNote\n\n\n\nYour current platform will be automatically detected and added to the environment. If you want to add different platforms, you add them with the -p linux-64 flag. In this example, you are adding Linux64. See the Pixi docs for the full list of supported platforms.\nIf you are adding a platform that doesn’t natively run on your OS (e.g. adding Linux when running on Windows) be sure to add the OS you are running your system on as well!\n\n\nOnce you have used the environment, or added a dependency/tool to it, you will find yet another file, called pixi.lock.\n\n\npixi.lock\nThe .lock file give you information on the channels you have decided to add, as well as the information on where the packages were downloaded from, license information, md5 information, and more.\n\n\n\n\n\n\nImportant\n\n\n\nDo not delete the .toml or .lock files, or you will break your environment!"
  },
  {
    "objectID": "course_environments.html#adding-dependencies",
    "href": "course_environments.html#adding-dependencies",
    "title": "Introduction to Pixi",
    "section": "Adding Dependencies",
    "text": "Adding Dependencies\nAdding dependencies to the .toml file is telling Pixi to install the specified program for you. However, instead of installing it globally, it only gets installed in the environment.\nTo do this, we use the pixi add function. Let’s try adding Quarto to our environments.\n\n\n\n\n\n\nImportant\n\n\n\nYou must be in the folder of the project to add software!\n\n\npixi add quarto\n\n\n\n\n\n\nHow to summon the help function\n\n\n\n\n\nIf you are unsure of how a function works, you can always query it, usually with the --help or -h flags. Here’s how it would look for the pixi add function\npixi add --help\nA general rule of thumb is that a single hyphen - is followed by a single letter flag, while double hyphens -- are usually followed by multi-letter flags\n\n\n\nHere is the pixi.toml we’ve seen earlier after I have added Quarto to my environment. You can see that the dependencies have been updated to include Quarto.\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"amrei_pixi_training\"\nplatforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\nquarto = \"&gt;=1.7.32,&lt;2\""
  },
  {
    "objectID": "course_environments.html#running-a-package",
    "href": "course_environments.html#running-a-package",
    "title": "Introduction to Pixi",
    "section": "Running a Package",
    "text": "Running a Package\nNow that we have an environment with a tool installed, we actually want to use it. For this we use the pixi run function.\nLet’s query the help function within Quarto:\npixi run quarto --help\n\n\n\n\n\n\nNote\n\n\n\nTo use other tools, simply substitute quarto with the package you’ve added, and the tool specific commands.\n\n\nAnd just to see that we have not installed Quarto on the server itself, try out running Quarto without Pixi: it will tell you:\nquarto: command not found"
  },
  {
    "objectID": "before_git_and_github.html#version-control",
    "href": "before_git_and_github.html#version-control",
    "title": "git and GitHub",
    "section": "Version control",
    "text": "Version control\nWe all have worked on data before, done analyses, talked with our PI, changed the analyses, worked a bit more… and in the end we have something like this:\n\n\n\nWhich one of these is the latest version?\n\n\nVersion control, the practice of tracking and managing changes to files, can help us not descend into chaos. With a version controlled project you always know which file, and even which part of the file, is the most recent, and you can even go back to older versions if you have to.\nVersion control can be used on the local system, where both the version database and the checked out file - the one that is actively being worked on - are on the local computer. Good, but the local computer can be corrupted and then the data is compromised.\n\n\n\nLocal version control\n\n\nVersion control can also be centralized, where the version database is on a central server, and the active file can be checked out from several different computers. This is useful when working from different systems, or when working with collaborators. However, when the central servers is compromised the historical version are lost.\n\n\n\nCentralized version control\n\n\nAt last, version control can be fully distributed, with all versions of the file being on the server and different computers. Each computer checks out the file from its own version database to work on them. The databases are then synchronized between the different computers and the server. One such distributed version control system is git. It can handle everything from small to very large projects and is simple to use. GitHubis a code hosting platform for version control and collaboration, built on git.\n\n\n\nDistributed version control\n\n\nDistributed version control facilitates collaboration with others. Software like git automatically tracks differences in files, and flags conflicts between files.\nAdditionally, GitHub, the code hosting platform based on git that we are using in this course, can be used to maintain uniformity within a working group. The group can develop their own project template that people can use and populate for their own projects.",
    "crumbs": [
      "Home",
      "Before the course",
      "git and GitHub"
    ]
  },
  {
    "objectID": "before_git_and_github.html#git-and-github",
    "href": "before_git_and_github.html#git-and-github",
    "title": "git and GitHub",
    "section": "git and GitHub",
    "text": "git and GitHub\nGit is a version control software that is fully distributed - meaning that each project folder contains the full history of the project. These project folders are also called repositories and can be on several computers, or servers.\nGithub is a code hosting platform that is based on git. Here you can store, track and publish code (and code only, do NOT use github for data!). On Github you can collaborate with colleagues and work on projects together.\n\n\n\n\n\n\nNote\n\n\n\nA repository in git is the .git/ folder inside of your directory. This repository tracks all changes made to files in your project and contains your project history. Usually we refer to the git repository as the local repository.\nA repository in GitHub is where you can store your code, your files, together with their revision history. Repositories can be public or private, and might have several collaborators. Usually we refer to the Github repository as the remote repository.\n\n\nLet’s have a closer look at how git works:",
    "crumbs": [
      "Home",
      "Before the course",
      "git and GitHub"
    ]
  },
  {
    "objectID": "before_git_and_github.html#git",
    "href": "before_git_and_github.html#git",
    "title": "git and GitHub",
    "section": "git",
    "text": "git\n\nGit has three main states that your files can reside in: modified, staged, and committed:\n\n\n\nModified means that you have changed the file but have not committed it to your database yet.\nStaged means that you have marked a modified file in its current version to go into your next commit snapshot.\nCommitted means that the data is safely stored in your local database.\n\n\n\nsource: git documentation\n\nThis leads to the three main sections of a Git project: the working directory, the staging area, and the Git directory (or repository).\n\n\n\nWorking directory, staging area, and Git directory\n\n\nAnd the basic commands of git:\n\n\n\nBasic git commands\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese basic operations are all done on your local system. You have the entire history of the project on your local disk, and do not need an internet connection to work on your data with git. You can do all your commits on your local computer and later push them to a remote repostitory, like Github.\n\n\n\n\n\n\n\n\nTo do for you\n\n\n\nInstall git, and follow the\nMake an account on GitHub.",
    "crumbs": [
      "Home",
      "Before the course",
      "git and GitHub"
    ]
  },
  {
    "objectID": "before_VScode.html",
    "href": "before_VScode.html",
    "title": "Visual Studio Code",
    "section": "",
    "text": "During the pre-course assignements, and the course itself, we will be using Visual Studio Code (VScode). It is available for free and runs on all major platforms. Extensions make it very versatile, and we will be using it to edit and render text, to us version control, and to connect to and work on our course server.\n\n\n\n\n\n\nTo do for you\n\n\n\nInstall VScode using the instructions here.\n\n\nOnce you have installed VScode and open it, you will see the starting screen. Here is my starting screen: \nYours will look slightly different, especially in the activity bar all the way on the left, because I have already installed extensions when working with VScode. So don’t worry about the exact look.\n\n\n\n\n\n\nTo do for you\n\n\n\nTo get aquainted with some of the features of VScode, please watch this 7-minute video from the Visual Studio homepage. Follow along the tutorial (as far as you can, he’s not providing the last script he’s showcasing).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you can’t see the terminal on the lower part of the screen, you can open it by clicking on the button on the upper right of the screen.\n\n\n\nClick here to open the terminal",
    "crumbs": [
      "Home",
      "Before the course",
      "Visual Studio Code"
    ]
  },
  {
    "objectID": "course_data_management.html#data-life-cycle",
    "href": "course_data_management.html#data-life-cycle",
    "title": "Data Management for Reproducible Research",
    "section": "Data Life Cycle",
    "text": "Data Life Cycle\nWhen working with any type of data, it makes sense to sit down before the project starts to think through the different life stages of the data in your project. This will help counteract some of the problems that can arise when projects grow more organically, and will help consistency within the research group, ease collaboration, and mostly your future self that will understand what past-self has been up to in the project.\n\n\n\n\n\n\nNote\n\n\n\nMore and more funding agencies expect a Data Management Plan at some point of a project application. In there, you need to document that you have thought of, and planned for, the life cycle of your data.\n\n\n\n\n\nThe Research Data Management toolkit for Life Sciences"
  },
  {
    "objectID": "course_data_management.html#fair-principles",
    "href": "course_data_management.html#fair-principles",
    "title": "Data Management for Reproducible Research",
    "section": "FAIR principles",
    "text": "FAIR principles\nIn the past, research data was often generated with one question in mind. Often, they would afterwards land in some drawer and be forgoten about. Nowadays researchers acknowledge that data can also be re-used, or combined with other data, to answer different questions.\nThe FAIR principles promote efficient data discovery and reuse by providing guidelines to make digital resources:\n\n\n\nWilkinson et al. (2016)\n\n\nFAIR principles, in turn, rely on good data management practices in all phases of research:\n\nResearch documentation\nData organisation\nInformation security\nEthics and legislation"
  },
  {
    "objectID": "course_data_management.html#reproducible-research",
    "href": "course_data_management.html#reproducible-research",
    "title": "Data Management for Reproducible Research",
    "section": "Reproducible research",
    "text": "Reproducible research\nLucky for us, once we implement good data management practices, we will also increase the reproducibility of our analyses. Extensive documentation will increase faith in the outcome of analyses, and will help people (again, future-you) understand what has been done.\nLast, but not least, reproducible research practices make project hand-overs smoother, when the next person already understands the structure of the project, and can rely on good documentation."
  },
  {
    "objectID": "course_data_management.html#what-data-do-we-work-with",
    "href": "course_data_management.html#what-data-do-we-work-with",
    "title": "Data Management for Reproducible Research",
    "section": "What data do we work with?",
    "text": "What data do we work with?\n\nBioinformatics is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex. (Wikipedia)\n\nThis data can come from a variety of different biological processes:\n\n\n\nsource: Lizel Potgieter\n\n\nEarly on, sequencing data was not readily available, but due to decreasing costs and increased computational power biological data is now being produced in ever increasing quantities:\n\n\n\nGrowth of the Sequence Read Archive, SRA, from 2012 to 2021\n\n\nAt the same time, new technologies are being developed, and new tools that might or might not be maintained or benchmarked against existing tools. It’s the wild west out there!\n\n\n\nOverview of modern sequencing technologies and where they apply to biological processes"
  },
  {
    "objectID": "course_data_management.html#working-with-data",
    "href": "course_data_management.html#working-with-data",
    "title": "Data Management for Reproducible Research",
    "section": "Working with data",
    "text": "Working with data\nOften, with a new project, one sits down with the data, tries out things and see if they worked. A lot of bioinformatics is not being afraid to try things, and reading the documentation.\nThis traditional way of working with bioinformatics data can have merits and lead to new discoveries. However, in this course we would like to introduce you to a more structured way to make sense of your data.\nLet’s have a look at how a beginning PhD student might approach their data:\n\nThey might analyse their data, and get some results.\nAfter talking with their supervisor they might get a few other samples from a collaborator, or need to drop them from the analyses due to quality concerns.\nThey run the analyses again and get a different set of results.\nThere might be a few iterations of this process, and then the reviewers require some additional analyses…\n\nIn the “end” we have something like this:\n\n\n\nWhich one of these is the latest version?\n\n\n\n\n\n\n\n\nBest practices file organization\n\n\n\n\nThere is a folder for the raw data, which does not get altered.\nCode is kept separate from data.\nUse a version control system (at least for code) – e.g. git.\nThere should be a README in every directory, describing the purpose of the directory and its contents.\nUse file naming schemes that makes it easy to find files and understand what they are (for humans and machines) and document them.\nUse non-proprietary formats – .csv rather than .xlsx"
  },
  {
    "objectID": "course_data_management.html#literate-programming",
    "href": "course_data_management.html#literate-programming",
    "title": "Data Management for Reproducible Research",
    "section": "Literate programming",
    "text": "Literate programming\nOur hypothetical PhD student, even if taking into account the best practice tips from above, is still likely to run the same analyses over and over whenever the input data changes. Sometimes, this might be months, or even years, after the original analysis was performed.\nLuckily for our student, they can save their code snippets (with intuitive file names) and re-use the code from back then. This is often done with R-scripts, but can just as well be applied to bash scripts, python scripts etc.\nIn the past years, the development went even further and one can even combine code and documentation in the same document. The code is wrapped in so called chunks, or code cells, that are executable from within the document.\nBefore the course you have already worked with one such notebook - Quarto."
  },
  {
    "objectID": "course_data_management.html#quarto",
    "href": "course_data_management.html#quarto",
    "title": "Data Management for Reproducible Research",
    "section": "Quarto",
    "text": "Quarto\nNow we will set up a quarto homepage."
  },
  {
    "objectID": "course_data_management.html#version-control",
    "href": "course_data_management.html#version-control",
    "title": "Data Management for Reproducible Research",
    "section": "Version control",
    "text": "Version control\nNow that our student has reproducible documents, with reasonable names, that can execute their analyses reliably over and over again, what happens if they modify their analyses? Will they end up again with different result files and their project sink down in chaos?\nNo, because there is version control, the practice of tracking and managing changes to files."
  },
  {
    "objectID": "course_data_management.html#git-and-github",
    "href": "course_data_management.html#git-and-github",
    "title": "Data Management for Reproducible Research",
    "section": "Git and Github",
    "text": "Git and Github"
  },
  {
    "objectID": "course_data_management.html#environment-managers",
    "href": "course_data_management.html#environment-managers",
    "title": "Data Management for Reproducible Research",
    "section": "Environment managers",
    "text": "Environment managers\nUsing git, our PhD student can now share their reproducible code with their colloaborators, or between systems. They can rest assured that the different versions of the notebook are tracked and can be checked out when necessary. But what about the bioinformatic tools?\nDifferent computers can run on different operating systems, or can have different versions of databases installed. This can lead to conflicts between tools, or software versions and can impact code usability, or reproducibility.\nFortunately, smart people have developed environment managers such as conda, bioconda, or pixi. These tools find and install packages, so that the same package versions are being run between different computers. However, the code might still give different results on different operating systems."
  },
  {
    "objectID": "course_data_management.html#hands-on-managing-environments-with-pixi",
    "href": "course_data_management.html#hands-on-managing-environments-with-pixi",
    "title": "Data Management for Reproducible Research",
    "section": "Hands-on: Managing environments with Pixi",
    "text": "Hands-on: Managing environments with Pixi"
  },
  {
    "objectID": "course_data_management.html#containers-in-bioinformatics",
    "href": "course_data_management.html#containers-in-bioinformatics",
    "title": "Data Management for Reproducible Research",
    "section": "Containers in bioinformatics",
    "text": "Containers in bioinformatics\nBut what if our PhD student needs to run their code on different operating systems?\nThey can use containers, that contain everything needed to run the application, even the operating system. Containers are being exchanged as container images, which makes them lightweight. Containers do not change over time, so the results will be the same today and in a few years. Everyone gets the same container that works in the same way."
  },
  {
    "objectID": "course_data_management.html#hands-on-containers",
    "href": "course_data_management.html#hands-on-containers",
    "title": "Data Management for Reproducible Research",
    "section": "Hands-on: containers",
    "text": "Hands-on: containers"
  },
  {
    "objectID": "course_data_management.html#workflow-manager---nextflow",
    "href": "course_data_management.html#workflow-manager---nextflow",
    "title": "Data Management for Reproducible Research",
    "section": "Workflow manager - Nextflow",
    "text": "Workflow manager - Nextflow\nNow our PhD student can use containers, or environments, to provide a uniform environment for their version controlled, wonderfully documented and reproducible code. Fantastic! But they still have to deploy, or at least monitor, their scripts manually.\nFortunately there are workflow managers that can integrate all of the above, submit your jobs for you, and even monitor and re-submit scripts after failure. They will automatically submit jobs for you, decreasing downtime and increasing efficiency.\n\n\n\n\n\n\nTip\n\n\n\nHumans tend to do mistakes, especially when it comes to tedious or repetitive tasks. If you automate data handling, formatting etc. you are less likely to make mistakes like typos, or changing colors in images."
  },
  {
    "objectID": "course_data_management.html#hands-on-nextflow",
    "href": "course_data_management.html#hands-on-nextflow",
    "title": "Data Management for Reproducible Research",
    "section": "Hands-on: Nextflow",
    "text": "Hands-on: Nextflow"
  },
  {
    "objectID": "before_command_line.html",
    "href": "before_command_line.html",
    "title": "Command line",
    "section": "",
    "text": "Unix-like operating systems are built under the model of free and open-source development and distribution. They often come with a graphical user interface (GUI) and can be run from the command line (CLI) or terminal. The CLI is a text-based interface that works exactly the same way as you would use your mouse, but you use words. It can be intimidating at first, but once you have mastered the basics, it’s really not different than using your mouse!\nIt is important to know how to use the terminal as all servers, and most bioinformatics tools do not have a GUI and rely on the use of the terminal.\nFor this course, we will use a GUI for some parts, but all our interaction with the remote server will be on the command line. As such, it’s important that you know your way around the command line.\n\n\n\n\n\n\nTo do for you\n\n\n\nI know that you have varying proficiency with the command line.\nPlease follow this excellent software carpentries tutorial on the Unix shell."
  },
  {
    "objectID": "course_AI.html",
    "href": "course_AI.html",
    "title": "Caution: AI in Bioinformatics",
    "section": "",
    "text": "Caution\n\n\n\nWhile the lure of AI is becoming more present in our daily lives, remember that you do not need it. Before AI, you were perfectly able to design a packing list for your upcoming trip. You were able to look at a paper to find answers to your scientific questions. You knew how to query a vignette in R to determine how a function should be used.\nLife was a bit slower, but you used your mind and your agency. You made decisions. Please do not confuse convenience with need!"
  },
  {
    "objectID": "course_AI.html#introduction",
    "href": "course_AI.html#introduction",
    "title": "Caution: AI in Bioinformatics",
    "section": "Introduction",
    "text": "Introduction\nIn the past 3 years, AI has become more mainstream. The primary kind of AI people think of, is a Large Language Model (LLM). At this point, you cannot really avoid contact with these models anymore as they have infiltrated every aspect of the internet.\n\n\n\nSearching LLM with Google to be answered with an LLM response\n\n\nThese algorithms are language models trained on incredibly large datasets. They are trained to recognise and generate natural language. The chatbots we are all familiar with are generative pretrained transformers (GPTs). These can be trained for specific tasks, or guided by prompt generation.\nThis is not an AI theory course, and we really do not have enough knowledge to explain the underlying theory beyond a rudimentary level. We would, however, like to discuss the impact of AI on you as a user, and advocate for responsible use of AI in your current and future work."
  },
  {
    "objectID": "course_AI.html#machine-learning-in-bioinformatics",
    "href": "course_AI.html#machine-learning-in-bioinformatics",
    "title": "Caution: AI in Bioinformatics",
    "section": "Machine Learning in Bioinformatics",
    "text": "Machine Learning in Bioinformatics\nMachine learning (ML) has been used in bioinformatics and other fields of data science for many decades on every level. Before ML, algorithms had to be programmed by hand rather than having the algorithms learn features of a dataset. With ML, features of a dataset can be annotated based on previously annotated datasets. These algorithms were a mix of supervised (learning on annotated data) and unsupervised (learning on unannotated data) learning, depending on the function of the algorithm.\nSupervised algorithms are used for classification and regression analyses. Unsupervised algorithms are used to discover hidden patterns in data without needing a human’s input. Unsupervised algorithms are used in clustering, association, and dimensionality reduction\n\n\n\n\n\n\nNote\n\n\n\n\n\nClassification: Output is a discrete variable. Linear classifiers, support vector machines, decision trees, random forests. E.g. annotating a new genome based on genome annotations from existing species.\nRegression: Focus on understanding dependent and indepedent variables.\n\n\n\nFor more info, see here, and here.\nThere is no arguing that these algorithms have led to great progress within the field of bioinformatics. Generative AI is one of the next steps in the evolution of applying machine learning in our lives."
  },
  {
    "objectID": "course_AI.html#incorporation-of-ai-in-our-lives",
    "href": "course_AI.html#incorporation-of-ai-in-our-lives",
    "title": "Caution: AI in Bioinformatics",
    "section": "Incorporation of AI in Our Lives",
    "text": "Incorporation of AI in Our Lives\nChatGPT gained 100 million users in the 2 months after its release in 2022, making it the fastest-growing consumer application in history. Generative AI (GenAI) models now come in many different flavours, depending on the developer.\n\n\n\nGenAI chatbots by market share in August 2025 from FirstPageSage\n\n\n\nTraining Data\nAs with earlier iterations of supervised and unsupervised algorithms, GenAI models have all been trained on existing data. And this existing data can be biased: in a historical context, history was recorded by the party that won the war. History changed as different empires and narratives rose and fell. With digitisation, this information has landed on the internet. In the more modern “Internet Age”, everyone with an internet connection can technically post anything they’d like on the internet. This can add different types of biases - not everyone has equal access to the internet, some people may not have strong enough opinions to post about something online, some people prefer to read rather than contribute, while others take pleasure in “shit-posting”.\nAs the GenAI training data contains large amounts of data from the internet, these differences in how people use the internet have an effect on how useful the trained models become. In this example, you can clearly see what the effect of bad training data is:\n\n\n\nGenAI answer to a simple question\n\n\nFurthermore, GenAI’s are not programmed to say that they do not know something, and will happily hallucinate an answer. If you do not know better, or trust the computers, you may take a made-up answer as true and post it elsewhere. As the use of AI’s increases, AI generated content is used to train new AI’s. Gary Illyes from Google has spoken about “human curated” vs “human created” data being used as training data.\nIn a perfect world, GenAI would be trained on perfectly curated data, but even with all of the data that we have on the internet at the moment, we do not have nearly enough data. We have to make do with what we have."
  },
  {
    "objectID": "course_AI.html#some-words-of-caution",
    "href": "course_AI.html#some-words-of-caution",
    "title": "Caution: AI in Bioinformatics",
    "section": "Some Words of Caution",
    "text": "Some Words of Caution\nGenAI is becoming more integrated in every sector of our lives. It is important that we use the new technology responsibly. When Google first came out, there were classes on how to use the search engine, determine validity of sources and information, and how to stay safe on the internet. This section aims to raise awareness about commonly overlooked aspects of GPT use.\n\nLearning with AI\nAI has great potential in the field of education. ChatGPT has been shown to be highly beneficial in an educational environment when integrated properly. However, the use of AI in this setting must be balanced and carefully curated. A 2025 pre-print by Kosmyna et al showed that adults that used ChatGPT to write SAT type essays were outperformed consistently by adults that wrote the same essay without the support of an AI, and had significantly lower brain engagement.\n\n\nProductivity with AI\nA recent study by a non-profit group, Model Evaluation and Threat Research (METR) aimed to quantify the difference in productivity when using AI. Participants in this study were not new to their field, with at least 5 years of experience prior to this study being conducted.\n\n\n\nAI reducing productivity\n\n\nThe study also found that when AI is allowed, the participants spent less time coding and seeking solutions to the problems. Rather, they spent time prompting the AI, reading and reviewing responses, and being idle. Intel produced similar findings.\n\n\n\nReasons for loss of productivity with AI\n\n\n\n\nData Privacy and Legal Concerns\nData privacy concerns have been present throuhout the development and use of AI.\nThe use of copyrighted material in training AI, and its legality, is being discussed and debated in several courts. The questions around using text and data mining are divisive with some parties arguing that finding patterns, trends, and insights in existing data being how new research is done by humans and should be extended to AI, while others, particularly in the European context, disagree to some extent. Understanding the legality of the service you use in different countries falls on you as a user.\nThe Terms of Service (ToS) of different GPTs are important when deciding whether to use a GPT at all. For example, Deepseek’s ToS (collected on 15.08.2025) states:\nAccount Personal Data. We collect Personal Data that you provide when you set up an account, such as your date of birth (where applicable), username (where applicable), email address and/or telephone number, and password.\n\nUser Input. When you use our Services, we may collect your text input, prompt, uploaded files, feedback, chat history, or other content that you provide to our model and Services (“Prompts” or \"Inputs\"). We generate responses (“Outputs”) based on your Inputs.\n\nPersonal Data When You Contact Us. When you contact us, we collect the Personal Data you send us, such as proof of identity or age, contact details, feedback or inquiries about your use of the Services or Personal Data about possible violations of our Terms of Service (our “Terms”) or other policies.\nBiological data enjoys a high level of protection, and is often considered as highly sensitive. Too often, users will input sequences, tables, or other data into a GPT to find ways to plot data, perform sequence annotations, or similar tasks. Be wary of doing this. Check the ToS explicitly, and frequently. Try to use GDPR compliant GPT’s if you must use a GPT. Try using only a description of your data - such as column names and type of data - instead of the actual data set.\nAI as browser extensions steal deeply personal information when enabled in a users’ browser including financial, education, and medical information, whether an extension is being actively used or not. The authors have also commented on how these practices interfere with the company’s own ToS as well as privacy legislation.\n\n\nPersonal Responsibility\nYou as a user are responsible for the AI generated content you choose to use. If you, for instance, ask AI to generate a brand logo for you that is too close to something that exists already, the original owner is free to sue you as an individual for copyright infringement.\nAs scientists, we know that we should use peer-reviewed resources whenever possible. It is why we cannot cite Wikipedia in a scientific article. GPT’s have been widely shown to fabricate citations based on how it has learnt a citation should look. It is up to you as a user to check every single citation that AI generates since you are responsible for what you write.\n\n\nGlobal Linguistic Changes\nEven though ChatGPT has only been widely used for 3 years, it has already started leaving its traces in how we speak. Words like delve and meticulous are being used more frequently in academic YouTube talks.\n\n\n\nGPT words in YouTube videos from Yakura et al 2025\n\n\nIt has also been shown that different GPT’s have different writing styles, also known as idiolects.\nSome projects like this one are trying to customize GPT ideolects to match writing styles of unique users. This will make detecting AI use more difficult in future.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis course was written by two people, and each person wrote their own sections without the use of any AI. Can you tell who wrote which sections based on idiolects?\n\n\n\nWe know that subtle linguistic shifts can change emotional regulation within individuals. We also know from sociolinguistics, that even the slightest linguistic features can serve to bind or divide us.\n\n\nAI on the Internet\nSocial media platforms are an important aspect in the development, improvement, and implementation of GenAI models. OpenAI, the creators of ChatGPT, have used a subreddit on the social media platform, Reddit, to train their new algorithm. Google and OpenAI have contractual agreements with Reddit to license data from the users on the platform. Earlier in 2025, researchers from the University of Zurich were implicated in an experiment on the same subreddit OpenAI used to train a model. They wanted to test whether an interaction with a bot was more likely to make people change their minds than an interaction with a real person. This was heavily frowned upon, and posts were all removed as users had no ability to consent to participating in a study. It has also been suggested that interactions observed by the researchers were just bots arguing with each other.\nA preprint released in February 2025 by Liang et al. found that the amount of content generated by AI rose from 2-3% in November 2022 to 24% by the end of 2023.\n\n\n\nAI slop trough by Yahoo! News!\n\n\n\n\nEnvironmental Impact\nThe facilities to run GenAI require a significant amount of resources. These facilities need a huge amount of electricity to power the facility (this places extreme strain on exising infrastructure and increases the grid’s carbon footprint) as well as water to cool the hardware. Currently, data centers use more electricity than many independent countries.\nSome data centers are being built near poor communities, drain resources from, and add pollution to the community (see Colossus that has been built in Memphis to power the X bot, Grok for an example. Musk is not the only offender.)\nWhile we cannot do anything about where data centers are built, we can make informed decisions about which platforms we use. We can also be careful with the number of queries we send, and how we use our queries. In April 2025, the CEO of OpenAI said that polite requests like “please” and “thank you” have cost tens of millions of dollars due to the cost of electricity."
  },
  {
    "objectID": "course_AI.html#how-to-decide-when-to-use-ai",
    "href": "course_AI.html#how-to-decide-when-to-use-ai",
    "title": "Caution: AI in Bioinformatics",
    "section": "How To Decide When To Use AI",
    "text": "How To Decide When To Use AI\nIf we know the risks and the true cost of what we are doing, we can make informed decisions about how we chose to incorporate new technologies into our day-to-day and working lives.\nHere are some questions that we find useful to ask ourselves before opening a GPT:\n\nAm I phrasing my prompt in a good way? Here is a guide to prompt engineering that might be useful.\nCan I find this information any other way?\nHow much time am I saving by looking this question up here vs on BioStars, for example?\nDo I know enough about the topic to know whether the GPT is lying to me?\nWhat are the consequences of testing the validity of the GPT solution? Can I potentially corrupt my data or my system? Is there a potential for me to lie to someone who trusts me enough to ask my opinion?\n\nIf a GPT is used to learn a new skill, remember how important active learning is. Seek explanations for everything the GPT tells you. Find independent sources that were produced by experts to validate your learning.\nHold on to the ability to learn and the desire to be curious."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Bioinformatics",
    "section": "",
    "text": "The aim of this course is to share how to use bioinformatics tools in a reproducible and scalable way. We will use environments, containers, and established pipelines so that you can run these analyses on any operating system, as well as on systems that are not high performance computing clusters. And first and foremost: these tools and techniques can be used regardless of which type of bioinformatics you are ultimately working with.\nThe website will remain active after the course so that you have access to the material even after the course.\nWe will meet in person in Ultuna between Monday, October 6th, and Friday, October 10th, 2025. However, to get the most out of this course, we expect you to do some preparation in advance. This is to set-up and get aquainted with some of the tools we will be using.\nI want to thank Lizel Potgieter for her valuable contributions to the materials for this course!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Applied Bioinformatics",
    "section": "Course content",
    "text": "Course content\n\nbefore the course:\n\nBefore the course\n\n\nTopic\nContent\n\n\n\n\nServer access\nSSH connect to the course server\n\n\nCommand line\nUse the command line to manipulate data\n\n\nVScode\nSettting up and getting aquainted with VScode\n\n\nQuarto\nInstalling and using Quarto with VScode\n\n\ngit and GitHub\nVersion control with git and GitHub\n\n\nData Management\nBest practices when working with bioinformatics data\n\n\n\n\n\nduring the course:\n\nProgram\n\n\nDay\nSession\n\n\n\n\nMonday\nWelcome, introduction\n\n\n\nDiscussion: Data Management\n\n\n\nHands-on: Markdown, quarto homepage\n\n\n\nHands-on: Publish a homepage with Github\n\n\n\nIntroduction to environments\n\n\nTuesday\nQC of sequencing data\n\n\n\nHands-on: fastqc & multiqc with pixi\n\n\n\nIntroduction to containers\n\n\n\nHands-on: containers\n\n\n\nnextflow\n\n\n\nquality control with nextflow\n\n\nWednesday\ncontainers cont.\n\n\n\nnf-core\n\n\n\nHands-on: Test a nf-core pipeline\n\n\n\nHands-on: set up a nf-core pipeline\n\n\nThursday\n\n\n\n\n\n\n\n\n\n\n\nFriday\nVisualize your results: ggplot\n\n\n\nDiscussion: AI in Bioinformatics\n\n\n\nclean-up and finishing",
    "crumbs": [
      "Home"
    ]
  }
]
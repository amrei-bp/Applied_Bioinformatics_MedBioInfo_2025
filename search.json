[
  {
    "objectID": "before_VScode.html",
    "href": "before_VScode.html",
    "title": "Visual Studio Code",
    "section": "",
    "text": "During the pre-course assignements, and the course itself, we will be using Visual Studio Code (VScode). It is available for free and runs on all major platforms. Extensions make it very versatile, and we will be using it to edit and render text, to us version control, and to connect to and work on our course server.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nInstall VScode using the instructions here.\n\n\nOnce you have installed VScode and open it, you will see the starting screen. Here is my starting screen: \nYours will look slightly different, especially in the activity bar all the way on the left, because I have already installed extensions when working with VScode. So don’t worry about the exact look.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nTo get aquainted with some of the features of VScode, please watch this 7-minute video from the Visual Studio homepage. Follow along the tutorial (as far as you can, he’s not providing the last script he’s showcasing).\nRead up more on the graphical user interface of VScode here.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you can’t see the terminal on the lower part of the screen, you can open it by clicking on the button on the upper right of the screen.\n\n\n\nClick here to open the terminal",
    "crumbs": [
      "Home",
      "Before the course",
      "Visual Studio Code"
    ]
  },
  {
    "objectID": "course_publish_blog.html",
    "href": "course_publish_blog.html",
    "title": "Publish your blog",
    "section": "",
    "text": "To publish your blog we need to first start tracking it with git:"
  },
  {
    "objectID": "course_publish_blog.html#tracking-your-blog-with-git",
    "href": "course_publish_blog.html#tracking-your-blog-with-git",
    "title": "Publish your blog",
    "section": "Tracking your blog with git",
    "text": "Tracking your blog with git\nIn the terminal, make sure you are in the directory of your blog, then type:\ngit init\nThis will initialize your blog directory as a git repository.\n\nInitialized empty Git repository in path/to/directory\n\nNow, if you check with\ngit status\nyou will see that the repository is indeed empty. You will also see some untracked files.\n\n\n\nTerminal after checking the git status\n\n\nAdd all the files to the repository and commit the changes to git."
  },
  {
    "objectID": "course_publish_blog.html#add-the-blog-to-github",
    "href": "course_publish_blog.html#add-the-blog-to-github",
    "title": "Publish your blog",
    "section": "add the blog to GitHub",
    "text": "add the blog to GitHub\nNow we need to add the local repository to GitHub. Log into GitHub on the homepage. Then add a new repository, give it a name and make it public. Do not add anything to it, not .gitignore, README, license etc. \nOnce initialized, GitHub will give you several suggestions of what to do with your new repository. Follow the instructions of the option: ...or push an existing repository from the command line\nBack in VScode, check in the terminal that you are in the correct directory (the one of your blog).\nYou can then add the remote origin etc with the commands given on GitHub:\ngit remote add origin &lt;specifics of your repository&gt;\ngit branch -M main\ngit push -u origin main\nCheck on Github if the content of your blog has been added. You might have to refresh your page."
  },
  {
    "objectID": "course_publish_blog.html#publish-via-github-actions",
    "href": "course_publish_blog.html#publish-via-github-actions",
    "title": "Publish your blog",
    "section": "publish via GitHub actions",
    "text": "publish via GitHub actions\nNow that you have a local git repository that is synced to GitHub you can publish your blog using GitHub Pages, a website hosting service.\nThe default URL of your blog will be a combination of your GitHub user name and the name of the repository. If you have your own domain you can also use that at a later point in time.\nThe steps for publishing your blog with GitHub Actions are here. Follow the instructions until the section Executing code.\nNow, the publish command will be triggered automatically every time you push changes to the remote repository."
  },
  {
    "objectID": "course_publish_blog.html#tips-and-tricks",
    "href": "course_publish_blog.html#tips-and-tricks",
    "title": "Publish your blog",
    "section": "Tips and tricks",
    "text": "Tips and tricks\nWhen working on the blog, use git (adding, committing, pushing) often. Try to keep commits to one topic - maybe one blog entry per time. This way it is easier to label the commits and backtrack what has happened in the repository.\nDo not add large files to the repository. GitHub is a code hosting platform, not a data hosting platform. If you have data in your local directory you can exlude it from the repository with the .gitignore file."
  },
  {
    "objectID": "before_server.html",
    "href": "before_server.html",
    "title": "Connect to the server",
    "section": "",
    "text": "For this course, we will be working on a remote server, HPC2N. You should already have applied for a user account there. If you haven’t, do it now.\nHere, you will access the server for the first time. You will need a terminal, your user name and the temporary password you will have gotten from HPC2N.",
    "crumbs": [
      "Home",
      "Before the course",
      "Connect to the server"
    ]
  },
  {
    "objectID": "before_server.html#access-server-via-vscode",
    "href": "before_server.html#access-server-via-vscode",
    "title": "Connect to the server",
    "section": "Access server via VScode",
    "text": "Access server via VScode\nWe can use VScode to connect to the server. For this, we need to install the remote SSH extension from within VScode.\n\n\n\n\n\n\nTip\n\n\n\nIn the side bar, click on the extensions symbol (looks like building blocks), and search for remote SSH. Click then on install in the lower right corner.\n\n\n\nadd new SSH host\n\nOpen the Command Palette (View -&gt; Command Palette).\nType Remote-SSH and select Remote-SSH: Add New SSH Host.\nType ssh username@kebnekaise.hpc2n.umu.se, where you substitute “username” with your HPC2N user name.\nIf VScode cannot automatically detect the trype of server you need to select Linux.\nYou will be asked to pick a SSH configuration file to update. Choose the default (top of the list .ssh/config).\n\n\n\nconnect to server\n\nType Remote-SSH and select Remote-SSH: Connect to Host.\nSelect the HPC2N server.\ntype in your password.\n\nAfter a bit of setting up you will see in the bottom left corner the verification that you are connected to the server:\n\n\n\nConnected!\n\n\n\n\n\n\n\n\nCautionAfter the first log-in: reset password\n\n\n\nIn the welcome mail you got when your HPC2N account was created there was a link to create a first, temporary password. When you have logged in using that, you need to change your password.\nFrom the HPC2N documentation:\nThis is done using the passwd command:\npasswd\nUse a good password that combines letters of different case. Do not use dictionary words. Avoid using the same password that you also use in other places.\n\nIt will first ask for your current password. The first time you login, that will be the temporary password you created with the HPC2N password reset service.\nType in that and press enter.\nThen type in the new password you want, enter, and repeat.\nYou have changed the password.\n\n\n\nYou are now on connected to the login node of the cluster.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nRead here more about HPC cluster architecture. Read until the header MPI.\n\n\nThis location is the home folder of your account.\nOpen up the terminal in VScode (View -&gt; Terminal), and you are ready for the next chapter, learning (or repeating) the basics of the command line.",
    "crumbs": [
      "Home",
      "Before the course",
      "Connect to the server"
    ]
  },
  {
    "objectID": "course_git.html",
    "href": "course_git.html",
    "title": "Using git collaboratively",
    "section": "",
    "text": "This module will be run by Sam, and he has his own slides and content on Canvas."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This course is a hosted by the Swedish Agricultural University’s Bioinformatics Infrastructure (SLUBI).\nIn this course we hope to give you information on how to use reproducible bioinformatics pipelines, report results in a streamlined manner, and implement the system in your own research.\nFeedback is appreciated!\n(This is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites.)"
  },
  {
    "objectID": "before_git_and_github.html#version-control",
    "href": "before_git_and_github.html#version-control",
    "title": "git and GitHub",
    "section": "Version control",
    "text": "Version control\nWe all have worked on data before, done analyses, talked with our PI, changed the analyses, worked a bit more… and in the end we have something like this:\n\n\n\nWhich one of these is the latest version?\n\n\nVersion control, the practice of tracking and managing changes to files, can help us not descend into chaos. With a version controlled project you always know which file, and even which part of the file, is the most recent, and you can even go back to older versions if you have to.\nVersion control can be used on the local system, where both the version database and the checked out file - the one that is actively being worked on - are on the local computer. Good, but the local computer can be corrupted and then the data is compromised.\n\n\n\nLocal version control\n\n\nVersion control can also be centralized, where the version database is on a central server, and the active file can be checked out from several different computers. This is useful when working from different systems, or when working with collaborators. However, when the central servers is compromised the historical version are lost.\n\n\n\nCentralized version control\n\n\nAt last, version control can be fully distributed, with all versions of the file being on the server and different computers. Each computer checks out the file from its own version database to work on them. The databases are then synchronized between the different computers and the server. One such distributed version control system is git. It can handle everything from small to very large projects and is simple to use. GitHubis a code hosting platform for version control and collaboration, built on git.\n\n\n\nDistributed version control\n\n\nDistributed version control facilitates collaboration with others. Software like git automatically tracks differences in files, and flags conflicts between files.\nAdditionally, GitHub, the code hosting platform based on git that we are using in this course, can be used to maintain uniformity within a working group. The group can develop their own project template that people can use and populate for their own projects.",
    "crumbs": [
      "Home",
      "Before the course",
      "git and GitHub"
    ]
  },
  {
    "objectID": "before_git_and_github.html#git-and-github",
    "href": "before_git_and_github.html#git-and-github",
    "title": "git and GitHub",
    "section": "git and GitHub",
    "text": "git and GitHub\nGit is a version control software that is fully distributed - meaning that each project folder contains the full history of the project. These project folders are also called repositories and can be on several computers, or servers.\nGithub is a code hosting platform that is based on git. Here you can store, track and publish code (and code only, do NOT use github for data!). On Github you can collaborate with colleagues and work on projects together.\n\n\n\n\n\n\nNote\n\n\n\nA repository in git is the .git/ folder inside of your directory. This repository tracks all changes made to files in your project and contains your project history. Usually we refer to the git repository as the local repository.\nA repository in GitHub is where you can store your code, your files, together with their revision history. Repositories can be public or private, and might have several collaborators. Usually we refer to the Github repository as the remote repository.\n\n\nLet’s have a closer look at how git works:",
    "crumbs": [
      "Home",
      "Before the course",
      "git and GitHub"
    ]
  },
  {
    "objectID": "before_git_and_github.html#git",
    "href": "before_git_and_github.html#git",
    "title": "git and GitHub",
    "section": "git",
    "text": "git\n\nGit has three main states that your files can reside in: modified, staged, and committed:\n\n\n\nModified means that you have changed the file but have not committed it to your database yet.\nStaged means that you have marked a modified file in its current version to go into your next commit snapshot.\nCommitted means that the data is safely stored in your local database.\n\n\n\nsource: git documentation\n\nThis leads to the three main sections of a Git project: the working directory, the staging area, and the Git directory (or repository).\n\n\n\nWorking directory, staging area, and Git directory\n\n\nAnd the basic commands of git:\n\n\n\nBasic git commands\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese basic operations are all done on your local system. You have the entire history of the project on your local disk, and do not need an internet connection to work on your data with git. You can do all your commits on your local computer and later push them to a remote repostitory, like Github.\n\n\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nInstall git, and follow the Software Carpentries tutorial on Version Control with Git, chapters 1 to 13.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure you configure git with your GitHub account e-mail (either the one you signed up with, or the one provided by GitHub to hide your actual e-mail).\nI am using vim as a text editor, so I have never changed the default text editor for git. In the tutorial they give you options to change if you are used to a different editor.",
    "crumbs": [
      "Home",
      "Before the course",
      "git and GitHub"
    ]
  },
  {
    "objectID": "before_quarto_and_git.html",
    "href": "before_quarto_and_git.html",
    "title": "Quarto Homepage on Github",
    "section": "",
    "text": "Creating a website in Quarto\nPublish the quarto homepage on GitHub via GitHub Actions.",
    "crumbs": [
      "Home",
      "Before the course",
      "Quarto Homepage on Github"
    ]
  },
  {
    "objectID": "course_AI.html",
    "href": "course_AI.html",
    "title": "Caution: AI in Bioinformatics",
    "section": "",
    "text": "Caution\n\n\n\nWhile the lure of AI is becoming more present in our daily lives, remember that you do not need it. Before AI, you were perfectly able to design a packing list for your upcoming trip. You were able to look at a paper to find answers to your scientific questions. You knew how to query a vignette in R to determine how a function should be used.\nLife was a bit slower, but you used your mind and your agency. You made decisions. Please do not confuse convenience with need!"
  },
  {
    "objectID": "course_AI.html#introduction",
    "href": "course_AI.html#introduction",
    "title": "Caution: AI in Bioinformatics",
    "section": "Introduction",
    "text": "Introduction\nIn the past 3 years, AI has become more mainstream. The primary kind of AI people think of, is a Large Language Model (LLM). At this point, you cannot really avoid contact with these models anymore as they have infiltrated every aspect of the internet.\n\n\n\nSearching LLM with Google to be answered with an LLM response\n\n\nThese algorithms are language models trained on incredibly large datasets. They are trained to recognise and generate natural language. The chatbots we are all familiar with are generative pretrained transformers (GPTs). These can be trained for specific tasks, or guided by prompt generation.\nThis is not an AI theory course, and we really do not have enough knowledge to explain the underlying theory beyond a rudimentary level. We would, however, like to discuss the impact of AI on you as a user, and advocate for responsible use of AI in your current and future work."
  },
  {
    "objectID": "course_AI.html#machine-learning-in-bioinformatics",
    "href": "course_AI.html#machine-learning-in-bioinformatics",
    "title": "Caution: AI in Bioinformatics",
    "section": "Machine Learning in Bioinformatics",
    "text": "Machine Learning in Bioinformatics\nMachine learning (ML) has been used in bioinformatics and other fields of data science for many decades on every level. Before ML, algorithms had to be programmed by hand rather than having the algorithms learn features of a dataset. With ML, features of a dataset can be annotated based on previously annotated datasets. These algorithms were a mix of supervised (learning on annotated data) and unsupervised (learning on unannotated data) learning, depending on the function of the algorithm.\nSupervised algorithms are used for classification and regression analyses. Unsupervised algorithms are used to discover hidden patterns in data without needing a human’s input. Unsupervised algorithms are used in clustering, association, and dimensionality reduction\n\n\n\n\n\n\nNote\n\n\n\n\n\nClassification: Output is a discrete variable. Linear classifiers, support vector machines, decision trees, random forests. E.g. annotating a new genome based on genome annotations from existing species.\nRegression: Focus on understanding dependent and indepedent variables.\n\n\n\nFor more info, see here, and here.\nThere is no arguing that these algorithms have led to great progress within the field of bioinformatics. Generative AI is one of the next steps in the evolution of applying machine learning in our lives."
  },
  {
    "objectID": "course_AI.html#incorporation-of-ai-in-our-lives",
    "href": "course_AI.html#incorporation-of-ai-in-our-lives",
    "title": "Caution: AI in Bioinformatics",
    "section": "Incorporation of AI in Our Lives",
    "text": "Incorporation of AI in Our Lives\nChatGPT gained 100 million users in the 2 months after its release in 2022, making it the fastest-growing consumer application in history. Generative AI (GenAI) models now come in many different flavours, depending on the developer.\n\n\n\nGenAI chatbots by market share in August 2025 from FirstPageSage\n\n\n\nTraining Data\nAs with earlier iterations of supervised and unsupervised algorithms, GenAI models have all been trained on existing data. And this existing data can be biased: in a historical context, history was recorded by the party that won the war. History changed as different empires and narratives rose and fell. With digitisation, this information has landed on the internet. In the more modern “Internet Age”, everyone with an internet connection can technically post anything they’d like on the internet. This can add different types of biases - not everyone has equal access to the internet, some people may not have strong enough opinions to post about something online, some people prefer to read rather than contribute, while others take pleasure in “shit-posting”.\nAs the GenAI training data contains large amounts of data from the internet, these differences in how people use the internet have an effect on how useful the trained models become. In this example, you can clearly see what the effect of bad training data is:\n\n\n\nGenAI answer to a simple question\n\n\nFurthermore, GenAI’s are not programmed to say that they do not know something, and will happily hallucinate an answer. If you do not know better, or trust the computers, you may take a made-up answer as true and post it elsewhere. As the use of AI’s increases, AI generated content is used to train new AI’s. Gary Illyes from Google has spoken about “human curated” vs “human created” data being used as training data.\nIn a perfect world, GenAI would be trained on perfectly curated data, but even with all of the data that we have on the internet at the moment, we do not have nearly enough data. We have to make do with what we have."
  },
  {
    "objectID": "course_AI.html#some-words-of-caution",
    "href": "course_AI.html#some-words-of-caution",
    "title": "Caution: AI in Bioinformatics",
    "section": "Some Words of Caution",
    "text": "Some Words of Caution\nGenAI is becoming more integrated in every sector of our lives. It is important that we use the new technology responsibly. When Google first came out, there were classes on how to use the search engine, determine validity of sources and information, and how to stay safe on the internet. This section aims to raise awareness about commonly overlooked aspects of GPT use.\n\nLearning with AI\nAI has great potential in the field of education. ChatGPT has been shown to be highly beneficial in an educational environment when integrated properly. However, the use of AI in this setting must be balanced and carefully curated. A 2025 pre-print by Kosmyna et al showed that adults that used ChatGPT to write SAT type essays were outperformed consistently by adults that wrote the same essay without the support of an AI, and had significantly lower brain engagement.\n\n\nProductivity with AI\nA recent study by a non-profit group, Model Evaluation and Threat Research (METR) aimed to quantify the difference in productivity when using AI. Participants in this study were not new to their field, with at least 5 years of experience prior to this study being conducted.\n\n\n\nAI reducing productivity\n\n\nThe study also found that when AI is allowed, the participants spent less time coding and seeking solutions to the problems. Rather, they spent time prompting the AI, reading and reviewing responses, and being idle. Intel produced similar findings.\n\n\n\nReasons for loss of productivity with AI\n\n\n\n\nData Privacy and Legal Concerns\nData privacy concerns have been present throuhout the development and use of AI.\nThe use of copyrighted material in training AI, and its legality, is being discussed and debated in several courts. The questions around using text and data mining are divisive with some parties arguing that finding patterns, trends, and insights in existing data being how new research is done by humans and should be extended to AI, while others, particularly in the European context, disagree to some extent. Understanding the legality of the service you use in different countries falls on you as a user.\nThe Terms of Service (ToS) of different GPTs are important when deciding whether to use a GPT at all. For example, Deepseek’s ToS (collected on 15.08.2025) states:\nAccount Personal Data. We collect Personal Data that you provide when you set up an account, such as your date of birth (where applicable), username (where applicable), email address and/or telephone number, and password.\n\nUser Input. When you use our Services, we may collect your text input, prompt, uploaded files, feedback, chat history, or other content that you provide to our model and Services (“Prompts” or \"Inputs\"). We generate responses (“Outputs”) based on your Inputs.\n\nPersonal Data When You Contact Us. When you contact us, we collect the Personal Data you send us, such as proof of identity or age, contact details, feedback or inquiries about your use of the Services or Personal Data about possible violations of our Terms of Service (our “Terms”) or other policies.\nBiological data enjoys a high level of protection, and is often considered as highly sensitive. Too often, users will input sequences, tables, or other data into a GPT to find ways to plot data, perform sequence annotations, or similar tasks. Be wary of doing this. Check the ToS explicitly, and frequently. Try to use GDPR compliant GPT’s if you must use a GPT. Try using only a description of your data - such as column names and type of data - instead of the actual data set.\nAI as browser extensions steal deeply personal information when enabled in a users’ browser including financial, education, and medical information, whether an extension is being actively used or not. The authors have also commented on how these practices interfere with the company’s own ToS as well as privacy legislation.\n\n\nPersonal Responsibility\nYou as a user are responsible for the AI generated content you choose to use. If you, for instance, ask AI to generate a brand logo for you that is too close to something that exists already, the original owner is free to sue you as an individual for copyright infringement.\nAs scientists, we know that we should use peer-reviewed resources whenever possible. It is why we cannot cite Wikipedia in a scientific article. GPT’s have been widely shown to fabricate citations based on how it has learnt a citation should look. It is up to you as a user to check every single citation that AI generates since you are responsible for what you write.\n\n\nGlobal Linguistic Changes\nEven though ChatGPT has only been widely used for 3 years, it has already started leaving its traces in how we speak. Words like delve and meticulous are being used more frequently in academic YouTube talks.\n\n\n\nGPT words in YouTube videos from Yakura et al 2025\n\n\nIt has also been shown that different GPT’s have different writing styles, also known as idiolects.\nSome projects like this one are trying to customize GPT ideolects to match writing styles of unique users. This will make detecting AI use more difficult in future.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis course was written by two people, and each person wrote their own sections without the use of any AI. Can you tell who wrote which sections based on idiolects?\n\n\n\nWe know that subtle linguistic shifts can change emotional regulation within individuals. We also know from sociolinguistics, that even the slightest linguistic features can serve to bind or divide us.\n\n\nAI on the Internet\nSocial media platforms are an important aspect in the development, improvement, and implementation of GenAI models. OpenAI, the creators of ChatGPT, have used a subreddit on the social media platform, Reddit, to train their new algorithm. Google and OpenAI have contractual agreements with Reddit to license data from the users on the platform. Earlier in 2025, researchers from the University of Zurich were implicated in an experiment on the same subreddit OpenAI used to train a model. They wanted to test whether an interaction with a bot was more likely to make people change their minds than an interaction with a real person. This was heavily frowned upon, and posts were all removed as users had no ability to consent to participating in a study. It has also been suggested that interactions observed by the researchers were just bots arguing with each other.\nA preprint released in February 2025 by Liang et al. found that the amount of content generated by AI rose from 2-3% in November 2022 to 24% by the end of 2023.\n\n\n\nAI slop trough by Yahoo! News!\n\n\n\n\nEnvironmental Impact\nThe facilities to run GenAI require a significant amount of resources. These facilities need a huge amount of electricity to power the facility (this places extreme strain on exising infrastructure and increases the grid’s carbon footprint) as well as water to cool the hardware. Currently, data centers use more electricity than many independent countries.\nSome data centers are being built near poor communities, drain resources from, and add pollution to the community (see Colossus that has been built in Memphis to power the X bot, Grok for an example. Musk is not the only offender.)\nWhile we cannot do anything about where data centers are built, we can make informed decisions about which platforms we use. We can also be careful with the number of queries we send, and how we use our queries. In April 2025, the CEO of OpenAI said that polite requests like “please” and “thank you” have cost tens of millions of dollars due to the cost of electricity."
  },
  {
    "objectID": "course_AI.html#how-to-decide-when-to-use-ai",
    "href": "course_AI.html#how-to-decide-when-to-use-ai",
    "title": "Caution: AI in Bioinformatics",
    "section": "How To Decide When To Use AI",
    "text": "How To Decide When To Use AI\nIf we know the risks and the true cost of what we are doing, we can make informed decisions about how we chose to incorporate new technologies into our day-to-day and working lives.\nHere are some questions that we find useful to ask ourselves before opening a GPT:\n\nAm I phrasing my prompt in a good way? Here is a guide to prompt engineering that might be useful.\nCan I find this information any other way?\nHow much time am I saving by looking this question up here vs on BioStars, for example?\nDo I know enough about the topic to know whether the GPT is lying to me?\nWhat are the consequences of testing the validity of the GPT solution? Can I potentially corrupt my data or my system? Is there a potential for me to lie to someone who trusts me enough to ask my opinion?\n\nIf a GPT is used to learn a new skill, remember how important active learning is. Seek explanations for everything the GPT tells you. Find independent sources that were produced by experts to validate your learning.\nHold on to the ability to learn and the desire to be curious."
  },
  {
    "objectID": "course_container.html#reproducibility-in-bioinformatics",
    "href": "course_container.html#reproducibility-in-bioinformatics",
    "title": "Containers",
    "section": "Reproducibility in Bioinformatics",
    "text": "Reproducibility in Bioinformatics\nIn an ideal world I would be able to write a piece of software, or a develop some code to analyse data on my computer, and then send someone else this software or code and they could run it as well, getting the same results.\nIn reality, I would very likely run into at least one of the following problems:\n\nIt is not uncommon for people within the same team to use different operating systems (whether MacOS, Windows, or different flavours of Unix builds). Even if everyone is using a MacOS, there are still different versions that impact the way people are able to work with their machines.\nAlmost every piece of software has some sort of dependency - other software - it needs to run. Some programs might “just” need a bash shell or basic python, while others need a variety of compilers and additional libraries to function. Often, these dependencies require further dependencies to be installed. It is also not uncommon for dependencies for Program 1 to clash with the dependencies for Program 2, requiring the user to uninstall dependencies to be able to install others.\nIn bioinformatics, tools are very often not maintained after the student that wrote the software graduated, the PI moved to a different university, or the funding simply ran out. This leads to a lot of really good software not really being supported by newer operating systems, usually due to dependencies not being easily available or, as before, clashing with newer versions. This makes installing a tool one of the biggest hurdles to overcome in bioinformatics.\nYou often cannot install different versions of the same program on one computer due to conflicting names. This is particularly problematic when you want to rerun an analysis for a publication where you need to use the same software all the way through.\n\nAny of these points might lead to you not being able to run my code, or it running but giving different results. They make bioinformatics less reproducible as tools and code cannot be moved easily betweem systems (for example if you upgrade your computer or want to share your pipeline with a colleague). Fortunately, most of these problems can be overcome with containers."
  },
  {
    "objectID": "course_container.html#containers",
    "href": "course_container.html#containers",
    "title": "Containers",
    "section": "Containers",
    "text": "Containers\n\nWhat are containers?\nContainers are stand-alone pieces of software that require a container management tool to run. They are build and exchanged as container images that specify the contents of the container, such as the operating system, all dependencies, and software in an isolated environment. The container management tool then takes the images and build the container. These management tools can be run on all operating systems, and since the container has the operating system within it, it will run the same in all environments. Container images are easily portable and immutable, so they are stable over time.\n\n\nRunning Containers\nThere are several programs that can be used to build and run containers. Docker, Appptainer, and Podman are the most commonly used platforms to date. They all have their pros and cons. If you are using a Windows machine that only you are using, then Docker is likely the least complex tool to install. On multi-user systems like a server, Apptainer is the best tool for the job. For this tutorial and the rest of the course, we will use Apptainer commands. There are small syntax changes between bash and powershell commands, but they are very similar.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLog in to the course server.\n\n\n\n\nDownloading Container Images\nThere are several repositories for people to publish container images that they have specified. Dockerhub and Seqera are two commonly used platforms for downloading container images. You are able to use container images from dockerhub on Apptainer without any problems.\n\ndockerhub Tutorial\nOn the dockerhub landing page, you have a search bar, and some login options. You do not need to create an account to access the containers on dockerhub.\n\n\n\ndockerhub landing page\n\n\nFor these tutorials, we’ll search VCFtools, a commonly used software for VCF manipulation and querying. The results of the search give us several different containers with the same name.\n\n\n\nRegistry search\n\n\nYou can see who made the container image, how many times it has been downloaded (or pulled), when it was updated (here updated means different versions of the image being uploaded), and how many people have starred it. It is usually a good rule of thumb to use the most popular images from users that have uploaded a lot of container images. The biocontainers and pegi3s profiles have builds for a lot of tools, and they are built really well!\nIf we click on the vcftools from biocontainers we get to a typical dockerhub image landing page:\n\n\n\nVCFtools page\n\n\nThere is information on the frequency of the container image being pulled, as well as a pull command to download the image. This command is for docker, so we need to modify it for Apptainer.\napptainer pull vcftools_0.1.16-1.sif docker://biocontainers/vcftools:v0.1.16-1-deb_cv1\n\n\n\n\n\n\nImportantApptainer\n\n\n\nAs you can see, Apptainer is installed on the course server. If it wasn’t, could we still use it?\n\n\nThis command has several parts to it:\n\napptainer calls on the Apptainer software to run\npull tells Apptainer which function to use. In this case, we want it to go fetch something from a repository\nvcftools_0.1.16-1.sif is the name of the container image on our local machine. We could call it I_Love_Dogs but that is not very informative at all. Your collaborator won’t know what it means, and you certainly won’t know what it means in 6 months from now! It is also good practice to put the version number in your image name in case you want to have several versions at the same time, and you need to tell them apart.\ndocker:// is the registry you are pulling from. There are several different registries, but we are only going to show 2 during this course. (You will see another one in the Seqera tutorial)\nbiocontainers/vcftools is the profile/repository and container you are pulling\n\n\n\n\n\n\n\nTip\n\n\n\nFile format extensions like .txt and .sif are really only important for us. However, it is good practice to append your files with appropriate extensions to ensure that you follow good data management practices\n\n\nIf you are interested in a different version than the current version, there are other versions under the tags tab:\n\n\n\nContainer versions\n\n\nIf you wanted to download another version of the container, you simply copy the command shown on the right side, and alter the syntax, for example\napptainer pull vcftools0.1.14.sif docker://biocontainers/vcftools:v0.1.14_cv2\n\n\nSeqera Tutorial\nThe Seqera landing page is a bit different from the dockerhub landing page, and it works a bit differently from dockerhub. Dockerhub hosts container images that users have uploaded, while Seqera makes container images as you request them. They use bioconda, conda-forge, and pypi libraries to build their containers images with Wave. The advantage is that you can include several different softwares in your container image at once. The disadvantage is that you are limited to software hosted on the aforementioned repositories. Usually this isn’t a problem, but sometimes you want to use something that isn’t hosted there.\n\n\n\nSeqera containers landing page\n\n\nWhen you pull an image from Seqera and want to run it with Apptainer, you need to remember to change the container setting from Docker to Singularity, the older name of Apptainer.\n\n\n\nSelecting Singularity\n\n\nSince Seqera builds containers on-demand, sometimes you have to wait for the container to finish compiling. You can see that it is still preparing the container image from the fetching container comment. Don’t try to pull it when it is still building!\n\n\n\nWaiting to build\n\n\nWhen the container image is ready, you can copy the text and pull the image to your system:\n\n\n\nReady to download\n\n\napptainer pull vcftools_0.1.17.sif oras://community.wave.seqera.io/library/vcftools:0.1.17--b541aa8d9f9213f9\nHere we use oras:// instead of docker:// as we are pulling from the oras registry. We are also pulling a different version from Seqera, so the name of the container is different.\n\n\n\nRunning Containers\nOnce you have the container image on your local machine, you want to be able to use it. Apptainer can be used to build the container from the image. Then you can either enter the container and run as if you had the exact same operating system as the person who built it, or you can run the software inside the container from outside of the container.\n\nrunning “from the outside”\nThere are 2 different ways to use a container: run and exec. The apptainer run command launches the container and first runs the %runscript for the container if one is defined, and then runs your command (we will cover %runscript in the Building Containers section). The apptainer exec command will not run the %runscript even if one is defined. It is a small, fiddly detail that might be applicable if you use other people’s containers. After calling Apptainer and the run or exec commands, you can use your software as you usually would\napptainer exec vcftools_0.1.17.sif vcftools --version\nThis command runs your vcftools_0.1.17.sif container from the image, calls on the program vcftools that is within the container, and shows you the version. If you had installed VCFtools locally, you would have just used\nvcftools --version\n\n\n\n\n\n\nImportant\n\n\n\nPlease remember that VCFtools is just an example. If you want to run any other tool everything after apptainer run or apptainer exec has to be substituted by the name of the specific container image and the run commands for that particular tool!\n\n\n\n\nrunning interactively “from the inside”\nYou can also enter the container, and work interactively from within. For that you use the apptainer shell command:\napptainer shell &lt;name-of-container&gt;\nInside the container, your prompt will change to Singularity (remember, that is the legacy name for Apptainer). Now you can use the tools inside the container.\nHowever, when entering the container, the file system outside of it becomes inaccessible - with the exception of paths that are explicitely bound into it. There are some system defined bind paths that are automatically accessible within the container (such as your home directory), but you might have to manually set others with the ´-B´ flag. This will make the bound file path accessible from within the container and you can interact with the directories jsut as you normally would.\napptainer shell &lt;name-of-container&gt;\napptainer shell -B outside/path:inside/path &lt;name-of-container&gt;\nIn the above, we bind in the outside/path (which could be ../data) and makes it accessible within the container as inside/path (which could be plain /data).\nTo exit the container, simply type and enter exit.\n\n\n\nRunning containers in batch jobs\nYou can run Apptainer containers as part of a batch job by integrating them into a SLURM job submission script. Here is an example batch script.\nLet’s go back to our FastQC sbatch script and add the container:\n#! /bin/bash -l\n#SBATCH -A project_ID\n#SBATCH -t 30:00\n#SBATCH -n 1\n\napptainer exec -B ../data:data fastqc -o . --noextract data/*fastq.gz\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nWhat has changed? And what do these changes do? Discuss with your neighbour.\n\n\n\n\nBuilding Containers\nIf the software you would like to use is not packaged into a container by anyone else, you might want to build it yourself. For this, we are just going to show a very simple example. Building containers from scratch is a computationally intensive task. You build containers from a definition file with the extension .def\nHere we are going to build a container with a cow telling us the date. Save this in a file called lolcow.def.\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    date | cowsay | lolcat    \nThere are several components to this definition file.\n\nYou can set the operating system you want in the container, in this case Ubuntu 20.04.\n%post section is where you update the OS from its base state, install dependencies and so on.\n%environment is where you export paths and modify the environment.\n%runscript is the script that will run when you use apptainer run container.sif. If you don’t include a runscript, then nothing will happen when you try to run it without any commands. You could build this container without anything in the %runscript section, and use apptainer run container.sif date | cowsay | lolcat to get the same output.\n\napptainer build lolcow.sif lolcow.def\nYou’ll get a lot of output on the status of the build, ending in\nINFO:    Adding environment to container\nINFO:    Adding runscript\nINFO:    Creating SIF file...\nINFO:    Build complete: lolcow.sif\nWe can now run our new container with\napptainer run lolcow.sif\n\n\n\nBoring cow\n\n\n\n\n\n\n\n\nNote\n\n\n\nTry removing the %runscript, build it again, and see what happens.\n\n\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    fortune | cowsay | lolcat    \nIf we use the same definition file as before, but substitute date for fortune in the runscript and build the container, we now get a philosophical cow with a dark sense of humour:\n\n\n\nFun cow\n\n\n\n\n\nInspirational cow\n\n\nTo show the difference between the run and exec commands, we can use the same container with fortune in the runscript and run:\napptainer run lolcow.sif bash -c \"date|cowsay\"\nand\napptainer exec lolcow.sif bash -c \"date|cowsay\"\nThe run command gives us a philosophical cow while exec gives us our boring cow again."
  },
  {
    "objectID": "before_command_line.html",
    "href": "before_command_line.html",
    "title": "Command line",
    "section": "",
    "text": "Unix-like operating systems are built under the model of free and open-source development and distribution. They often come with a graphical user interface (GUI) and can be run from the command line (CLI) or terminal. The CLI is a text-based interface that works exactly the same way as you would use your mouse, but you use words.\nIt is important to know how to use the terminal as all servers, and most bioinformatics tools, do not have a GUI and rely on the use of the terminal.\nFor this course, we will use a GUI for some parts, but all our interaction with the remote server will be on the command line. As such, it’s important that you know your way around the command line.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nI know that you have varying experience with the command line. The following tutorial gives an excellent overview over the basics (and not so basics). If you have already worked with the command line it will probably be enough to browse and brush up on some parts, if you haven’t this will be your chance to learn:\nSoftware carpentries tutorial on the Unix shell.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe above tutorial is designed to be taken on your local machine, where you have a home directory and a Desktop.\nYou can also follow the tutorial on the course server. However, you won’t have a Desktop there, and will have to adjust accordingly.\nIf you want to follow the course on the server you can download the tutorial data with:\ncurl https://swcarpentry.github.io/shell-novice/data/shell-lesson-data.zip -o shell-lesson-data.zip \nThen decompress with:\nunzip shell-lesson-data.zip"
  },
  {
    "objectID": "course_data_management.html#data-life-cycle",
    "href": "course_data_management.html#data-life-cycle",
    "title": "Data Management for Reproducible Research",
    "section": "Data Life Cycle",
    "text": "Data Life Cycle\nWhen working with any type of data, it makes sense to sit down before the project starts to think through the different life stages of the data in your project. This will help counteract some of the problems that can arise when projects grow more organically, and will help consistency within the research group, ease collaboration, and mostly your future self that will understand what past-self has been up to in the project.\n\n\n\n\n\n\nNote\n\n\n\nMore and more funding agencies expect a Data Management Plan at some point of a project application. In there, you need to document that you have thought of, and planned for, the life cycle of your data.\n\n\n\n\n\nThe Research Data Management toolkit for Life Sciences"
  },
  {
    "objectID": "course_data_management.html#fair-principles",
    "href": "course_data_management.html#fair-principles",
    "title": "Data Management for Reproducible Research",
    "section": "FAIR principles",
    "text": "FAIR principles\nIn the past, research data was often generated with one question in mind. Often, they would afterwards land in some drawer and be forgoten about. Nowadays researchers acknowledge that data can also be re-used, or combined with other data, to answer different questions.\nThe FAIR principles promote efficient data discovery and reuse by providing guidelines to make digital resources:\n\n\n\nWilkinson et al. (2016)\n\n\nFAIR principles, in turn, rely on good data management practices in all phases of research:\n\nResearch documentation\nData organisation\nInformation security\nEthics and legislation"
  },
  {
    "objectID": "course_data_management.html#reproducible-research",
    "href": "course_data_management.html#reproducible-research",
    "title": "Data Management for Reproducible Research",
    "section": "Reproducible research",
    "text": "Reproducible research\nLucky for us, once we implement good data management practices, we will also increase the reproducibility of our analyses. Extensive documentation will increase faith in the outcome of analyses, and will help people (again, future-you) understand what has been done.\nLast, but not least, reproducible research practices make project hand-overs smoother, when the next person already understands the structure of the project, and can rely on good documentation."
  },
  {
    "objectID": "course_data_management.html#what-data-do-we-work-with",
    "href": "course_data_management.html#what-data-do-we-work-with",
    "title": "Data Management for Reproducible Research",
    "section": "What data do we work with?",
    "text": "What data do we work with?\n\nBioinformatics is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex. (Wikipedia)\n\nThis data can come from a variety of different biological processes:\n\n\n\nsource: Lizel Potgieter\n\n\nEarly on, sequencing data was not readily available, but due to decreasing costs and increased computational power biological data is now being produced in ever increasing quantities:\n\n\n\nGrowth of the Sequence Read Archive, SRA, from 2012 to 2021\n\n\nAt the same time, new technologies are being developed, and new tools that might or might not be maintained or benchmarked against existing tools. It’s the wild west out there!\n\n\n\nOverview of modern sequencing technologies and where they apply to biological processes"
  },
  {
    "objectID": "course_data_management.html#working-with-data",
    "href": "course_data_management.html#working-with-data",
    "title": "Data Management for Reproducible Research",
    "section": "Working with data",
    "text": "Working with data\nOften, with a new project, one sits down with the data, tries out things and see if they worked. A lot of bioinformatics is not being afraid to try things, and reading the documentation.\nThis traditional way of working with bioinformatics data can have merits and lead to new discoveries. However, in this course we would like to introduce you to a more structured way to make sense of your data.\nLet’s have a look at a typical PhD student’s research project:\n\nThey might analyse their data, and get some results.\nAfter talking with their supervisor they might get a few other samples from a collaborator, or need to drop them from the analyses due to quality concerns.\nThey run the analyses again and get a different set of results.\nThere might be a few iterations of this process, and then the reviewers require some additional analyses…\n\nIn the “end” we have something like this:\n\n\n\nWhich one of these is the latest version?\n\n\n\n\n\n\n\n\nTipBest practices file organization\n\n\n\n\nThere is a folder for the raw data, which does not get altered.\nCode is kept separate from data.\nUse a version control system (at least for code) – e.g. git.\nThere should be a README in every directory, describing the purpose of the directory and its contents.\nUse file naming schemes that makes it easy to find files and understand what they are (for humans and machines) and document them.\nUse non-proprietary formats – .csv rather than .xlsx"
  },
  {
    "objectID": "course_data_management.html#literate-programming",
    "href": "course_data_management.html#literate-programming",
    "title": "Data Management for Reproducible Research",
    "section": "Literate programming",
    "text": "Literate programming\nOur hypothetical PhD student, even if taking into account the best practice tips from above, is still likely to run the same analyses over and over whenever the input data changes. Sometimes, this might be months, or even years, after the original analysis was performed.\nLuckily, they can save their code snippets (with intuitive file names) and re-use the code from back then. This is often done with R-scripts, but can just as well be applied to bash scripts, python scripts etc.\nIn the past years, the development went even further and one can even combine code and documentation in the same document. The code is wrapped in so called chunks, or code cells, that are executable from within the document.\n\nDebugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it. Brian Kernighan\n\nBefore the course you have already worked with one such notebook - Quarto. We will continue to work with it during this course.\n\n\n\n\n\n\nTip\n\n\n\n\nDocument your methods and workflows.\nDocument where and when you downloaded data.\nDocument the versions of the software that you ran."
  },
  {
    "objectID": "course_data_management.html#version-control",
    "href": "course_data_management.html#version-control",
    "title": "Data Management for Reproducible Research",
    "section": "Version control",
    "text": "Version control\nNow that our student has reproducible documents, with reasonable names, that can execute their analyses reliably over and over again, what happens if they modify their analyses? Will they end up again with different result files and their project sink down in chaos?\nNo, because there is version control, the practice of tracking and managing changes to files.\nAgain, before the course you worked through the basics of git, and how to use it with GitHub collaboratively. We will continue using git during the course as well."
  },
  {
    "objectID": "course_data_management.html#environment-managers",
    "href": "course_data_management.html#environment-managers",
    "title": "Data Management for Reproducible Research",
    "section": "Environment managers",
    "text": "Environment managers\nUsing git, our PhD student can now share their reproducible code with their colloaborators, or between systems. They can rest assured that the different versions of the notebook are tracked and can be checked out when necessary. But what about the bioinformatic tools? Can they also be shared easily?\nDifferent computers can run on different operating systems, or can have different versions of databases installed. This can lead to conflicts between tools, or software versions and can impact code usability, or reproducibility.\nFortunately, smart people have developed environment managers such as conda, bioconda, or pixi. These tools find and install packages, so that the same package versions are being run between different computers. However, the code might still give different results on different operating systems.\nDuring this course we will be building our own environments with Pixi - you’ll see how great it is not having to manually install tools anymore!"
  },
  {
    "objectID": "course_data_management.html#containers-in-bioinformatics",
    "href": "course_data_management.html#containers-in-bioinformatics",
    "title": "Data Management for Reproducible Research",
    "section": "Containers in bioinformatics",
    "text": "Containers in bioinformatics\nBut what if our PhD student needs to run their code on different operating systems?\nThey can use containers, that contain everything needed to run the application, even the operating system. Containers are being exchanged as container images, which makes them lightweight. Containers do not change over time, so the results will be the same today and in a few years. Everyone gets the same container that works in the same way.\nIn this course, you will have guessed it, we will learn about containers, where to get them and how to use them."
  },
  {
    "objectID": "course_data_management.html#workflow-manager",
    "href": "course_data_management.html#workflow-manager",
    "title": "Data Management for Reproducible Research",
    "section": "Workflow manager",
    "text": "Workflow manager\nNow our PhD student can use containers, or environments, to provide a uniform environment for their version controlled, wonderfully documented and reproducible code. Fantastic! But they still have to deploy, or at least monitor, their scripts manually.\nFortunately there are workflow managers that can integrate all of the above, submit your jobs for you, and even monitor and re-submit scripts after failure. They will automatically submit jobs for you, decreasing downtime and increasing efficiency.\n\n\n\n\n\n\nTip\n\n\n\nHumans tend to do mistakes, especially when it comes to tedious or repetitive tasks. If you automate data handling, formatting etc. you are less likely to make mistakes like typos, or changing colors in images.\n\n\nIn this course, we will also cover a workflow manager, Nextflow and learn how to make our own workflow, and how to run already developed workflows."
  },
  {
    "objectID": "course_data_management.html#goal-of-the-course",
    "href": "course_data_management.html#goal-of-the-course",
    "title": "Data Management for Reproducible Research",
    "section": "Goal of the course",
    "text": "Goal of the course\nWith this, we want to give you tools that will help you plan and carry out your research. These tools will make your work more efficient and more reproducible. No matter what kind of data you use, you will take something useful from this course.\n\n\n\nOverview of modern sequencing technologies and where they apply to biological processes"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Bioinformatics",
    "section": "",
    "text": "The aim of this course is to share how to use bioinformatics tools in a reproducible and scalable way. We will use environments, containers, and established pipelines so that you can run these analyses on any operating system, as well as on systems that are not high performance computing clusters. And first and foremost: these tools and techniques can be used regardless of which type of bioinformatics you are ultimately working with.\nThe website will remain active after the course so that you have access to the material even after the course.\nWe will meet in person in Ultuna between Monday, October 6th, and Friday, October 10th, 2025. However, to get the most out of this course, we expect you to do some preparation in advance. This is to set-up and get aquainted with some of the tools we will be using.\nI want to thank Lizel Potgieter for her valuable contributions to the materials for this course!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Applied Bioinformatics",
    "section": "Course content",
    "text": "Course content\n\nbefore the course:\n\nBefore the course\n\n\nTopic\nContent\n\n\n\n\nHPC access\nCreate user accounts for the HPC resources\n\n\nVScode\nSettting up and getting aquainted with VScode\n\n\nServer access\nSSH connect to the course server\n\n\nCommand line\nUse the command line to manipulate data\n\n\nscreen\nManage persistent bash sessions\n\n\nQuarto\nInstalling and using Quarto with VScode\n\n\ngit and GitHub\nVersion control with git and GitHub\n\n\n\n\n\nduring the course:\n\nProgram\n\n\nDay\nSession\n\n\n\n\nMonday\nWelcome, introduction\n\n\n\nData Management & Reproducible Research\n\n\n\nUsing git collaboratively\n\n\n\nCreate a blog with Quarto\n\n\n\nPublish your blog with GitHub actions\n\n\nTuesday\nIntroduction to environments\n\n\n\nQuality control of sequencing data\n\n\n\nIntroduction to containers\n\n\n\nUse containers\n\n\nWednesday\nnextflow\n\n\n\nquality control with nextflow\n\n\n\ncontainers cont.\n\n\nThursday\nnf-core\n\n\n\nHands-on: Test a nf-core pipeline\n\n\n\nHands-on: set up a nf-core pipeline\n\n\nFriday\nVisualize your results: ggplot\n\n\n\nDiscussion: AI in Bioinformatics\n\n\n\nclean-up and finishing",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "course_environments.html",
    "href": "course_environments.html",
    "title": "Introduction to Pixi",
    "section": "",
    "text": "Different operating systems bring different challenges to bioinformatics. Windows, for instance, doesn’t really support most bioinformatics tools, or your computer might run a different version of a specific tool than mine. One way to solve this, and make research more reproducible, is through the use of environments. Bioinformatics environments specify the tools neded for the task at hand, and environment managers install these tools with all their dependencies. There are many different kinds of environment managers, and for this course we are going to use Pixi.\nThere are, of course, other ways to solve tool access and compatibility issues, such as running virtual machines. In our experience, however, environments are a bit easier to manage and are more portable across different systems and users.\n\n\n\nPixi landing page\n\n\nAs you can see, you can run Pixi on all major operating systems, and you can include various platforms in your Pixi environment.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLog in to the course server and follow along:\n\n\n\n\nInstalling Pixi is really easy and described thoroughly here with separate installation guides for Windows and Mac/Linux.\n\n\n\n\n\n\nTip\n\n\n\nTo source your shell, you need to source the startup files, in Linux it’s the ~/.bashrc file, in Mac it’s the ~/.zshrc file."
  },
  {
    "objectID": "course_environments.html#introduction",
    "href": "course_environments.html#introduction",
    "title": "Introduction to Pixi",
    "section": "",
    "text": "Different operating systems bring different challenges to bioinformatics. Windows, for instance, doesn’t really support most bioinformatics tools, or your computer might run a different version of a specific tool than mine. One way to solve this, and make research more reproducible, is through the use of environments. Bioinformatics environments specify the tools neded for the task at hand, and environment managers install these tools with all their dependencies. There are many different kinds of environment managers, and for this course we are going to use Pixi.\nThere are, of course, other ways to solve tool access and compatibility issues, such as running virtual machines. In our experience, however, environments are a bit easier to manage and are more portable across different systems and users.\n\n\n\nPixi landing page\n\n\nAs you can see, you can run Pixi on all major operating systems, and you can include various platforms in your Pixi environment.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLog in to the course server and follow along:\n\n\n\n\nInstalling Pixi is really easy and described thoroughly here with separate installation guides for Windows and Mac/Linux.\n\n\n\n\n\n\nTip\n\n\n\nTo source your shell, you need to source the startup files, in Linux it’s the ~/.bashrc file, in Mac it’s the ~/.zshrc file."
  },
  {
    "objectID": "course_environments.html#setting-up-an-environment",
    "href": "course_environments.html#setting-up-an-environment",
    "title": "Introduction to Pixi",
    "section": "Setting Up An Environment",
    "text": "Setting Up An Environment\nYou should create separate environments for each project you run, just to keep things tidy. To create an environment, you have to specify a name for your environment. You can include different platforms/operating systems in your environment, for example if you want to develop your code on a Windows system, and later use the same code on many samples on a large Linux cluster. You can also include different vetted sources for your tools, the so called channels.\nHere, we will create a project called name_pixi_training (please use your own name to avoid creating multiple environments with the same name). We are adding the conda-forge and bioconda channels with the -c flag.\npixi init name_pixi_training -c conda-forge -c bioconda\nPixi will create a folder named name_pixi_training with a a file pixi.toml. Let’s have a look at that file!\n\n\n\n\n\n\nTipHow to get to the file\n\n\n\n\n\nHere is the code for changing directories, listing files, and viewing the contents of a file:\ncd name_sida_training\nls\nless pixi.toml\nTo exit the less view, press q for quit.\n\n\n\n\npixi.toml\nThe .toml file give your information about your project. Let’s have a look at one I made one my computer before adding any dependencies to my environment. How is it different from the one you’ve made on HPC2N?\n\n\n\n\n\n\nTip\n\n\n\nClick on the numbers in the list below to highlight the corresponding code.\n\n\n1[workspace]\n2channels = [\"conda-forge\", \"bioconda\"]\n3name = \"amrei_pixi_training\"\n4platforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n5[tasks]\n\n6[dependencies]\n\n1\n\nOverview section of the environment.\n\n2\n\nAdded channels - these are the sources we allow for our dependencies/ tools.\n\n3\n\nThe name of the environment.\n\n4\n\nOperating system the environment is optimised for.\n\n5\n\nThis is where we can define tasks - interesting, but not covered in this course.\n\n6\n\nHere will be the list of tool installed within the environment (once we have added them).\n\n\n\n\n\n\n\n\nNote\n\n\n\nYour current platform will be automatically detected and added to the environment. If you want to add different platforms, you add them with the -p linux-64 flag. In this example, you are adding Linux64. See the Pixi docs for the full list of supported platforms.\nIf you are adding a platform that doesn’t natively run on your OS (e.g. adding Linux when running on Windows) be sure to add the OS you are running your system on as well!\n\n\nOnce you have used the environment, or added a dependency/tool to it, you will find yet another file, called pixi.lock.\n\n\npixi.lock\nThe .lock file give you information on the channels you have decided to add, as well as the information on where the packages were downloaded from, license information, md5 information, and more.\n\n\n\n\n\n\nImportant\n\n\n\nDo not delete the .toml or .lock files, or you will break your environment!"
  },
  {
    "objectID": "course_environments.html#adding-dependencies",
    "href": "course_environments.html#adding-dependencies",
    "title": "Introduction to Pixi",
    "section": "Adding Dependencies",
    "text": "Adding Dependencies\nAdding dependencies to the .toml file is telling Pixi to install the specified program for you. However, instead of installing it globally, it only gets installed in the environment.\nTo do this, we use the pixi add function. Let’s try adding Quarto to our environments.\n\n\n\n\n\n\nImportant\n\n\n\nYou must be in the folder of the project to add software!\n\n\npixi add quarto\n\n\n\n\n\n\nTipHow to summon the help function\n\n\n\n\n\nIf you are unsure of how a function works, you can always query it, usually with the --help or -h flags. Here’s how it would look for the pixi add function\npixi add --help\nA general rule of thumb is that a single hyphen - is followed by a single letter flag, while double hyphens -- are usually followed by multi-letter flags\n\n\n\nHere is the pixi.toml we’ve seen earlier after I have added Quarto to my environment. You can see that the dependencies have been updated to include Quarto.\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"amrei_pixi_training\"\nplatforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\nquarto = \"&gt;=1.7.32,&lt;2\""
  },
  {
    "objectID": "course_environments.html#running-a-package",
    "href": "course_environments.html#running-a-package",
    "title": "Introduction to Pixi",
    "section": "Running a Package",
    "text": "Running a Package\nNow that we have an environment with a tool installed, we actually want to use it. For this we have several options:\n\npixi run\nUse pixi to run the tool with the pixi run command.\nLet’s query the help function within Quarto:\npixi run quarto --help\n\n\n\n\n\n\nNote\n\n\n\nTo use other tools, simply substitute quarto with the package you’ve added, and the tool specific commands.\n\n\nAnd just to see that we have not installed Quarto on the server itself, try out running Quarto without Pixi: it will tell you:\nquarto: command not found"
  },
  {
    "objectID": "course_environments.html#pixi-shell",
    "href": "course_environments.html#pixi-shell",
    "title": "Introduction to Pixi",
    "section": "pixi shell",
    "text": "pixi shell\nAlternatively, you can create and “enter” the environment with\npixi shell\nand then interact with the installed dependencies as if they were installed the traditional way:\nquarto --help\n\n\n\n\n\n\nNote\n\n\n\nOnce you are using the pixi shell, you will see that your prompt has changed and is now prefixex with [name_of_environment]. This is how you can check in which environment you operate (and this is why it is important that you name your environments in a way that makes sense).\n\n\nTo exit the environment, type\nexit"
  },
  {
    "objectID": "before_quarto.html",
    "href": "before_quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "When analysing data, it is good practice to work as reproducibleas possible. Part of that is not only developing and executing code (for example in a R session), but to save and comment the code. As always, this documentation step is crucial to understand the code, especially when sharing code with others, or getting back to it at a later point in time.\nLiterate programming combines code and documentation in the same document: the documentation is in plain text, and the code is wrapped in so called chunks, that are executable from within the document.\nThese notebooks that allow for literate programming come in different flavors, for example jupyter notebooksand marimo for Python applications, Rmarkdown for R code. Its successor, quarto can be used to integrate a variety of coding languanges. In this course, we will introduce you to quarto.",
    "crumbs": [
      "Home",
      "Before the course",
      "Quarto"
    ]
  },
  {
    "objectID": "before_quarto.html#literate-programming",
    "href": "before_quarto.html#literate-programming",
    "title": "Quarto",
    "section": "",
    "text": "When analysing data, it is good practice to work as reproducibleas possible. Part of that is not only developing and executing code (for example in a R session), but to save and comment the code. As always, this documentation step is crucial to understand the code, especially when sharing code with others, or getting back to it at a later point in time.\nLiterate programming combines code and documentation in the same document: the documentation is in plain text, and the code is wrapped in so called chunks, that are executable from within the document.\nThese notebooks that allow for literate programming come in different flavors, for example jupyter notebooksand marimo for Python applications, Rmarkdown for R code. Its successor, quarto can be used to integrate a variety of coding languanges. In this course, we will introduce you to quarto.",
    "crumbs": [
      "Home",
      "Before the course",
      "Quarto"
    ]
  },
  {
    "objectID": "before_quarto.html#quarto",
    "href": "before_quarto.html#quarto",
    "title": "Quarto",
    "section": "Quarto",
    "text": "Quarto\n\nAn open-source scientific and technical publishing system.\n\nMeaning that you can use your favorite text editor (such as VScode (o; ) to write documents in plain text markdown.\nQuarto chunks, or code cells, are executable from within the document and can be written in a variety of different languages - such as python, R, Julia, bash, Observable, and more.\nYour document can then be rendered into a variety of different output formats: html, pdf, MS Word, Markdown, and more.\nWith this you can make presentations, dashboards, homepages (like this one), reports, books, manuscripts, and more.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nInstall Quarto, and follow the tutorial for VScode.\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has a very good documentation, that you can access on their homepage -&gt; Guide, or by searching the web for “Quarto, feature you are looking for”.",
    "crumbs": [
      "Home",
      "Before the course",
      "Quarto"
    ]
  },
  {
    "objectID": "before_accounts.html",
    "href": "before_accounts.html",
    "title": "Create a user accounts for the server",
    "section": "",
    "text": "The following procedure takes a few days to get through because of clearance delays, so please start straight away, since every thing else in the course depends on it :)\nWe will run parts of this course on a HighPerformance Computing cluster, HPC2N. The cluster is a local resource in Umeå, and to use it you need a user account both at the National Academic Infrastructure for Super­computing in Sweden (NAISS), and at HPC2N."
  },
  {
    "objectID": "before_accounts.html#create-a-user-account-at-suprnaiss",
    "href": "before_accounts.html#create-a-user-account-at-suprnaiss",
    "title": "Create a user accounts for the server",
    "section": "Create a user account at SUPR/NAISS",
    "text": "Create a user account at SUPR/NAISS\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nFill out & submit account request form. Read carefully and follow the provided instructions.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nUse Register with Federated Identity if you are enrolled in a Swedish university.\nIf you are not enrolled in a Swedish university or if the previous option doesn’t work for you, use instead Register without Federated Identity.\n\n\n\n\nAccept NAISS user agreement\nJust after creating the SUPR account, on the page you are taken to, there will be a section titled “User Agreement” at the very top. Just under it is the button named “Accept NAISS User Agreement”.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nClick it.\n\n\nAlternately, if you do this at a later point. Login to SUPR. At the very top it will say “NAISS User Agreement Pending”.\n\n\n\n\n\n\nTip\n\n\n\n\nUse With SWAMID if you are enrolled in a Swedish university - this option is much faster!\nIf you are not enrolled in a Swedish university use instead Alternative: Offline via Paper Form. Here you will have to print out, sign and send in the form. This may take up to a week. \n\n\n\nWhen you have a SUPR account you can join our course project on HPC2N:"
  },
  {
    "objectID": "before_accounts.html#join-the-course-project-at-hpc2n",
    "href": "before_accounts.html#join-the-course-project-at-hpc2n",
    "title": "Create a user accounts for the server",
    "section": "Join the course project at HPC2N",
    "text": "Join the course project at HPC2N\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLog in to SUPR.\n\nClick “Projects” in the left side column.\nUnder “Requesting Membership in Projects”, put in my name, since I am the PI of the project you wish to join. Click “Search for Project”.\nClick “Request” on the MedBioInfo project.\nWhen I have accepted your membership in the project, go to SUPR again to apply for an account at HPC2N. You will receive an email when your application for membership in a project has been accepted."
  },
  {
    "objectID": "before_accounts.html#create-a-user-account-at-hpc2n",
    "href": "before_accounts.html#create-a-user-account-at-hpc2n",
    "title": "Create a user accounts for the server",
    "section": "Create a user account at HPC2N",
    "text": "Create a user account at HPC2N\nNote: you must be a member of a project before you do this!\n\n\n\n\n\n\nImportantTo do for you\n\n\n\n\nLogin to SUPR.\nClick “Accounts” in the left side column.\nYou can request an account at HPC2N now. Look under the heading “Account Requests”.\nClick “Request account”.\nYour information will then be sent to HPC2N, and you will be taken back to a webpage where you can choose your username.\nUser accounts are usually created once a week. You will get an email from HPC2N when your account has been created."
  },
  {
    "objectID": "before_accounts.html#questions",
    "href": "before_accounts.html#questions",
    "title": "Create a user accounts for the server",
    "section": "Questions?",
    "text": "Questions?\nContact me via Canvas!"
  },
  {
    "objectID": "course_blog.html",
    "href": "course_blog.html",
    "title": "Create a Quarto blog",
    "section": "",
    "text": "For this course we will create a blog in Quarto and publish it with GitHub pages (in the next part). At the end of every day, I want you to take some time to review the day’s topics and write (and publish) a blog entry for each topic.\nThis gives you time to review what we’ve done, and practice literate programming, presentation, and git. You can explore Quarto and it’s possibilities, or add bits and pieces to topics you’re already familiar with.\nAnd at the end of the course you will hand in the blog, and the corresponding repository. But you will also have a resource you can go back to, for yourself, or maybe for teaching purposes.\nLast, but not least, you can use the blog to showcase skills you have mastered (especially if you add to it during your PhD)."
  },
  {
    "objectID": "course_blog.html#how-to-start-a-blog-with-quarto",
    "href": "course_blog.html#how-to-start-a-blog-with-quarto",
    "title": "Create a Quarto blog",
    "section": "How to start a blog with Quarto",
    "text": "How to start a blog with Quarto\nIn VScode, open the command palette:\nView -&gt; Command Palette\nAfter the prompt, &gt;, type:\nQuarto: Create Project\nSelect Blog Project, the location on your computer, and fill in the name."
  },
  {
    "objectID": "course_blog.html#explore-the-blog-and-change-things",
    "href": "course_blog.html#explore-the-blog-and-change-things",
    "title": "Create a Quarto blog",
    "section": "Explore the blog and change things",
    "text": "Explore the blog and change things\n\nRender the blog and explore - where are the posts, what can you change?\nWrite your first blog post: How to make a quarto blog."
  },
  {
    "objectID": "course_blog.html#tips-and-tricks",
    "href": "course_blog.html#tips-and-tricks",
    "title": "Create a Quarto blog",
    "section": "Tips and tricks",
    "text": "Tips and tricks\nWith listings, you can control which posts will be shown where.\nThere are a lot of options for you executable code.\nYou can change the color scheme of the blog.\nLightbox images might be interesting.\nMake use of the categories to sort blog posts into groups.\nCheck other Quarto homepages for cool features!"
  },
  {
    "objectID": "before_screen.html",
    "href": "before_screen.html",
    "title": "Manage persistent bash sessions",
    "section": "",
    "text": "If you work a lot in the console/terminal, especially when working on remote machines/servers via SSH, then you are likely sooner or later to get very annoyed when:\nScreen (and similar programs) allows you to run persistent bash sessions, so that if any of the above happens your bash session on the remote server will just continue. All running processes will proceed in your absence, and you can later re-attach to your session and find things exactly how they would have been had you sat right there, starring at the screen for hours 😎!\nThis is very useful since your re-attached session will continue in the same directory and will have all the project relevant command history saved."
  },
  {
    "objectID": "before_screen.html#practice-a-screen-tutorial",
    "href": "before_screen.html#practice-a-screen-tutorial",
    "title": "Manage persistent bash sessions",
    "section": "Practice a screen tutorial",
    "text": "Practice a screen tutorial\nConnect to the HPC2N login node, enter the screen command in your terminal and practice the following tutorial starting from “2. Name Your Session”.\nAfter the above practice, in your terminal window you can test out the power of screen:\n\nwithin your screen session, run this infinite loop: while true; do date; sleep 1; done\nclose the terminal window running your above split screen session altogether (brutal closing, ie clicking the “close window” garbage can icon on the top right corner of the terminal window in VScode, or closing VScode and your connection to HPC2n completely).\nthen open a fresh terminal window, (be sure you are connected to HPC2N) and re-attach to your still alive and kicking screen session.\nnotice how the date/times have continued to be printed to the screen in the background.\npress CTRL-C to interrupt the loop, then CTRL-A ESC to enter scrolling mode, then you can use the PAGE-UP and PAGE-DOWN keys to observe the activity that took place in screen even whilst you were detached from the screen !\n\nThank-you to Gabriele Pozatti for this section!"
  },
  {
    "objectID": "course_qc.html",
    "href": "course_qc.html",
    "title": "Quality control of sequencing data",
    "section": "",
    "text": "Regardless of what type of bioinformatics project you are working on, you will have to assess whether the data you have is of good enough quality to proceed with - bad input data can seriously impair your ability to draw conclusions from the samples later on!\nPoor quality sequencing data can be caused by a variety of factors, such as sample contamination, improper sample handling, technical problems. Some of these problems will be checked for before and during the sequencing - it is always good to read all documentation coming from the sequencing facility.\nWhen it comes to sequencing data, FastQC is a well known and often used software to check raw reads for their quality."
  },
  {
    "objectID": "course_qc.html#quality-control-tutorial",
    "href": "course_qc.html#quality-control-tutorial",
    "title": "Quality control of sequencing data",
    "section": "Quality Control Tutorial",
    "text": "Quality Control Tutorial\nWithin this tutorial we will\n\nget familiar with the command line.\nuse the job scheduler slurm.\nmake a Pixi environment.\nrun FastQC on some raw sequencing files to practice and to understand the output.\nuse MultiQC to summarize the FastQC output.\n\n\nScreen\nScreen or GNU Screen is a terminal multiplexer. In other words, it means that you can start a screen session and then open any number of windows (virtual terminals) inside that session. Processes running in Screen will continue to run when their window is not visible even if you get disconnected.\nStart a named session\nscreen -S fastqc\nDetach from Linux Screen Session\nYou can detach from the screen session at any time by typing:\nCtrl + a d\nReattach to a Linux Screen\nTo find the session ID list the current running screen sessions with:\nscreen -ls\nTo resume your screen session use the following command:\nscreen -r name"
  },
  {
    "objectID": "course_qc.html#choose-your-adventure",
    "href": "course_qc.html#choose-your-adventure",
    "title": "Quality control of sequencing data",
    "section": "Choose your adventure:",
    "text": "Choose your adventure:\nThe next part - choosing your data - will be different depending on which dataset you want to work with. Please select the tab that applies.\n\n\n\n\n\n\nTip\n\n\n\nSince we will create a lot of output, some we will use in a downstream analysis, I would recommend to get very well organized with a clear system of directories.\nI called my working directory after the type of data (e.g. RNASeq), so I know which project I am working on. Within that directory I have my data in a sub-directory called data. This way, the data is not in the way, and I remember to not alter it.\n\n\n\nRNAseqMetagenomicsWhole genome sequencingproteomicsimage\n\n\nTime series of the induction of human regulatory T cells (Tregs).\nThe dataset is available at the European Nucleotide Archive (ENA) under the accession: PRJNA369563.\nI have downloaded four paired samples for you, and they are available at:\nmedbioinfo2025/common_data/RNAseq\nIf you want to work with this data, make a symbolic link to the data\nln -s path/to/common_data/RNAseq/*fastq.gz data/"
  },
  {
    "objectID": "course_qc.html#data-format",
    "href": "course_qc.html#data-format",
    "title": "Quality control of sequencing data",
    "section": "Data format",
    "text": "Data format\nYou now have the raw sequencing data, in a very common data format, fastq. Let’s have a look at the data (change accordingly):\nzcat data/sample.fastq.gz | head -n 10 \nFor each read we see an entry consisting of four lines. Every new read starts with an @.\n\nFastq format\n\n\n\n\n\n\nLine\nContent\n\n\n\n\n1\nInformation about the read, always starts with an @\n\n\n2\nnucleic sequence of the read\n\n\n3\nstarts with `+`, can (but does not have to) contain data\n\n\n4\ncharacters representing the quality scores with the bases of the read sequence\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe quality score is encoded in ASCII, to represent scores from 0 (high probability of error in calling the base) to 42 (low probability of error in calling the base).\n\n\nWe could now look through the fastq file and scan the quality scores for the reads in our sample one by one. Sounds kind of tedious, doesn’t it? Luckily, here comes FastQC (and other, similar tools)."
  },
  {
    "objectID": "course_qc.html#fastqc",
    "href": "course_qc.html#fastqc",
    "title": "Quality control of sequencing data",
    "section": "FastQC",
    "text": "FastQC\nFastqc is a tool to monitor the quality and the properties of a NGS sequencing file in fastq, SAM and BAM format. It summarizes its findings for a quick visual overview. More information here.\nFastQC will give us an overview of the entire sample, so we won’t have to look at the quality score of every single read in the sample. In addition to the quality scores, FastQC also checks other quality measures.\nYou will now check the quality of the data set you chose.\n\n\n\n\n\n\nTip\n\n\n\nFind someone who is working on the same data set, so you can discuss what you are finding in just your data.\n\n\nNow, first we check if FastQC is installed on the system:\nfastqc --help\nSince it isn’t (bash: fastqc: command not found) we will now make a Pixi environment that contains FastQC.\n\nPixi environment with FastQC\nInitiate a pixi environment, with permitted package sources from conda-forge and bioconda. Then add FastQC to the environment (at the moment we do not care which version, so we do not specify a version). Last, queck that we can run FastQC via Pixi.\npixi init -c conda-forge -c bioconda\npixi add fastqc\npixi run fastqc --help\nThen, activate the environment with:\npixi shell\n\n\nSLURM\nHPC2N is running SLURM (Simple Linux Utility for Resource Management) as its job scheduling system. When you submit a job on the cluster log-in node, SLURM will start, execute and monitor your jobs on the working nodes. It allocates access to the cluster resources, and manages the queue of pending jobs.\nTo be able to do so, SLURM needs a bit of information from us when we submit a job:\n-A: project_ID (to deduct the used computing time from the correct project) -t allocated time dd-hh:mm:ss (to optimize the job queue) -n number of cores (default is one)\nThe basic usage of slurm on the command line is thus:\nsrun -A project_ID -t 30:00 -n 1 &lt;tool options and commands&gt;\nWe want to use FastQC on the samples, so we add the FastQC specific commands:\nsrun -A project_ID -t 15:00 -n 1 fastqc --noextract -o fastqc data data/sample_1.fastq.gz data/sample_2.fastq.gz \nYou will of course have to modify for your project structure.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nNow you can download the .html report from the server (in VScode: right click on the file in the file explorer –&gt; download, otherwise rsync from the local system) and look at them.\n\nWhat is the quality of your sample?\nCan you use it for downstream analyses? Discuss with your neighbour.\nLook at online resources to see if what you see is a problem, or expected because of the type of data you work with.\n\n\n\nNow, srun works fine if you’re working on one sample. But what if you want to run FastQC on many samples? Or if you want to go back and run the command again in a few weeks, will you remember exactly what you ran?"
  },
  {
    "objectID": "course_qc.html#sbatch",
    "href": "course_qc.html#sbatch",
    "title": "Quality control of sequencing data",
    "section": "sbatch",
    "text": "sbatch\nMake a new directory, called scripts. This is where you will house all scripts of your project. Within scripts, touch a new file, fastqc.sh.\nThis is how my project directory looks like at this point:\n.\n├── data\n│   ├── sample1_1.fastq.gz\n│   ├── sample1_2.fastq.gz\n│   ├── sample2_1.fastq.gz\n│   ├── sample2_2.fastq.gz\n├── fastqc\n│   ├── sample1_1_fastqc.html\n│   ├── sample1_1_fastqc.zip\n│   ├── sample1_2_fastqc.html\n│   └── sample1_2_fastqc.zip\n├── pixi.lock\n├── pixi.toml\n└── scripts\n    └── fastqc.sh\nCopy the following into the fastqc.sh file, and save the contents. Read through the file and try to understand what the different lines are doing.\n#! /bin/bash -l\n#SBATCH -A project_ID\n#SBATCH -t 30:00\n#SBATCH -n 1\n\nfastqc -o ../fastqc --noextract ../data/*fastq.gz\nSave and run within your Pixi environment:\nsbatch fastqc.sh \nCheck your job with:\nsqueue -u &lt;user-name&gt;\nAfter running a bash script you will get a slurm output file in the directory you submitted your job from. Look at that output. See if you understand what that output contains.\nless slurm-&lt;jobID&gt;.out\nNow you have a .html file for each sample, which is fine for a few samples, but gets tedious when running a project with many samples. So let’s go and summarize them with MultiQC."
  },
  {
    "objectID": "course_qc.html#multiqc",
    "href": "course_qc.html#multiqc",
    "title": "Quality control of sequencing data",
    "section": "MultiQC",
    "text": "MultiQC\nMulti-QC summarises the output of a lot of different tools. Only having run FastQC it might not seem powerful, but at the end of workflows it is really nice to have a program go through all the files and summarize them, saving you the hassle.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nSet up and run MultiQC on your FastQC output. Things to think about:\n\nadd MultiQC to the Pixi environment\nwrite a sbatch script, just because you can\nsave the output in a separate directory\n\nOnce you have run MultiQC, go through the report and understand what it says about your data."
  },
  {
    "objectID": "course_qc.html#notes-tips-and-tricks",
    "href": "course_qc.html#notes-tips-and-tricks",
    "title": "Quality control of sequencing data",
    "section": "Notes, tips and tricks",
    "text": "Notes, tips and tricks\nRNAseq failed per base sequence content\nCheck your running jobs:\nsqueue -u &lt;user_name&gt;\nCheck the projects you are a member of:\nprojinfo"
  }
]
[
  {
    "objectID": "course_qc_container.html",
    "href": "course_qc_container.html",
    "title": "Quality control with containers",
    "section": "",
    "text": "Now, let‚Äôs do some more quality control (because we already know how to do it, and can concentrate on the container part), and run the tools with containers, and not (only) with the Pixi environment.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nRun FastQC and MultiQC on the data, using containers. Run MultiQC from within the container AND the outside.\n\n\n\n\n\n\n\n\nTip\n\n\n\nGet together with a neighbour and think about setting up this ‚Äúproject‚Äù. What do you need?\n\nScreen session?\nData directory with symbolic links?\nPixi environment?\nWhere to get the container?\n‚Ä¶\n\nMaybe use a different data set this time, to mix it up a little? Remember to make symbolic links to the data!",
    "crumbs": [
      "Home",
      "Working with containers",
      "QC with Apptainer"
    ]
  },
  {
    "objectID": "course_qc.html",
    "href": "course_qc.html",
    "title": "Quality control of sequencing data",
    "section": "",
    "text": "Regardless of what type of bioinformatics project you are working on, you will have to assess whether the data you have is of good enough quality to proceed with - bad input data can seriously impair your ability to draw conclusions from the samples later on!\nPoor quality sequencing data can be caused by a variety of factors, such as sample contamination, improper sample handling, technical problems. Some of these problems will be checked for before and during the sequencing - it is always good to read all documentation coming from the sequencing facility.\nWhen it comes to sequencing data, FastQC is a well known and often used software to check raw reads for their quality.",
    "crumbs": [
      "Home",
      "Working with environments",
      "QC with Pixi"
    ]
  },
  {
    "objectID": "course_qc.html#quality-control-tutorial",
    "href": "course_qc.html#quality-control-tutorial",
    "title": "Quality control of sequencing data",
    "section": "Quality Control Tutorial",
    "text": "Quality Control Tutorial\nWithin this tutorial we will\n\nget familiar with the command line.\nuse the job scheduler slurm.\nmake a Pixi environment.\nrun FastQC on some raw sequencing files to practice and to understand the output.\nuse MultiQC to summarize the FastQC output.\n\n\nScreen\nScreen or GNU Screen is a terminal multiplexer. In other words, it means that you can start a screen session and then open any number of windows (virtual terminals) inside that session. Processes running in Screen will continue to run when their window is not visible even if you get disconnected.\nStart a named session\nscreen -S fastqc\nDetach from Linux Screen Session\nYou can detach from the screen session at any time by typing:\nCtrl + a d\nReattach to a Linux Screen\nTo find the session ID list the current running screen sessions with:\nscreen -ls\nTo resume your screen session use the following command:\nscreen -r fastqc",
    "crumbs": [
      "Home",
      "Working with environments",
      "QC with Pixi"
    ]
  },
  {
    "objectID": "course_qc.html#choose-your-adventure",
    "href": "course_qc.html#choose-your-adventure",
    "title": "Quality control of sequencing data",
    "section": "Choose your adventure:",
    "text": "Choose your adventure:\nThe next part - choosing your data - will be different depending on which dataset you want to work with. Please select the tab that applies.\n\n\n\n\n\n\nTip\n\n\n\nSince we will create a lot of output, some we will use in a downstream analysis, I would recommend to get very well organized with a clear system of directories.\nI called my working directory after the type of data (e.g.¬†RNASeq), so I know which project I am working on. Within that directory I have my data in a sub-directory called data. This way, the data is not in the way, and I remember to not alter it.\n\n\n\nRNAseqMetagenomicsWhole genome sequencing\n\n\nTime series of the induction of human regulatory T cells (Tregs).\nThe dataset is available at the European Nucleotide Archive (ENA) under the accession: PRJNA369563.\nI have downloaded four paired samples for you, and they are available at:\nmedbioinfo2025/common_data/RNAseq\nIf you want to work with this data, make a symbolic link to the data, do this from within a subdirectory called data(or something similar). You want to avoid mingling your data with the analyses:\nln -s path/to/common_data/RNAseq/*fastq.gz .\n\n\nThis metagenomics data set is a soil sample taken in the Argentinian pampa, for which the 16S rDNA V4 region has been sequenced using 454 GS FLX Titanium. This allows us to analyze the genetic variation the 16S rDNA, which is present in all living organisms.\nThe dataset is available at the European Nucleotide Archive (ENA) under the accession: PRJNA178180.\nI have downloaded two samples for you, and they are available at:\nmedbioinfo2025/common_data/metagenomics\nIf you want to work with this data, make a symbolic link to the data, do this from within a subdirectory called data(or something similar). You want to avoid mingling your data with the analyses:\nln -s path/to/common_data/metagenomics/*fastq.gz .\n\n\nThis data set contains population genomics data of the Shiitake mushroom, Lentinula edodes, which was sequenced with Illumina, using whole genome sequencing.\nThis data set is available at the European Nucleotide Archive (ENA) under the accession: PRJNA535345.\nI have downloaded four paired end samples for you, which are available at:\nmedbioinfo2025/common_data/wgs\nIf you want to work with this data, make a symbolic link to the data, do this from within a subdirectory called data(or something similar). You want to avoid mingling your data with the analyses:\nln -s path/to/common_data/wgs/*fastq.gz .",
    "crumbs": [
      "Home",
      "Working with environments",
      "QC with Pixi"
    ]
  },
  {
    "objectID": "course_qc.html#data-format",
    "href": "course_qc.html#data-format",
    "title": "Quality control of sequencing data",
    "section": "Data format",
    "text": "Data format\nYou now have the raw sequencing data, in a very common data format, fastq. Let‚Äôs have a look at the data (change accordingly):\nzcat data/sample.fastq.gz | head -n 10 \nFor each read we see an entry consisting of four lines. Every new read starts with an @.\n\nFastq format\n\n\n\n\n\n\nLine\nContent\n\n\n\n\n1\nInformation about the read, always starts with an @\n\n\n2\nnucleic sequence of the read\n\n\n3\nstarts with `+`, can (but does not have to) contain data\n\n\n4\ncharacters representing the quality scores with the bases of the read sequence\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe quality score is encoded in ASCII, to represent scores from 0 (high probability of error in calling the base) to 42 (low probability of error in calling the base).\n\n\nWe could now look through the fastq file and scan the quality scores for the reads in our sample one by one. Sounds kind of tedious, doesn‚Äôt it? Luckily, here comes FastQC (and other, similar tools).",
    "crumbs": [
      "Home",
      "Working with environments",
      "QC with Pixi"
    ]
  },
  {
    "objectID": "course_qc.html#fastqc",
    "href": "course_qc.html#fastqc",
    "title": "Quality control of sequencing data",
    "section": "FastQC",
    "text": "FastQC\nFastqc is a tool to monitor the quality and the properties of a NGS sequencing file in fastq, SAM and BAM format. It summarizes its findings for a quick visual overview. More information here.\nFastQC will give us an overview of the entire sample, so we won‚Äôt have to look at the quality score of every single read in the sample. In addition to the quality scores, FastQC also checks other quality measures.\nYou will now check the quality of the data set you chose.\n\n\n\n\n\n\nTip\n\n\n\nFind someone who is working on the same data set, so you can discuss what you are finding in just your data.\n\n\nNow, first we check if FastQC is installed on the system:\nfastqc --help\nSince it isn‚Äôt (bash: fastqc: command not found) we will now make a Pixi environment that contains FastQC.\n\nPixi environment with FastQC\nInitiate a pixi environment, with permitted package sources from conda-forge and bioconda. Then add FastQC to the environment (at the moment we do not care which version, so we do not specify a version). Last, queck that we can run FastQC via Pixi.\npixi init -c conda-forge -c bioconda\npixi add fastqc\npixi run fastqc --help\n\n\n\n\n\n\nImportant\n\n\n\nThe order of the channels matters. Add conda-forge first, then bioconda.\n\n\nThen, activate the environment with:\npixi shell\n\n\nSLURM\nHPC2N is running SLURM (Simple Linux Utility for Resource Management) as its job scheduling system. When you submit a job on the cluster log-in node, SLURM will start, execute and monitor your jobs on the working nodes. It allocates access to the cluster resources, and manages the queue of pending jobs.\nTo be able to do so, SLURM needs a bit of information from us when we submit a job:\n-A: project_ID (to deduct the used computing time from the correct project) -t allocated time dd-hh:mm:ss (to optimize the job queue) -n number of cores (default is one)\nThe basic usage of slurm on the command line is thus:\nsrun -A project_ID -t 30:00 -n 1 &lt;tool options and commands&gt;\nWe want to use FastQC on the samples, so we add the FastQC specific commands:\nsrun -A project_ID -t 15:00 -n 1 fastqc --noextract -o fastqc data data/sample_1.fastq.gz data/sample_2.fastq.gz \nYou will of course have to modify for your project structure.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nNow you can download the .html report from the server (in VScode: right click on the file in the file explorer ‚Äì&gt; download, otherwise rsync from the local system) and look at them.\n\nWhat is the quality of your sample?\nCan you use it for downstream analyses? Discuss with your neighbour.\nLook at online resources to see if what you see is a problem, or expected because of the type of data you work with. A nice resource here is for example the FastQC tutorial from the Michigan State University.\n\n\n\nNow, srun works fine if you‚Äôre working on one sample. But what if you want to run FastQC on many samples? Or if you want to go back and run the command again in a few weeks, will you remember exactly what you ran?",
    "crumbs": [
      "Home",
      "Working with environments",
      "QC with Pixi"
    ]
  },
  {
    "objectID": "course_qc.html#sbatch",
    "href": "course_qc.html#sbatch",
    "title": "Quality control of sequencing data",
    "section": "sbatch",
    "text": "sbatch\nMake a new directory, called scripts. This is where you will house all scripts of your project. Within scripts, touch a new file, fastqc.sh.\nThis is how my project directory looks like at this point:\n.\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ sample1_1.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ sample1_2.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ sample2_1.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ sample2_2.fastq.gz\n‚îú‚îÄ‚îÄ fastqc\n‚îÇ   ‚îú‚îÄ‚îÄ sample1_1_fastqc.html\n‚îÇ   ‚îú‚îÄ‚îÄ sample1_1_fastqc.zip\n‚îÇ   ‚îú‚îÄ‚îÄ sample1_2_fastqc.html\n‚îÇ   ‚îî‚îÄ‚îÄ sample1_2_fastqc.zip\n‚îú‚îÄ‚îÄ pixi.lock\n‚îú‚îÄ‚îÄ pixi.toml\n‚îî‚îÄ‚îÄ scripts\n    ‚îî‚îÄ‚îÄ fastqc.sh\nCopy the following into the fastqc.sh file, and save the contents. Read through the file and try to understand what the different lines are doing.\n#! /bin/bash -l\n#SBATCH -A project_ID\n#SBATCH -t 30:00\n#SBATCH -n 1\n\nfastqc -o ../fastqc --noextract ../data/*fastq.gz\nSave and run within your Pixi environment:\nsbatch fastqc.sh \nCheck your job with:\nsqueue -u &lt;user-name&gt;\nAfter running a bash script you will get a slurm output file in the directory you submitted your job from. Look at that output. See if you understand what that output contains.\nless slurm-&lt;jobID&gt;.out\nNow you have a .html file for each sample, which is fine for a few samples, but gets tedious when running a project with many samples. So let‚Äôs go and summarize them with MultiQC.",
    "crumbs": [
      "Home",
      "Working with environments",
      "QC with Pixi"
    ]
  },
  {
    "objectID": "course_qc.html#multiqc",
    "href": "course_qc.html#multiqc",
    "title": "Quality control of sequencing data",
    "section": "MultiQC",
    "text": "MultiQC\nMulti-QC summarises the output of a lot of different tools. Only having run FastQC it might not seem powerful, but at the end of workflows it is really nice to have a program go through all the files and summarize them, saving you the hassle.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nSet up and run MultiQC on your FastQC output. Things to think about:\n\nadd MultiQC to the Pixi environment\nwrite a sbatch script, just because you can\nsave the output in a separate directory\n\nOnce you have run MultiQC, go through the report and understand what it says about your data.",
    "crumbs": [
      "Home",
      "Working with environments",
      "QC with Pixi"
    ]
  },
  {
    "objectID": "course_qc.html#notes-tips-and-tricks",
    "href": "course_qc.html#notes-tips-and-tricks",
    "title": "Quality control of sequencing data",
    "section": "Notes, tips and tricks",
    "text": "Notes, tips and tricks\nRNAseq failed per base sequence content\nCheck your running jobs:\nsqueue -u &lt;user_name&gt;\nCheck the projects you are a member of:\nprojinfo",
    "crumbs": [
      "Home",
      "Working with environments",
      "QC with Pixi"
    ]
  },
  {
    "objectID": "before_screen.html",
    "href": "before_screen.html",
    "title": "Manage persistent bash sessions",
    "section": "",
    "text": "If you work a lot in the console/terminal, especially when working on remote machines/servers via SSH, then you are likely sooner or later to get very annoyed when:\nScreen (and similar programs) allows you to run persistent bash sessions, so that if any of the above happens your bash session on the remote server will just continue. All running processes will proceed in your absence, and you can later re-attach to your session and find things exactly how they would have been had you sat right there, starring at the screen for hours üòé!\nThis is very useful since your re-attached session will continue in the same directory and will have all the project relevant command history saved.",
    "crumbs": [
      "Home",
      "Preparations",
      "screen session"
    ]
  },
  {
    "objectID": "before_screen.html#practice-a-screen-tutorial",
    "href": "before_screen.html#practice-a-screen-tutorial",
    "title": "Manage persistent bash sessions",
    "section": "Practice a screen tutorial",
    "text": "Practice a screen tutorial\nConnect to the HPC2N login node, enter the screen command in your terminal and practice the following tutorial starting from ‚Äú2. Name Your Session‚Äù.\nAfter the above practice, in your terminal window you can test out the power of screen:\n\nwithin your screen session, run this infinite loop: while true; do date; sleep 1; done\nclose the terminal window running your above split screen session altogether (brutal closing, ie clicking the ‚Äúclose window‚Äù garbage can icon on the top right corner of the terminal window in VScode, or closing VScode and your connection to HPC2n completely).\nthen open a fresh terminal window, (be sure you are connected to HPC2N) and re-attach to your still alive and kicking screen session.\nnotice how the date/times have continued to be printed to the screen in the background.\npress CTRL-C to interrupt the loop, then CTRL-A ESC to enter scrolling mode, then you can use the PAGE-UP and PAGE-DOWN keys to observe the activity that took place in screen even whilst you were detached from the screen !\n\nThank-you to Gabriele Pozatti for this section!",
    "crumbs": [
      "Home",
      "Preparations",
      "screen session"
    ]
  },
  {
    "objectID": "course_nextflow_rnaseq.html",
    "href": "course_nextflow_rnaseq.html",
    "title": "Nextflow RNAseq pipeline",
    "section": "",
    "text": "To demonstrate a real-world biomedical scenario, we will implement a proof of concept RNA-Seq workflow which:\n\nIndexes a transcriptome file\nPerforms quality controls\nPerforms quantification\nCreates a MultiQC report\n\nThis will be done using a series of seven scripts, each of which builds on the previous to create a complete workflow. You can find these in the tutorial folder (script1.nf - script6.nf). These scripts will make use of third-party tools that are known by bioinformaticians but that may be new to you so we‚Äôll briefly introduce them below.\n\nSalmon is a tool for quantifying molecules known as transcripts through a type of data called RNA-seq data.\nFastQC is a tool to perform quality control for high throughput sequence data. You can think of it as a way to assess the quality of your data.\nMultiQC searches a given directory for analysis logs and compiles a HTML report. It‚Äôs a general use tool, perfect for summarizing the output from numerous bioinformatics tools.\n\nThough these tools may not be the ones you will use in your pipeline, they could just be replaced by any common tool of your area. That‚Äôs the power of Nextflow!",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow RNAseq"
    ]
  },
  {
    "objectID": "course_nextflow_rnaseq.html#simple-rna-seq-workflow-from-seqera-training-materials",
    "href": "course_nextflow_rnaseq.html#simple-rna-seq-workflow-from-seqera-training-materials",
    "title": "Nextflow RNAseq pipeline",
    "section": "",
    "text": "To demonstrate a real-world biomedical scenario, we will implement a proof of concept RNA-Seq workflow which:\n\nIndexes a transcriptome file\nPerforms quality controls\nPerforms quantification\nCreates a MultiQC report\n\nThis will be done using a series of seven scripts, each of which builds on the previous to create a complete workflow. You can find these in the tutorial folder (script1.nf - script6.nf). These scripts will make use of third-party tools that are known by bioinformaticians but that may be new to you so we‚Äôll briefly introduce them below.\n\nSalmon is a tool for quantifying molecules known as transcripts through a type of data called RNA-seq data.\nFastQC is a tool to perform quality control for high throughput sequence data. You can think of it as a way to assess the quality of your data.\nMultiQC searches a given directory for analysis logs and compiles a HTML report. It‚Äôs a general use tool, perfect for summarizing the output from numerous bioinformatics tools.\n\nThough these tools may not be the ones you will use in your pipeline, they could just be replaced by any common tool of your area. That‚Äôs the power of Nextflow!",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow RNAseq"
    ]
  },
  {
    "objectID": "course_nextflow_rnaseq.html#preparations",
    "href": "course_nextflow_rnaseq.html#preparations",
    "title": "Nextflow RNAseq pipeline",
    "section": "Preparations",
    "text": "Preparations\nIf you haven‚Äôt done so yet, copy the directory ‚Äúnf-training‚Äù from the common_data directory into your personal directory. Change directory into it.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow RNAseq"
    ]
  },
  {
    "objectID": "course_nextflow_rnaseq.html#set-the-executor",
    "href": "course_nextflow_rnaseq.html#set-the-executor",
    "title": "Nextflow RNAseq pipeline",
    "section": "Set the executor",
    "text": "Set the executor\nSo far we have used the local executor for our scripts, which is a big no no on clusters since that uses the log-in node for the execution. The pipeline we are building up now is a bit more complicated and requires more computing power than string manipulation, so it is time to set slurm as the executor. This can be done in a file called nextflow.config. I have prepared that file for you: locate the file nextflow.config in your folder and replace its content with the following:\nprocess{\n    executor = \"slurm\"\n    time = '2.h'  // Set default time limit for all processes\n    \n    // Other settings\n    cpus = 4\n}\n\nresume = true\nsingularity.enabled = true\n\nexecutor {\n    account = 'your_project_account'\n}\n\nHere, we set the executor for every process to slurm.\nWe set resume to true, so it is automatically used for all our runs (which means we do not have to specify this in our nextflow run command anymore).\nWe enable singularity and set some singularity run options.\nLastly, we specify the account name for the slurm execution.\n\nThis nextflow.config file is implicitly called when we execute nextflow from the folder it is in. We do not have to explicitely name it in the nextflow run command.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow RNAseq"
    ]
  },
  {
    "objectID": "course_nextflow_rnaseq.html#define-the-workflow-parameters",
    "href": "course_nextflow_rnaseq.html#define-the-workflow-parameters",
    "title": "Nextflow RNAseq pipeline",
    "section": "Define the workflow parameters",
    "text": "Define the workflow parameters\nParameters are inputs and options that can be changed when the workflow is run.\nThe script script1.nf defines the workflow input parameters:\nparams.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\n\nprintln \"reads: $params.reads\"\nRun it by using the following command:\npixi run nextflow run script1.nf\n\n\n\n\n\n\nNote\n\n\n\nNote that we do not need to specify that we want to use the nextflow.config file. Since it is in the same directory it will be used automatically.\n\n\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nModify script1.nf by adding a fourth parameter named outdir and set it to a default path that will be used as the workflow output directory.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow RNAseq"
    ]
  },
  {
    "objectID": "course_nextflow_rnaseq.html#create-a-transcriptome-index-file",
    "href": "course_nextflow_rnaseq.html#create-a-transcriptome-index-file",
    "title": "Nextflow RNAseq pipeline",
    "section": "Create a transcriptome index file",
    "text": "Create a transcriptome index file\nNextflow allows the execution of any command or script by using a process definition.\nA process is defined by providing three main declarations: the process input, output, and the command script.\nTo add a transcriptome INDEX processing step, add the following code blocks to your script1.nf. Alternatively, these code blocks have already been added to script2.nf.\nprocess INDEX {\n    input:\n    path transcriptome\n\n    output:\n    path 'salmon_index'\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\nAdditionally, add a workflow scope containing an input channel definition and the index process:\nworkflow {\n    index_ch = INDEX(file(params.transcriptome_file, checkIfExists: true))\n}\nHere, the params.transcriptome_file parameter is used as the input for the INDEX process. The INDEX process (using the salmon tool) creates salmon_index, an indexed transcriptome that is passed as an output to the index_ch channel.\nRun with:\npixi run nextflow run script1.nf \n\n\n\n\n\n\nCaution\n\n\n\nWhy doesn‚Äôt this work? What have we forgotten?\n\n\n\nadd Salmon to your environment\npixi add salmon\n\n\nadd Salmon in a container to the process\nAdding a container to the process and running the tool via the container is more reproducible than adding it to your environment. Use the container directive and add the URL to the container to the process scope:\ncontainer 'https://depot.galaxyproject.org/singularity/salmon:1.10.1--h7e5ed60_0'\n\n\n\n\n\n\nNote\n\n\n\nYou can also specify a process specific output directory in the process block:\npublishDir \"$params.outdir/salmon\"\nThis command line would typically be located just below the container directive.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow RNAseq"
    ]
  },
  {
    "objectID": "course_nextflow_rnaseq.html#define-run-time-environment",
    "href": "course_nextflow_rnaseq.html#define-run-time-environment",
    "title": "Nextflow RNAseq pipeline",
    "section": "Define run time environment",
    "text": "Define run time environment\nRemember how Nextflow is big on execution abstraction? This is where we will put this concept into practice. In the nextflow.config add the following just before the closing squiggly bracket from the process definitions:\n    withName:'INDEX'{\n        time = 15.m\n        cpus = 2\n    }\nWith this we specify the alloted time and cpus for this specific process.\nTo specify memory add mem = xx.GB - check with your cluster to see how much memory is available and how to specify it. In our case we don‚Äôt need to because the memory implicit in the 2 cpus is enough for our purposes.\npixi run nextflow run script1.nf \nCollect read files by pairs\nThis step shows how to match read files into pairs, so they can be mapped by Salmon.\nRun script3.nf:\npixi run nextflow run script3.nf\nNow add\nread_pairs_ch.view()\nto the bottom of the script, save it and run it again.\nIt will print something similar to this:\n[gut, [/.../data/ggal/gut_1.fq, /.../data/ggal/gut_2.fq]]\nThe above example shows how the read_pairs_ch channel emits tuples composed of two elements, where the first is the read pair prefix and the second is a list representing the actual files.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow RNAseq"
    ]
  },
  {
    "objectID": "course_nextflow_rnaseq.html#expression-quantification",
    "href": "course_nextflow_rnaseq.html#expression-quantification",
    "title": "Nextflow RNAseq pipeline",
    "section": "Expression quantification",
    "text": "Expression quantification\nLook at the file ‚Äúscript4.nf‚Äù. This script adds a gene expression QUANTIFICATION process and a call to it within the workflow scope. Quantification requires the index transcriptome and RNA-Seq read pair fastq files.\nIn the workflow scope, note how the index_ch channel is assigned as output in the INDEX process.\nNext, note that the first input channel for the QUANTIFICATION process is the previously declared index_ch, which contains the path to the salmon_index.\nAlso, note that the second input channel for the QUANTIFICATION process, is the read_pair_ch we just created. This being a tuple** composed of two elements (a value: ‚Äúsample_id‚Äù and a list of paths to the fastq reads: ‚Äúreads‚Äù) in order to match the structure of the items emitted by the fromFilePairs channel factory.\n**tuple: a data structure consisting of multiple parts.\npixi run nextflow run script4.nf\nExecute the same script again with more read files, as shown below:\npixi run nextflow run script4.nf --reads 'data/ggal/*_{1,2}.fq'\nYou will notice that the QUANTIFICATION process is executed multiple times.\nNextflow parallelizes the execution of your workflow simply by providing multiple sets of input data to your script.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nAdd the process specific runtime environment definition for QUANTIFICATION to nextflow.config.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow RNAseq"
    ]
  },
  {
    "objectID": "course_nextflow_rnaseq.html#quality-control",
    "href": "course_nextflow_rnaseq.html#quality-control",
    "title": "Nextflow RNAseq pipeline",
    "section": "Quality control",
    "text": "Quality control\nNext, we implement a FASTQC quality control step for your input reads (using the label fastqc). The inputs are the same as the read pairs used in the QUANTIFICATION step.\nYou can run it by using the following command:\npixi run nextflow run script5.nf\nafter adding the required containers (and publish directory) again.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\n\nWhere can you get a FastQC container?\nHow do you add it to the process?\n\n\n\nNextflow DSL2 knows to split the reads_pair_ch into two identical channels as they are required twice as an input for both of the FASTQC and the QUANTIFICATION process.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow RNAseq"
    ]
  },
  {
    "objectID": "course_nextflow_rnaseq.html#multiqc-report",
    "href": "course_nextflow_rnaseq.html#multiqc-report",
    "title": "Nextflow RNAseq pipeline",
    "section": "MultiQC report",
    "text": "MultiQC report\nThis step collects the outputs from the QUANTIFICATION and FASTQC processes to create a final report using the MultiQC tool.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLook at script.6.nf. What does the new process do?\n\n\nAdd the containers:\n    container 'https://depot.galaxyproject.org/singularity/multiqc:1.14--pyhdfd78af_0'\n    publishDir \"$params.outdir/multiqc\", mode:'copy'\nAnd run on all samples with the following command:\npixi run nextflow run script6.nf --reads 'data/ggal/*_{1,2}.fq' \nIt creates the final report in the results folder in the current work directory.\nIn this script, note the use of the mix and collect operators chained together to gather the outputs of the QUANTIFICATION and FASTQC processes as a single input. Operators can be used to combine and transform channels:\nMULTIQC(quant_ch.mix(fastqc_ch).collect()) \nWe only want one task of MultiQC to be executed to produce one report. Therefore, we use the mix channel operator to combine the two channels followed by the collect operator, to return the complete channel contents as a single element.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nTake a breather!",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow RNAseq"
    ]
  },
  {
    "objectID": "course_blog.html",
    "href": "course_blog.html",
    "title": "Create a Quarto blog",
    "section": "",
    "text": "For this course we will create a blog in Quarto and publish it with GitHub pages (in the next part). At the end of every day, I want you to take some time to review the day‚Äôs topics and write (and publish) a blog entry for each topic.\nThis gives you time to review what we‚Äôve done, and practice literate programming, presentation, and git. You can explore Quarto and it‚Äôs possibilities, or add bits and pieces to topics you‚Äôre already familiar with.\nAnd at the end of the course you will hand in the blog, and the corresponding repository. But you will also have a resource you can go back to, for yourself, or maybe for teaching purposes.\nLast, but not least, you can use the blog to showcase skills you have mastered (especially if you add to it during your PhD).",
    "crumbs": [
      "Home",
      "Version control & documentation",
      "Quarto blog"
    ]
  },
  {
    "objectID": "course_blog.html#how-to-start-a-blog-with-quarto",
    "href": "course_blog.html#how-to-start-a-blog-with-quarto",
    "title": "Create a Quarto blog",
    "section": "How to start a blog with Quarto",
    "text": "How to start a blog with Quarto\nIn VScode, open the command palette:\nView -&gt; Command Palette\nAfter the prompt, &gt;, type:\nQuarto: Create Project\nSelect Blog Project, the location on your computer, and fill in the name.",
    "crumbs": [
      "Home",
      "Version control & documentation",
      "Quarto blog"
    ]
  },
  {
    "objectID": "course_blog.html#explore-the-blog-and-change-things",
    "href": "course_blog.html#explore-the-blog-and-change-things",
    "title": "Create a Quarto blog",
    "section": "Explore the blog and change things",
    "text": "Explore the blog and change things\n\nRender the blog and explore - where are the posts, what can you change?\nWrite your first blog post: How to make a quarto blog.",
    "crumbs": [
      "Home",
      "Version control & documentation",
      "Quarto blog"
    ]
  },
  {
    "objectID": "course_blog.html#tips-and-tricks",
    "href": "course_blog.html#tips-and-tricks",
    "title": "Create a Quarto blog",
    "section": "Tips and tricks",
    "text": "Tips and tricks\nWith listings, you can control which posts will be shown where.\nThere are a lot of options for you executable code.\nYou can change the color scheme of the blog.\nLightbox images might be interesting.\nMake use of the categories to sort blog posts into groups.\nCheck other Quarto homepages for cool features!",
    "crumbs": [
      "Home",
      "Version control & documentation",
      "Quarto blog"
    ]
  },
  {
    "objectID": "course_git.html",
    "href": "course_git.html",
    "title": "Using git collaboratively",
    "section": "",
    "text": "This module will be run by Sam, and he has his own slides and content on Canvas.",
    "crumbs": [
      "Home",
      "Version control & documentation",
      "git pull requests"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This course is a hosted by the Swedish Agricultural University‚Äôs Bioinformatics Infrastructure (SLUBI).\nIn this course we hope to give you information on how to use reproducible bioinformatics pipelines, report results in a streamlined manner, and implement the system in your own research.\nFeedback is appreciated!\n(This is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites.)"
  },
  {
    "objectID": "course_intro.html#hello-and-welcome-to-the-course",
    "href": "course_intro.html#hello-and-welcome-to-the-course",
    "title": "Personal course homepage",
    "section": "Hello and welcome to the course!",
    "text": "Hello and welcome to the course!\nToday we will talk about the why‚Äôs and how‚Äôs of this course, and you will set up your own course homepage. This homepage will be your personal learning space throughout the course. Each evening, you‚Äôll write a short blog post to reflect on what you‚Äôve learned during the day. Some days, you might write several posts if you have a lot to share. These daily reflections will help you keep track of your progress, deepen your understanding, and build a clear record of your learning, as well as a habit of documentation.",
    "crumbs": [
      "Home",
      "Why this course?",
      "Personal course homepage"
    ]
  },
  {
    "objectID": "course_intro.html#your-homepage-and-github",
    "href": "course_intro.html#your-homepage-and-github",
    "title": "Personal course homepage",
    "section": "Your Homepage and GitHub",
    "text": "Your Homepage and GitHub\nYour Quarto homepage will be version-controlled using Git and GitHub. By the end of the course, you will share both the URL of your GitHub repository and the URL of your published homepage. This way I can review your work, follow your progress, and see how your site has developed over time.",
    "crumbs": [
      "Home",
      "Why this course?",
      "Personal course homepage"
    ]
  },
  {
    "objectID": "course_intro.html#good-git-practices",
    "href": "course_intro.html#good-git-practices",
    "title": "Personal course homepage",
    "section": "Good Git Practices",
    "text": "Good Git Practices\nThroughout the course, I expect you to follow good Git practices. This means writing clear and informative commit messages, keeping each commit focused on one topic or change, and committing regularly rather than saving everything for the end. These habits are an essential part of professional software development and will make your project much easier to follow and maintain.",
    "crumbs": [
      "Home",
      "Why this course?",
      "Personal course homepage"
    ]
  },
  {
    "objectID": "course_intro.html#writing-your-daily-blog",
    "href": "course_intro.html#writing-your-daily-blog",
    "title": "Personal course homepage",
    "section": "Writing Your Daily Blog",
    "text": "Writing Your Daily Blog\nYour daily posts do not have to be long or perfect. The goal is to build a habit of documentation and communication. Use simple language, write honestly about what you‚Äôve learned, and don‚Äôt be afraid to include code, images, or links to illustrate your thoughts. Over time, these posts will form a valuable record of your growth and achievements during the course - and maybe later?",
    "crumbs": [
      "Home",
      "Why this course?",
      "Personal course homepage"
    ]
  },
  {
    "objectID": "course_environments.html",
    "href": "course_environments.html",
    "title": "Introduction to Pixi",
    "section": "",
    "text": "Different operating systems bring different challenges to bioinformatics. Windows, for instance, doesn‚Äôt really support most bioinformatics tools, or your computer might run a different version of a specific tool than mine. One way to solve this, and make research more reproducible, is through the use of environments. Bioinformatics environments specify the tools neded for the task at hand, and environment managers install these tools with all their dependencies. There are many different kinds of environment managers, and for this course we are going to use Pixi.\nThere are, of course, other ways to solve tool access and compatibility issues, such as running virtual machines. In our experience, however, environments are a bit easier to manage and are more portable across different systems and users.\n\n\n\nPixi landing page\n\n\nAs you can see, you can run Pixi on all major operating systems, and you can include various platforms in your Pixi environment.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLog in to the course server and follow along:\n\n\n\n\nInstalling Pixi is really easy and described thoroughly here with separate installation guides for Windows and Mac/Linux.\n\n\n\n\n\n\nTip\n\n\n\nTo source your shell, you need to source the startup files, in Linux it‚Äôs the ~/.bashrc file, in Mac it‚Äôs the ~/.zshrc file.",
    "crumbs": [
      "Home",
      "Working with environments",
      "Pixi"
    ]
  },
  {
    "objectID": "course_environments.html#introduction",
    "href": "course_environments.html#introduction",
    "title": "Introduction to Pixi",
    "section": "",
    "text": "Different operating systems bring different challenges to bioinformatics. Windows, for instance, doesn‚Äôt really support most bioinformatics tools, or your computer might run a different version of a specific tool than mine. One way to solve this, and make research more reproducible, is through the use of environments. Bioinformatics environments specify the tools neded for the task at hand, and environment managers install these tools with all their dependencies. There are many different kinds of environment managers, and for this course we are going to use Pixi.\nThere are, of course, other ways to solve tool access and compatibility issues, such as running virtual machines. In our experience, however, environments are a bit easier to manage and are more portable across different systems and users.\n\n\n\nPixi landing page\n\n\nAs you can see, you can run Pixi on all major operating systems, and you can include various platforms in your Pixi environment.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLog in to the course server and follow along:\n\n\n\n\nInstalling Pixi is really easy and described thoroughly here with separate installation guides for Windows and Mac/Linux.\n\n\n\n\n\n\nTip\n\n\n\nTo source your shell, you need to source the startup files, in Linux it‚Äôs the ~/.bashrc file, in Mac it‚Äôs the ~/.zshrc file.",
    "crumbs": [
      "Home",
      "Working with environments",
      "Pixi"
    ]
  },
  {
    "objectID": "course_environments.html#setting-up-an-environment",
    "href": "course_environments.html#setting-up-an-environment",
    "title": "Introduction to Pixi",
    "section": "Setting Up An Environment",
    "text": "Setting Up An Environment\nYou should create separate environments for each project you run, just to keep things tidy. To create an environment, you have to specify a name for your environment. You can include different platforms/operating systems in your environment, for example if you want to develop your code on a Windows system, and later use the same code on many samples on a large Linux cluster. You can also include different vetted sources for your tools, the so called channels.\nHere, we will create a project called name_pixi_training (please use your own name to avoid creating multiple environments with the same name). We are adding the conda-forge and bioconda channels with the -c flag.\npixi init name_pixi_training -c conda-forge -c bioconda\nPixi will create a folder named name_pixi_training with a a file pixi.toml. Let‚Äôs have a look at that file!\n\n\n\n\n\n\nTipHow to get to the file\n\n\n\n\n\nHere is the code for changing directories, listing files, and viewing the contents of a file:\ncd name_sida_training\nls\nless pixi.toml\nTo exit the less view, press q for quit.\n\n\n\n\npixi.toml\nThe .toml file give your information about your project. Let‚Äôs have a look at one I made one my computer before adding any dependencies to my environment. How is it different from the one you‚Äôve made on HPC2N?\n\n\n\n\n\n\nTip\n\n\n\nClick on the numbers in the list below to highlight the corresponding code.\n\n\n1[workspace]\n2channels = [\"conda-forge\", \"bioconda\"]\n3name = \"amrei_pixi_training\"\n4platforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n5[tasks]\n\n6[dependencies]\n\n1\n\nOverview section of the environment.\n\n2\n\nAdded channels - these are the sources we allow for our dependencies/ tools.\n\n3\n\nThe name of the environment.\n\n4\n\nOperating system the environment is optimised for.\n\n5\n\nThis is where we can define tasks - interesting, but not covered in this course.\n\n6\n\nHere will be the list of tool installed within the environment (once we have added them).\n\n\n\n\n\n\n\n\nNote\n\n\n\nYour current platform will be automatically detected and added to the environment. If you want to add different platforms, you add them with the -p linux-64 flag. In this example, you are adding Linux64. See the Pixi docs for the full list of supported platforms.\nIf you are adding a platform that doesn‚Äôt natively run on your OS (e.g.¬†adding Linux when running on Windows) be sure to add the OS you are running your system on as well!\n\n\nOnce you have used the environment, or added a dependency/tool to it, you will find yet another file, called pixi.lock.\n\n\npixi.lock\nThe .lock file give you information on the channels you have decided to add, as well as the information on where the packages were downloaded from, license information, md5 information, and more.\n\n\n\n\n\n\nImportant\n\n\n\nDo not delete the .toml or .lock files, or you will break your environment!",
    "crumbs": [
      "Home",
      "Working with environments",
      "Pixi"
    ]
  },
  {
    "objectID": "course_environments.html#adding-dependencies",
    "href": "course_environments.html#adding-dependencies",
    "title": "Introduction to Pixi",
    "section": "Adding Dependencies",
    "text": "Adding Dependencies\nAdding dependencies to the .toml file is telling Pixi to install the specified program for you. However, instead of installing it globally, it only gets installed in the environment.\nTo do this, we use the pixi add function. Let‚Äôs try adding Quarto to our environments.\n\n\n\n\n\n\nImportant\n\n\n\nYou must be in the folder of the project to add software!\n\n\npixi add quarto\n\n\n\n\n\n\nTipHow to summon the help function\n\n\n\n\n\nIf you are unsure of how a function works, you can always query it, usually with the --help or -h flags. Here‚Äôs how it would look for the pixi add function\npixi add --help\nA general rule of thumb is that a single hyphen - is followed by a single letter flag, while double hyphens -- are usually followed by multi-letter flags\n\n\n\nHere is the pixi.toml we‚Äôve seen earlier after I have added Quarto to my environment. You can see that the dependencies have been updated to include Quarto.\n[workspace]\nchannels = [\"conda-forge\", \"bioconda\"]\nname = \"amrei_pixi_training\"\nplatforms = [\"osx-arm64\"]\nversion = \"0.1.0\"\n\n[tasks]\n\n[dependencies]\nquarto = \"&gt;=1.7.32,&lt;2\"",
    "crumbs": [
      "Home",
      "Working with environments",
      "Pixi"
    ]
  },
  {
    "objectID": "course_environments.html#running-a-package",
    "href": "course_environments.html#running-a-package",
    "title": "Introduction to Pixi",
    "section": "Running a Package",
    "text": "Running a Package\nNow that we have an environment with a tool installed, we actually want to use it. For this we have several options:\n\npixi run\nUse pixi to run the tool with the pixi run command.\nLet‚Äôs query the help function within Quarto:\npixi run quarto --help\n\n\n\n\n\n\nNote\n\n\n\nTo use other tools, simply substitute quarto with the package you‚Äôve added, and the tool specific commands.\n\n\nAnd just to see that we have not installed Quarto on the server itself, try out running Quarto without Pixi: it will tell you:\nquarto: command not found",
    "crumbs": [
      "Home",
      "Working with environments",
      "Pixi"
    ]
  },
  {
    "objectID": "course_environments.html#pixi-shell",
    "href": "course_environments.html#pixi-shell",
    "title": "Introduction to Pixi",
    "section": "pixi shell",
    "text": "pixi shell\nAlternatively, you can create and ‚Äúenter‚Äù the environment with\npixi shell\nand then interact with the installed dependencies as if they were installed the traditional way:\nquarto --help\n\n\n\n\n\n\nNote\n\n\n\nOnce you are using the pixi shell, you will see that your prompt has changed and is now prefixex with [name_of_environment]. This is how you can check in which environment you operate (and this is why it is important that you name your environments in a way that makes sense).\n\n\nTo exit the environment, type\nexit",
    "crumbs": [
      "Home",
      "Working with environments",
      "Pixi"
    ]
  },
  {
    "objectID": "course_nf-core_rnaseq.html",
    "href": "course_nf-core_rnaseq.html",
    "title": "nfcore RNAseq pipeline",
    "section": "",
    "text": "Great, now we have tested that nextflow and nf-core are set up correctly. Now we want to use the pipeline on our own data. For the sake of the tutorial, I will use the RNAseq data some of you have worked earlier on.\nAs a little reminder: the data is located in:\nTo practice we will go through some of the steps you have done before:",
    "crumbs": [
      "Home",
      "Pipelines",
      "Use nf-core"
    ]
  },
  {
    "objectID": "course_nf-core_rnaseq.html#create-the-pixi-environment",
    "href": "course_nf-core_rnaseq.html#create-the-pixi-environment",
    "title": "nfcore RNAseq pipeline",
    "section": "Create the Pixi environment",
    "text": "Create the Pixi environment\nWhile initializing, make a new directory for this analysis - think about naming it something with meaning.\nWhat dependencies do you need for nextflow and nf-core?",
    "crumbs": [
      "Home",
      "Pipelines",
      "Use nf-core"
    ]
  },
  {
    "objectID": "course_nf-core_rnaseq.html#link-your-data-into-the-directory",
    "href": "course_nf-core_rnaseq.html#link-your-data-into-the-directory",
    "title": "nfcore RNAseq pipeline",
    "section": "Link your data into the directory",
    "text": "Link your data into the directory\nMake a subdirectory, data, change into it and create a symlink to the data:\nUse ln -s SOURCE TARGET. Since you are in the directory datathe TARGET is simply ..\nChange back to the parent directory.\n\n\n\n\n\n\nNote\n\n\n\nThis is how my project directory looks like afterwards:\n.\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223504_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223504_1.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223504_2.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223504_2.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223517_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223517_1.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223517_2.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223517_2.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223546_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223546_1.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223546_2.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223546_2.fastq.gz\n‚îÇ   ‚îú‚îÄ‚îÄ SRR5223559_1.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223559_1.fastq.gz\n‚îÇ   ‚îî‚îÄ‚îÄ SRR5223559_2.fastq.gz -&gt; ../../common_data/RNAseq/SRR5223559_2.fastq.gz\n‚îú‚îÄ‚îÄ pixi.lock\n‚îî‚îÄ‚îÄ pixi.toml",
    "crumbs": [
      "Home",
      "Pipelines",
      "Use nf-core"
    ]
  },
  {
    "objectID": "course_nf-core_rnaseq.html#nf-core-launch",
    "href": "course_nf-core_rnaseq.html#nf-core-launch",
    "title": "nfcore RNAseq pipeline",
    "section": "nf-core launch",
    "text": "nf-core launch\nNf-core helps you set up the pipelines with the nf-core launcher. To access that, click on the launch version 3.19.0 button on the nf-core/rnaseq homepage. (This was the version available in the launcher at the time of writing this session. The version number will change in the future). We are then redirected to a page where we can fill in all of our information about input files, as well as selecting or deselecting certain parts of the pipeline. We will share the things here that you need to input each time, and go through some finer details based on the discussion with you.\n\nSetting working and results directories\n\n\n\n\n\n\nImportant\n\n\n\nWe recommend that you use absolute paths rather than relative paths for setting up your runs.\n\n\nDuring the first part, you need to set a working and result directory. If you are using a server that has a profile established, you can put the name of the server there. In our case, we will use the server configuration file locally.\nSet resume to True, otherwise you don‚Äôt need to change anything here.\n\n\n\nSetting work and output directories\n\n\nNext, the pipeline asks for the input CSV. Exact requirements for how that input file should look like can be found under the tab Usage on the pipeline homepage. This input CSV is unique to each analysis.\n\n\n\nSetting results and input CSV\n\n\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nCreate the necessary input file and add the path to it in the launcher.\nSet the outdir to the name and path to a directory you want the output to be saved to.\n\n\n\n\nSetting all other inputs that are required\nIn this section, you set variables that are related to your reference genome. If you are using something listed on iGenomes, you can input that name. If you are working with your own reference genome, or something not listed, you need to input the absolute path of the reference genomes you have downloaded.\n\n\n\nReference genome options\n\n\nDepending on your strategy, you might need to input a corresponding gff as well. It really depends on the kind of analysis you are hoping to perform.\nIn our case, we have human samples, so in theory we can use the iGenomes reference. However, the transcriptome and GTF files in iGenomes are out of date, so nf-core recommends downloading and using the more up-to-date version.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLet‚Äôs do that.\nThen, add the names and path to the downloaded reference (if located in a different directory) files in the launcher.\n\n\nOther than the options above, you don‚Äôt need to change anything to run the pipeline. However, for your project you might want to change some of the default parameters. For this evercise we will keep the rest as is.\n\n\nGetting your JSON file\nOnce everything is filled in, click on Launch and you will be redirected to another page containing your JSON file that has information on your run.\nCopy the JSON file a bit lower on the screen and saving it as nf-params.json in your folder on HPC2N.\nAdd the save_reference line as recommended by nf-core (because we are using their downloaded human genome).\n\n\nConfiguration profile\nAs before, you need the HPC2N configuration file, with the correct project ID.\nAdd your e-mail under email and you will receive a message with a summary of the run.",
    "crumbs": [
      "Home",
      "Pipelines",
      "Use nf-core"
    ]
  },
  {
    "objectID": "course_nf-core_rnaseq.html#starting-the-run",
    "href": "course_nf-core_rnaseq.html#starting-the-run",
    "title": "nfcore RNAseq pipeline",
    "section": "Starting The Run",
    "text": "Starting The Run\nThe launcher gives us the command to run the pipeline:\nnextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json\nWe need to change this slightly, to add that we are running it via Pixi, and to add the server specific configuration file:\n\nsubmit directly via pixi\nNow you can run the pipeline with the following command (you might have to change it a bit to add pathways to files that are not in your current working directory):\npixi run nextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json -c hpc2n.config\nThere are several layers to this command:\nFirst we invoke Pixi and tell it to run the following commands.\nThen we say which program we want to run, namely Nextflow.\nThe following commands are Nextflow/ nf-core commands:\n\nwe want to run the nf-core/rnaseq pipeline, version 3.19.0\nwe want to use the parameter file called nf-params.json\nwe want to use the hpc configuration file called hpc2n.config\n\n\n\nsubmit via sbatch\nAlternatively, you can run nextflow via pixi using a batch script and slurm: copy the following text to file called name_submit_rnaseq.sh where name is your name.\n#!/bin/bash -l\n#SBATCH -A our_proj_allocation\n#SBATCH -n 5\n#SBATCH -t 24:00:00\n\n/your_home_directory/.pixi/bin/pixi run nextflow run nf-core/rnaseq -r 3.19.0 -params-file /your_path/nf-params.json\nAnd then submit it to slurm with\nsbatch name_submit_rnaseq.sh\nYou can check the progress of your job with squeue -u your_username\nAnd now we wait until the run is done! - My run took a little more than five hours for all four samples.\n\n\n\n\n\n\nTip\n\n\n\nNextflow is notoriously bad at cleaning after itself. You can check previous runs with pixi run nextflow log. And then clean up with for example pixi run nextflow clean -f -before &lt;run_name&gt;. Here is an explanation of the command.",
    "crumbs": [
      "Home",
      "Pipelines",
      "Use nf-core"
    ]
  },
  {
    "objectID": "course_AI.html",
    "href": "course_AI.html",
    "title": "Caution: AI in Bioinformatics",
    "section": "",
    "text": "Caution\n\n\n\nWhile the lure of AI is becoming more present in our daily lives, remember that you do not need it. Before AI, you were perfectly able to design a packing list for your upcoming trip. You were able to look at a paper to find answers to your scientific questions. You knew how to query a vignette in R to determine how a function should be used.\nLife was a bit slower, but you used your mind and your agency. You made decisions. Please do not confuse convenience with need!",
    "crumbs": [
      "Home",
      "Miscellaneous",
      "AI in bioinformatics"
    ]
  },
  {
    "objectID": "course_AI.html#introduction",
    "href": "course_AI.html#introduction",
    "title": "Caution: AI in Bioinformatics",
    "section": "Introduction",
    "text": "Introduction\nIn the past 3 years, AI has become more mainstream. The primary kind of AI people think of, is a Large Language Model (LLM). At this point, you cannot really avoid contact with these models anymore as they have infiltrated every aspect of the internet.\n\n\n\nSearching LLM with Google to be answered with an LLM response\n\n\nThese algorithms are language models trained on incredibly large datasets. They are trained to recognise and generate natural language. The chatbots we are all familiar with are generative pretrained transformers (GPTs). These can be trained for specific tasks, or guided by prompt generation.\nThis is not an AI theory course, and we really do not have enough knowledge to explain the underlying theory beyond a rudimentary level. We would, however, like to discuss the impact of AI on you as a user, and advocate for responsible use of AI in your current and future work.",
    "crumbs": [
      "Home",
      "Miscellaneous",
      "AI in bioinformatics"
    ]
  },
  {
    "objectID": "course_AI.html#machine-learning-in-bioinformatics",
    "href": "course_AI.html#machine-learning-in-bioinformatics",
    "title": "Caution: AI in Bioinformatics",
    "section": "Machine Learning in Bioinformatics",
    "text": "Machine Learning in Bioinformatics\nMachine learning (ML) has been used in bioinformatics and other fields of data science for many decades on every level. Before ML, algorithms had to be programmed by hand rather than having the algorithms learn features of a dataset. With ML, features of a dataset can be annotated based on previously annotated datasets. These algorithms were a mix of supervised (learning on annotated data) and unsupervised (learning on unannotated data) learning, depending on the function of the algorithm.\nSupervised algorithms are used for classification and regression analyses. Unsupervised algorithms are used to discover hidden patterns in data without needing a human‚Äôs input. Unsupervised algorithms are used in clustering, association, and dimensionality reduction\n\n\n\n\n\n\nNote\n\n\n\n\n\nClassification: Output is a discrete variable. Linear classifiers, support vector machines, decision trees, random forests. E.g. annotating a new genome based on genome annotations from existing species.\nRegression: Focus on understanding dependent and indepedent variables.\n\n\n\nFor more info, see here, and here.\nThere is no arguing that these algorithms have led to great progress within the field of bioinformatics. Generative AI is one of the next steps in the evolution of applying machine learning in our lives.",
    "crumbs": [
      "Home",
      "Miscellaneous",
      "AI in bioinformatics"
    ]
  },
  {
    "objectID": "course_AI.html#incorporation-of-ai-in-our-lives",
    "href": "course_AI.html#incorporation-of-ai-in-our-lives",
    "title": "Caution: AI in Bioinformatics",
    "section": "Incorporation of AI in Our Lives",
    "text": "Incorporation of AI in Our Lives\nChatGPT gained 100 million users in the 2 months after its release in 2022, making it the fastest-growing consumer application in history. Generative AI (GenAI) models now come in many different flavours, depending on the developer.\n\n\n\nGenAI chatbots by market share in August 2025 from FirstPageSage\n\n\n\nTraining Data\nAs with earlier iterations of supervised and unsupervised algorithms, GenAI models have all been trained on existing data. And this existing data can be biased: in a historical context, history was recorded by the party that won the war. History changed as different empires and narratives rose and fell. With digitisation, this information has landed on the internet. In the more modern ‚ÄúInternet Age‚Äù, everyone with an internet connection can technically post anything they‚Äôd like on the internet. This can add different types of biases - not everyone has equal access to the internet, some people may not have strong enough opinions to post about something online, some people prefer to read rather than contribute, while others take pleasure in ‚Äúshit-posting‚Äù.\nAs the GenAI training data contains large amounts of data from the internet, these differences in how people use the internet have an effect on how useful the trained models become. In this example, you can clearly see what the effect of bad training data is:\n\n\n\nGenAI answer to a simple question\n\n\nFurthermore, GenAI‚Äôs are not programmed to say that they do not know something, and will happily hallucinate an answer. If you do not know better, or trust the computers, you may take a made-up answer as true and post it elsewhere. As the use of AI‚Äôs increases, AI generated content is used to train new AI‚Äôs. Gary Illyes from Google has spoken about ‚Äúhuman curated‚Äù vs ‚Äúhuman created‚Äù data being used as training data.\nIn a perfect world, GenAI would be trained on perfectly curated data, but even with all of the data that we have on the internet at the moment, we do not have nearly enough data. We have to make do with what we have.",
    "crumbs": [
      "Home",
      "Miscellaneous",
      "AI in bioinformatics"
    ]
  },
  {
    "objectID": "course_AI.html#some-words-of-caution",
    "href": "course_AI.html#some-words-of-caution",
    "title": "Caution: AI in Bioinformatics",
    "section": "Some Words of Caution",
    "text": "Some Words of Caution\nGenAI is becoming more integrated in every sector of our lives. It is important that we use the new technology responsibly. When Google first came out, there were classes on how to use the search engine, determine validity of sources and information, and how to stay safe on the internet. This section aims to raise awareness about commonly overlooked aspects of GPT use.\n\nLearning with AI\nAI has great potential in the field of education. ChatGPT has been shown to be highly beneficial in an educational environment when integrated properly. However, the use of AI in this setting must be balanced and carefully curated. A 2025 pre-print by Kosmyna et al showed that adults that used ChatGPT to write SAT type essays were outperformed consistently by adults that wrote the same essay without the support of an AI, and had significantly lower brain engagement.\n\n\nProductivity with AI\nA recent study by a non-profit group, Model Evaluation and Threat Research (METR) aimed to quantify the difference in productivity when using AI. Participants in this study were not new to their field, with at least 5 years of experience prior to this study being conducted.\n\n\n\nAI reducing productivity\n\n\nThe study also found that when AI is allowed, the participants spent less time coding and seeking solutions to the problems. Rather, they spent time prompting the AI, reading and reviewing responses, and being idle. Intel produced similar findings.\n\n\n\nReasons for loss of productivity with AI\n\n\n\n\nData Privacy and Legal Concerns\nData privacy concerns have been present throuhout the development and use of AI.\nThe use of copyrighted material in training AI, and its legality, is being discussed and debated in several courts. The questions around using text and data mining are divisive with some parties arguing that finding patterns, trends, and insights in existing data being how new research is done by humans and should be extended to AI, while others, particularly in the European context, disagree to some extent. Understanding the legality of the service you use in different countries falls on you as a user.\nThe Terms of Service (ToS) of different GPTs are important when deciding whether to use a GPT at all. For example, Deepseek‚Äôs ToS (collected on 15.08.2025) states:\nAccount Personal Data. We collect Personal Data that you provide when you set up an account, such as your date of birth (where applicable), username (where applicable), email address and/or telephone number, and password.\n\nUser Input. When you use our Services, we may collect your text input, prompt, uploaded files, feedback, chat history, or other content that you provide to our model and Services (‚ÄúPrompts‚Äù or \"Inputs\"). We generate responses (‚ÄúOutputs‚Äù) based on your Inputs.\n\nPersonal Data When You Contact Us. When you contact us, we collect the Personal Data you send us, such as proof of identity or age, contact details, feedback or inquiries about your use of the Services or Personal Data about possible violations of our Terms of Service (our ‚ÄúTerms‚Äù) or other policies.\nBiological data enjoys a high level of protection, and is often considered as highly sensitive. Too often, users will input sequences, tables, or other data into a GPT to find ways to plot data, perform sequence annotations, or similar tasks. Be wary of doing this. Check the ToS explicitly, and frequently. Try to use GDPR compliant GPT‚Äôs if you must use a GPT. Try using only a description of your data - such as column names and type of data - instead of the actual data set.\nAI as browser extensions steal deeply personal information when enabled in a users‚Äô browser including financial, education, and medical information, whether an extension is being actively used or not. The authors have also commented on how these practices interfere with the company‚Äôs own ToS as well as privacy legislation.\n\n\nPersonal Responsibility\nYou as a user are responsible for the AI generated content you choose to use. If you, for instance, ask AI to generate a brand logo for you that is too close to something that exists already, the original owner is free to sue you as an individual for copyright infringement.\nAs scientists, we know that we should use peer-reviewed resources whenever possible. It is why we cannot cite Wikipedia in a scientific article. GPT‚Äôs have been widely shown to fabricate citations based on how it has learnt a citation should look. It is up to you as a user to check every single citation that AI generates since you are responsible for what you write.\n\n\nGlobal Linguistic Changes\nEven though ChatGPT has only been widely used for 3 years, it has already started leaving its traces in how we speak. Words like delve and meticulous are being used more frequently in academic YouTube talks.\n\n\n\nGPT words in YouTube videos from Yakura et al 2025\n\n\nIt has also been shown that different GPT‚Äôs have different writing styles, also known as idiolects.\nSome projects like this one are trying to customize GPT ideolects to match writing styles of unique users. This will make detecting AI use more difficult in future.\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis course was written by two people, and each person wrote their own sections without the use of any AI. Can you tell who wrote which sections based on idiolects?\n\n\n\nWe know that subtle linguistic shifts can change emotional regulation within individuals. We also know from sociolinguistics, that even the slightest linguistic features can serve to bind or divide us.\n\n\nAI on the Internet\nSocial media platforms are an important aspect in the development, improvement, and implementation of GenAI models. OpenAI, the creators of ChatGPT, have used a subreddit on the social media platform, Reddit, to train their new algorithm. Google and OpenAI have contractual agreements with Reddit to license data from the users on the platform. Earlier in 2025, researchers from the University of Zurich were implicated in an experiment on the same subreddit OpenAI used to train a model. They wanted to test whether an interaction with a bot was more likely to make people change their minds than an interaction with a real person. This was heavily frowned upon, and posts were all removed as users had no ability to consent to participating in a study. It has also been suggested that interactions observed by the researchers were just bots arguing with each other.\nA preprint released in February 2025 by Liang et al. found that the amount of content generated by AI rose from 2-3% in November 2022 to 24% by the end of 2023.\n\n\n\nAI slop trough by Yahoo! News!\n\n\n\n\nEnvironmental Impact\nThe facilities to run GenAI require a significant amount of resources. These facilities need a huge amount of electricity to power the facility (this places extreme strain on exising infrastructure and increases the grid‚Äôs carbon footprint) as well as water to cool the hardware. Currently, data centers use more electricity than many independent countries.\nSome data centers are being built near poor communities, drain resources from, and add pollution to the community (see Colossus that has been built in Memphis to power the X bot, Grok for an example. Musk is not the only offender.)\nWhile we cannot do anything about where data centers are built, we can make informed decisions about which platforms we use. We can also be careful with the number of queries we send, and how we use our queries. In April 2025, the CEO of OpenAI said that polite requests like ‚Äúplease‚Äù and ‚Äúthank you‚Äù have cost tens of millions of dollars due to the cost of electricity.",
    "crumbs": [
      "Home",
      "Miscellaneous",
      "AI in bioinformatics"
    ]
  },
  {
    "objectID": "course_AI.html#how-to-decide-when-to-use-ai",
    "href": "course_AI.html#how-to-decide-when-to-use-ai",
    "title": "Caution: AI in Bioinformatics",
    "section": "How To Decide When To Use AI",
    "text": "How To Decide When To Use AI\nIf we know the risks and the true cost of what we are doing, we can make informed decisions about how we chose to incorporate new technologies into our day-to-day and working lives.\nHere are some questions that we find useful to ask ourselves before opening a GPT:\n\nAm I phrasing my prompt in a good way? Here is a guide to prompt engineering that might be useful.\nCan I find this information any other way?\nHow much time am I saving by looking this question up here vs on BioStars, for example?\nDo I know enough about the topic to know whether the GPT is lying to me?\nWhat are the consequences of testing the validity of the GPT solution? Can I potentially corrupt my data or my system? Is there a potential for me to lie to someone who trusts me enough to ask my opinion?\n\nIf a GPT is used to learn a new skill, remember how important active learning is. Seek explanations for everything the GPT tells you. Find independent sources that were produced by experts to validate your learning.\nHold on to the ability to learn and the desire to be curious.",
    "crumbs": [
      "Home",
      "Miscellaneous",
      "AI in bioinformatics"
    ]
  },
  {
    "objectID": "course_nextflow.html",
    "href": "course_nextflow.html",
    "title": "Nextflow",
    "section": "",
    "text": "When developing your code in bioinformatics, you will likely use different tool for different parts of your analyses. Traditionally, you would have about one script per tool, all of which you deploy by hand, one after the other. Together, this is called a workflowor pipeline.\nManual deployment of pipelines can be tedious, especially if you have analyses with many steps, or many samples of different sizes that might need a varying amount of computational power. Luckily for you, other bioinformaticians and software developers have developed something to make your life much easier:",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow"
    ]
  },
  {
    "objectID": "course_nextflow.html#workflow-managers",
    "href": "course_nextflow.html#workflow-managers",
    "title": "Nextflow",
    "section": "Workflow managers",
    "text": "Workflow managers\n\nWorkflow managers provide a framework for the creation, execution, and monitoring of pipeline. &lt;‚Ä¶&gt; They simplify pipeline development, optimize resource usage, handle software installation and versions, and run on different compute platforms, enabling workflow portability and sharing. Wratten et al.¬†(2021) Nature Methods\n\nWith workflow managers you can develop an automated pipeline from your scripts that can then be run on a variety of systems. Once it is developed, execute a single command to start the pipeline. The manager then coordinates the deployment of the scripts in the appropriate sequence, monitors the jobs, handles the file transfers between scripts, gathers the output, and handles re-execution of failed jobs for you. Workflow managed pipelines can run containers, which eliminates software installation and version conflicts.\nThat means that by design the pipelines are:\n\nportable\nmore time efficient (no more downtime between pipeline steps)\nmore resource efficient (mostly, but this might vary depending how skilled a developer you yourself are)\neasier to install (especially when combined with containers, or environment managers)\nmore reproducible\n\nThere are in principle two different flavors of workflow managers: snakemake, and nextflow. In this course we will be introducing you to nexflow.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow"
    ]
  },
  {
    "objectID": "course_nextflow.html#nextflow",
    "href": "course_nextflow.html#nextflow",
    "title": "Nextflow",
    "section": "Nextflow",
    "text": "Nextflow\nIn nextflow, your scripts are turned into processes, connected by channels that contain the data - input, output etc. The order of the processes, and their interaction with each other, is specfied in the workflow scope.\n\n\n\nsource: Software carpentries\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn this course, we will not write our own processes, or pipeline. However, if you are interested, there are a lot of very good training materials available online.\n\n\nThe executable part of the processes, the so called script, can be written in any language, so in theory you could always choose the language that is best suited for the job (in practice you might be limited to the languages you know). However, the modularity of the processes allows for easy re-use of existing scripts and processes.\nWhen moving the pipeline from one system to another, the script stays the same and does not change, the same containers are used. The only thing that changes are the parameters that pertain to the environment and available resources. In other words, with nextflow, the functional logic, the processes, are separated from the executive (how the workflow runs). This makes the nextflow pipelines highly interoperable and portable. They can be run on various platforms, such as HPC clusters, local computers, cloud systems etc..\nThe pipelines can be integrated with version control tools, such as git or bitbucket, and containers technologies, such as apptainer or docker. This makes the pipeline very reproducible.\nThe nextflow pipelines are extremely scalable, can be developed on a few samples and easily be run on hundreds or thousands of samples. When possible, processes are run in parallel automatically.\nNexflow performs automatic checks on the processes and their in- and output. It can automatically resume execution at a point of failure without having to re-compute successfully completed parts.\nNextflow is open source.\nHere is a more visual summary of some of the points above:\n\n\n\nsource: Maxime U Garcia, Seqera labs",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow"
    ]
  },
  {
    "objectID": "course_container.html#reproducibility-in-bioinformatics",
    "href": "course_container.html#reproducibility-in-bioinformatics",
    "title": "Containers",
    "section": "Reproducibility in Bioinformatics",
    "text": "Reproducibility in Bioinformatics\nIn an ideal world I would be able to write a piece of software, or a develop some code to analyse data on my computer, and then send someone else this software or code and they could run it as well, getting the same results.\nIn reality, I would very likely run into at least one of the following problems:\n\nIt is not uncommon for people within the same team to use different operating systems (whether MacOS, Windows, or different flavours of Unix builds). Even if everyone is using a MacOS, there are still different versions that impact the way people are able to work with their machines.\nAlmost every piece of software has some sort of dependency - other software - it needs to run. Some programs might ‚Äújust‚Äù need a bash shell or basic python, while others need a variety of compilers and additional libraries to function. Often, these dependencies require further dependencies to be installed. It is also not uncommon for dependencies for Program 1 to clash with the dependencies for Program 2, requiring the user to uninstall dependencies to be able to install others.\nIn bioinformatics, tools are very often not maintained after the student that wrote the software graduated, the PI moved to a different university, or the funding simply ran out. This leads to a lot of really good software not really being supported by newer operating systems, usually due to dependencies not being easily available or, as before, clashing with newer versions. This makes installing a tool one of the biggest hurdles to overcome in bioinformatics.\nYou often cannot install different versions of the same program on one computer due to conflicting names. This is particularly problematic when you want to rerun an analysis for a publication where you need to use the same software all the way through.\n\nAny of these points might lead to you not being able to run my code, or it running but giving different results. They make bioinformatics less reproducible as tools and code cannot be moved easily betweem systems (for example if you upgrade your computer or want to share your pipeline with a colleague). Fortunately, most of these problems can be overcome with containers.",
    "crumbs": [
      "Home",
      "Working with containers",
      "Apptainer"
    ]
  },
  {
    "objectID": "course_container.html#containers",
    "href": "course_container.html#containers",
    "title": "Containers",
    "section": "Containers",
    "text": "Containers\n\nWhat are containers?\nContainers are stand-alone pieces of software that require a container management tool to run. They are build and exchanged as container images that specify the contents of the container, such as the operating system, all dependencies, and software in an isolated environment. The container management tool then takes the images and build the container. These management tools can be run on all operating systems, and since the container has the operating system within it, it will run the same in all environments. Container images are easily portable and immutable, so they are stable over time.\n\n\nRunning Containers\nThere are several programs that can be used to build and run containers. Docker, Appptainer, and Podman are the most commonly used platforms to date. They all have their pros and cons. If you are using a Windows machine that only you are using, then Docker is likely the least complex tool to install. On multi-user systems like a server, Apptainer is the best tool for the job. For this tutorial and the rest of the course, we will use Apptainer commands. There are small syntax changes between bash and powershell commands, but they are very similar.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLog in to the course server.\n\n\n\n\nDownloading Container Images\nThere are several repositories for people to publish container images that they have specified. Dockerhub and Seqera are two commonly used platforms for downloading container images. You are able to use container images from dockerhub on Apptainer without any problems.\n\ndockerhub Tutorial\nOn the dockerhub landing page, you have a search bar, and some login options. You do not need to create an account to access the containers on dockerhub.\n\n\n\ndockerhub landing page\n\n\nFor these tutorials, we‚Äôll search VCFtools, a commonly used software for VCF manipulation and querying. The results of the search give us several different containers with the same name.\n\n\n\nRegistry search\n\n\nYou can see who made the container image, how many times it has been downloaded (or pulled), when it was updated (here updated means different versions of the image being uploaded), and how many people have starred it. It is usually a good rule of thumb to use the most popular images from users that have uploaded a lot of container images. The biocontainers and pegi3s profiles have builds for a lot of tools, and they are built really well!\nIf we click on the vcftools from biocontainers we get to a typical dockerhub image landing page:\n\n\n\nVCFtools page\n\n\nThere is information on the frequency of the container image being pulled, as well as a pull command to download the image. This command is for docker, so we need to modify it for Apptainer.\napptainer pull vcftools_0.1.16-1.sif docker://biocontainers/vcftools:v0.1.16-1-deb_cv1\n\n\n\n\n\n\nImportantApptainer\n\n\n\nAs you can see, Apptainer is installed on the course server. If it wasn‚Äôt, could we still use it?\n\n\nThis command has several parts to it:\n\napptainer calls on the Apptainer software to run\npull tells Apptainer which function to use. In this case, we want it to go fetch something from a repository\nvcftools_0.1.16-1.sif is the name of the container image on our local machine. We could call it I_Love_Dogs but that is not very informative at all. Your collaborator won‚Äôt know what it means, and you certainly won‚Äôt know what it means in 6 months from now! It is also good practice to put the version number in your image name in case you want to have several versions at the same time, and you need to tell them apart.\ndocker:// is the registry you are pulling from. There are several different registries, but we are only going to show 2 during this course. (You will see another one in the Seqera tutorial)\nbiocontainers/vcftools is the profile/repository and container you are pulling\n\n\n\n\n\n\n\nTip\n\n\n\nFile format extensions like .txt and .sif are really only important for us. However, it is good practice to append your files with appropriate extensions to ensure that you follow good data management practices\n\n\nIf you are interested in a different version than the current version, there are other versions under the tags tab:\n\n\n\nContainer versions\n\n\nIf you wanted to download another version of the container, you simply copy the command shown on the right side, and alter the syntax, for example\napptainer pull vcftools0.1.14.sif docker://biocontainers/vcftools:v0.1.14_cv2\n\n\nSeqera Tutorial\nThe Seqera landing page is a bit different from the dockerhub landing page, and it works a bit differently from dockerhub. Dockerhub hosts container images that users have uploaded, while Seqera makes container images as you request them. They use bioconda, conda-forge, and pypi libraries to build their containers images with Wave. The advantage is that you can include several different softwares in your container image at once. The disadvantage is that you are limited to software hosted on the aforementioned repositories. Usually this isn‚Äôt a problem, but sometimes you want to use something that isn‚Äôt hosted there.\n\n\n\nSeqera containers landing page\n\n\nWhen you pull an image from Seqera and want to run it with Apptainer, you need to remember to change the container setting from Docker to Singularity, the older name of Apptainer.\n\n\n\nSelecting Singularity\n\n\nSince Seqera builds containers on-demand, sometimes you have to wait for the container to finish compiling. You can see that it is still preparing the container image from the fetching container comment. Don‚Äôt try to pull it when it is still building!\n\n\n\nWaiting to build\n\n\nWhen the container image is ready, you can copy the text and pull the image to your system:\n\n\n\nReady to download\n\n\napptainer pull vcftools_0.1.17.sif oras://community.wave.seqera.io/library/vcftools:0.1.17--b541aa8d9f9213f9\nHere we use oras:// instead of docker:// as we are pulling from the oras registry. We are also pulling a different version from Seqera, so the name of the container is different.\n\n\n\nRunning Containers\nOnce you have the container image on your local machine, you want to be able to use it. Apptainer can be used to build the container from the image. Then you can either enter the container and run as if you had the exact same operating system as the person who built it, or you can run the software inside the container from outside of the container.\n\nrunning ‚Äúfrom the outside‚Äù\nThere are 2 different ways to use a container: run and exec. The apptainer run command launches the container and first runs the %runscript for the container if one is defined, and then runs your command (we will cover %runscript in the Building Containers section). The apptainer exec command will not run the %runscript even if one is defined. It is a small, fiddly detail that might be applicable if you use other people‚Äôs containers. After calling Apptainer and the run or exec commands, you can use your software as you usually would\napptainer exec vcftools_0.1.17.sif vcftools --version\nThis command runs your vcftools_0.1.17.sif container from the image, calls on the program vcftools that is within the container, and shows you the version. If you had installed VCFtools locally, you would have just used\nvcftools --version\n\n\n\n\n\n\nImportant\n\n\n\nPlease remember that VCFtools is just an example. If you want to run any other tool everything after apptainer run or apptainer exec has to be substituted by the name of the specific container image and the run commands for that particular tool!\n\n\n\n\nrunning interactively ‚Äúfrom the inside‚Äù\nYou can also enter the container, and work interactively from within. For that you use the apptainer shell command:\napptainer shell &lt;name-of-container&gt;\nInside the container, your prompt will change to Singularity (remember, that is the legacy name for Apptainer). Now you can use the tools inside the container.\nHowever, when entering the container, the file system outside of it becomes inaccessible - with the exception of paths that are explicitely bound into it. There are some system defined bind paths that are automatically accessible within the container (such as your home directory), but you might have to manually set others with the ¬¥-B¬¥ flag. This will make the bound file path accessible from within the container and you can interact with the directories jsut as you normally would.\napptainer shell &lt;name-of-container&gt;\napptainer shell -B outside/path:inside/path &lt;name-of-container&gt;\nIn the above, we bind in the outside/path (which could be ../data) and makes it accessible within the container as inside/path (which could be plain /data).\nTo exit the container, simply type and enter exit.\n\n\n\nRunning containers with sbatch\nYou can run Apptainer containers as part of a batch job by integrating them into a SLURM job submission script. Here is an example batch script.\nLet‚Äôs go back to our FastQC sbatch script and add the container:\n#! /bin/bash -l\n#SBATCH -A project_ID\n#SBATCH -t 30:00\n#SBATCH -n 1\n\napptainer exec -B ../data:data fastqc -o . --noextract data/*fastq.gz\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nWhat has changed? And what do these changes do? Discuss with your neighbour.\n\n\n\n\nBuilding Containers\nIf the software you would like to use is not packaged into a container by anyone else, you might want to build it yourself. For this, we are just going to show a very simple example. Building containers from scratch is a computationally intensive task. You build containers from a definition file with the extension .def\nHere we are going to build a container with a cow telling us the date. Save this in a file called lolcow.def.\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    date | cowsay | lolcat    \nThere are several components to this definition file.\n\nYou can set the operating system you want in the container, in this case Ubuntu 20.04.\n%post section is where you update the OS from its base state, install dependencies and so on.\n%environment is where you export paths and modify the environment.\n%runscript is the script that will run when you use apptainer run container.sif. If you don‚Äôt include a runscript, then nothing will happen when you try to run it without any commands. You could build this container without anything in the %runscript section, and use apptainer run container.sif date | cowsay | lolcat to get the same output.\n\napptainer build lolcow.sif lolcow.def\nYou‚Äôll get a lot of output on the status of the build, ending in\nINFO:    Adding environment to container\nINFO:    Adding runscript\nINFO:    Creating SIF file...\nINFO:    Build complete: lolcow.sif\nWe can now run our new container with\napptainer run lolcow.sif\n\n\n\nBoring cow\n\n\n\n\n\n\n\n\nNote\n\n\n\nTry removing the %runscript, build it again, and see what happens.\n\n\nBootstrap: docker\nFrom: ubuntu:20.04\n\n%post\n    apt-get -y update\n    apt-get -y install cowsay lolcat fortune\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    fortune | cowsay | lolcat    \nIf we use the same definition file as before, but substitute date for fortune in the runscript and build the container, we now get a philosophical cow with a dark sense of humour:\n\n\n\nFun cow\n\n\n\n\n\nInspirational cow\n\n\nTo show the difference between the run and exec commands, we can use the same container with fortune in the runscript and run:\napptainer run lolcow.sif bash -c \"date|cowsay\"\nand\napptainer exec lolcow.sif bash -c \"date|cowsay\"\nThe run command gives us a philosophical cow while exec gives us our boring cow again.",
    "crumbs": [
      "Home",
      "Working with containers",
      "Apptainer"
    ]
  },
  {
    "objectID": "course_nextflow_tutorial.html#training-material",
    "href": "course_nextflow_tutorial.html#training-material",
    "title": "Nextflow tutorial",
    "section": "Training material",
    "text": "Training material\nTo download the material, execute this command:\ngit clone https://github.com/nextflow-io/training.git\nThen cd into the relevant directory. By default, that is hello-nextflow.\nBasic concepts:\nNextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it ‚Äúeasy‚Äù to write data-intensive computational workflows.\nIt is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations.\nNextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment, based on the dataflow programming model. Nextflow‚Äôs core features are:\n\nWorkflow portability and reproducibility\nScalability of parallelization and deployment\nIntegration of existing tools, systems, and industry standards",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow basics"
    ]
  },
  {
    "objectID": "course_nextflow_tutorial.html#processes-and-channels",
    "href": "course_nextflow_tutorial.html#processes-and-channels",
    "title": "Nextflow tutorial",
    "section": "Processes and Channels:",
    "text": "Processes and Channels:\nIn practice, a Nextflow workflow is made by joining together different processes. Each process can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.).\nProcesses are executed independently and are isolated from each other, i.e., they do not share a common (writable) state. The only way they can communicate is via asynchronous first-in, first-out (FIFO) queues, called channels.\nAny process can define one or more channels as an input and output. The interaction between these processes, and ultimately the workflow execution flow itself, is implicitly defined by these input and output declarations.\nBelow is a visualization of a process:\n\n\n\nNextflow basics\n\n\nAnd here the way this could be implemented in a process block as part of a Nextflow pipeline:\nprocess PROCESS_NAME{\n\n    input:\n      data z\n      data y\n      data x\n\n    // directives\n    container\n\n    script:\n      task1\n      task2\n      task3\n\n    output:\n      output x\n      output y\n      output z\n}\n\n\n\n\n\n\nNote\n\n\n\nQuick side information about channels (advanced, but good to keep in mind):\nIn Nextflow there are two kinds of channels: queue channels and value channels.\nA queue channel is a non-blocking unidirectional FIFO queue which connects two processes, channel factories, or operators.\nThe content of a queue channel is consumed and can only be read once, which is usually fine in a workflow. However, if you need to re-use the same content you need a Value channel.\nA value channel a.k.a. singleton channel is bound to a single value and can be read an unlimited number of times without consuming its content.\nMore information here: https://www.nextflow.io/docs/latest/channel.html#channel-types",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow basics"
    ]
  },
  {
    "objectID": "course_nextflow_tutorial.html#execution-abstraction",
    "href": "course_nextflow_tutorial.html#execution-abstraction",
    "title": "Nextflow tutorial",
    "section": "Execution abstraction",
    "text": "Execution abstraction\nAnother important concept in Nextflow. This is the way you ran fastQC on your samples in pre-course exercise 7:\nsrun -A project_ID -t 15:00 -n 1 fastqc --noextract -o fastqc data data/sample_1.fastq.gz data/sample_2.fastq.gz \nHere, there is a mix of information on what command should be executed (fastqc with the noextract option) and information on how the script should be run on the target platform (debitted to which project, run for 15 minutes etc.).\nIn Nextflow the process defines what command or script is executed, the executor determines how that script is run in the target platform.\nIf not otherwise specified, processes are executed on the local computer. The local executor is very useful for workflow development and testing purposes, however, for real-world computational workflows a high-performance computing (HPC) or cloud platform is often required.\nIn other words, Nextflow provides an abstraction between the workflow‚Äôs functional logic and the underlying execution system (or runtime). Thus, it is possible to write a workflow that runs seamlessly on your computer, a cluster, or the cloud, without being modified. You simply define the target execution platform in the configuration file.\n\n\n\nExecution abstraction",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow basics"
    ]
  },
  {
    "objectID": "course_nextflow_tutorial.html#scripting-language",
    "href": "course_nextflow_tutorial.html#scripting-language",
    "title": "Nextflow tutorial",
    "section": "Scripting language",
    "text": "Scripting language\nNextflow is a Domain Specific Language (DSL) implemented as an extension of the Groovy programming language. This means that Nextflow can run any Groovy and Java code. It is not necessary to learn Groovy to use Nextflow DSL but it can be useful in edge cases where you need more functionality than the DSL provides.\nBut remember: Each process can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.). This is done in the script part of the process.\nYour first script\nHere you will execute your first Nextflow script (hello.nf), which we will go through line-by-line.\nIn this toy example, the script takes an input string (a parameter called params.greeting) and splits it into chunks of six characters in the first process. The second process then converts the characters to upper case. The result is finally displayed on-screen.\nBut first we need to retreive the files and set up the conda nextflow environment: Preparations: Start a screen session\nscreen -S nextflow\nActivate the pixi environment (if you haven‚Äôt done so yet)\npixi shell\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nCheck your nextflow version - which one are we using? Is it the same as you have specified in your Pixi environment?",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow basics"
    ]
  },
  {
    "objectID": "course_nextflow_tutorial.html#nextflow-code",
    "href": "course_nextflow_tutorial.html#nextflow-code",
    "title": "Nextflow tutorial",
    "section": "Nextflow code",
    "text": "Nextflow code\nHere is a simple nextflow file:\n#!/usr/bin/env nextflow\n\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\n\nprocess SPLITLETTERS {\n    input:\n    val x\n\n    output:\n    path 'chunk_*'\n\n    script:\n    \"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\n\nprocess CONVERTTOUPPER {\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    cat $y | tr '[a-z]' '[A-Z]' \n    \"\"\"\n}\n\nworkflow {\n    letters_ch = SPLITLETTERS(greeting_ch)\n    results_ch = CONVERTTOUPPER(letters_ch.flatten())\n    results_ch.view{ it }\n}\nThis code begins with a shebang that declares Nextflow as the interpreter.\nThen we declare a parameter ‚Äúgreeting‚Äù and initialize it with the value ‚ÄúHello world!‚Äù.\nThen we initialize a channel labeled ‚Äúgreeting_ch‚Äù. It contains the value from params.greeting. This channel is the input for the processes in the workflow.\nNext we define the process named ‚ÄúSPLITLETTERS‚Äù. This is also called a process block. It starts by naming the process, and its definition is enclosed by squiggly brackets.\nWithin the process block we declare the input for the process. Inputs can be values (val), files, paths, or other qualifiers (more see here). We tell the process to expect an input value (val), which we assign to the variable ‚Äúx‚Äù.\nThen, we define the output of the process.\nWe tell the process to expect an output file (path), with a filename starting with ‚Äúchunk_‚Äù, as output from the script. The process sends the output as a channel.\nWe then define the script portion of the process. Three double quotes start and end the code block to execute this process. Inside is the code to execute ‚Äî printing the input value x (called using the dollar symbol [$] prefix), splitting the string into chunks with a length of 6 characters (‚ÄúHello‚Äù and ‚Äúworld!‚Äù), and saving each to a file (chunk_aa and chunk_ab).\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLook at the next process. What happens in there?\n\n\nAt the end, we define the workflow scope, where each process is called. The workflow scope starts with ‚Äúworkflow‚Äù and is enclosed in squiggly brackets:\nFirst, execute the process SPLITLETTERS on the greeting_ch (aka greeting channel), and store the output in the channel letters_ch.\nThen, execute the process CONVERTTOUPPER on the letters channel letters_ch, which is flattened using the operator .flatten(). This transforms the input channel in such a way that every item is a separate element. We store the output in the channel results_ch.\nFinally, we print the final output (in results_ch) to screen using the view operator (appended onto the channel name).\n\n\n\n\n\n\nNote\n\n\n\nQuick side information about the Nextflow code:\nThe order in which the workflow scope and the process blocks appear in the nextflow file does not matter. As it is, I prefer having the workflow scope at the top (you will see that later), followed by the process blocks.\nSimilarly, within the process block it does not matter in which order you declare the different parts. My personal preference is the order input, process, output.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow basics"
    ]
  },
  {
    "objectID": "course_nextflow_tutorial.html#lets-run-this-script",
    "href": "course_nextflow_tutorial.html#lets-run-this-script",
    "title": "Nextflow tutorial",
    "section": "Let‚Äôs run this script!",
    "text": "Let‚Äôs run this script!\nIn the directory where the nextflow script is located, run\npixi run nextflow run hello.nf\nThe output looks something like this:\n N E X T F L O W   ~  version 25.04.7\n\nLaunching `hello.nf` [jolly_faraday] DSL2 - revision: f5e335f983\n\nexecutor &gt;  local (3)\n[96/fd5f07] SPLITLETTERS (1)   [100%] 1 of 1 ‚úî\n[7e/dad424] CONVERTTOUPPER (2) [100%] 2 of 2 ‚úî\nHELLO \nWORLD!\nThe standard output shows (line by line):\n\nThe version of Nextflow that was executed.\nThe script and version names.\nThe executor used (in the above case: local). Tsk tsk tsk.\nhe first process is executed once, which means there is one task. The line starts with a unique hexadecimal value**, and ends with the percentage and other task completion information.\nThe second process is executed twice (once for chunk_aa and once for chunk_ab), which means two tasks.\nThe result string from stdout is printed.\n\n** The hexadecimal numbers, like 96/fd5f07, identify the unique process execution, that we call a task. These numbers are also the prefix of the directories where each task is executed. You can inspect the files produced by changing to the directory $PWD/work and using these numbers to find the task-specific execution path.\n\n\n\n\n\n\nNote\n\n\n\nIt‚Äôs worth noting that the process CONVERTTOUPPER is executed in parallel, so there‚Äôs no guarantee that the instance processing the first split (the chunk Hello ‚Äô) will be executed before the one processing the second split (the chunk ‚Äôworld!).\nThis means you might see\nWORLD!\nHELLO \non your screen.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow basics"
    ]
  },
  {
    "objectID": "course_nextflow_tutorial.html#modify-and-resume",
    "href": "course_nextflow_tutorial.html#modify-and-resume",
    "title": "Nextflow tutorial",
    "section": "Modify and resume:",
    "text": "Modify and resume:\nNextflow keeps track of all the processes executed in your workflow. If you modify some parts of your script, only the processes that are changed will be re-executed. The execution of the processes that are not changed will be skipped and the cached result will be used instead.\nThis allows for testing or modifying part of your workflow without having to re-execute it from scratch.\nFor the sake of this tutorial, modify the CONVERTTOUPPER process in the previous example, replacing the process script with the string rev $y, so that the process looks like this:\nprocess CONVERTTOUPPER {\n\n    input:\n    path y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    rev $y\n    \"\"\"\n}\nThen, save the file with the same name, and execute it by adding the -resume option to the command line:\npixi run nextflow run hello.nf -resume\nYou will see that the execution of the process SPLITLETTERS is skipped (the task ID is the same as in the first output) ‚Äî its results are retrieved from the cache. The second process is executed as expected, printing the reversed strings.\n\n\n\n\n\n\nNote\n\n\n\nQuick side note on the cache:\nThe workflow results are cached by default in the directory $PWD/work. Depending on your script, this folder can take up a lot of disk space. If you are sure you won‚Äôt need to resume your workflow execution, clean this folder periodically. We will get into the how to in a bit.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow basics"
    ]
  },
  {
    "objectID": "course_nextflow_tutorial.html#workflow-parameters",
    "href": "course_nextflow_tutorial.html#workflow-parameters",
    "title": "Nextflow tutorial",
    "section": "Workflow parameters:",
    "text": "Workflow parameters:\nAs we have seen workflow parameters are simply declared by prepending the prefix params to a variable name, separated by a dot character (e.g.¬†params.greeting). Their value can alternatively be specified on the command line by prefixing the parameter name with a double dash character, i.e.¬†--paramName.\nNow, let‚Äôs try to execute the previous example specifying a different input string parameter, as shown below:\npixi run nextflow run hello.nf -resume --greeting 'Bonjour le monde!'\n\n\n\n\n\n\nNote\n\n\n\nQuick side note on configuration files and hierarchies:\nWhen a pipeline script is launched, Nextflow looks for configuration files in multiple locations. Since each configuration file can contain conflicting settings, the sources are ranked to determine which settings are applied. Possible configuration sources, in order of priority:\n\nParameters specified on the command line (‚Äìsomething value)\nParameters provided using the -params-file option\nConfig file specified using the -c my_config option\nThe config file named nextflow.config in the current directory\nThe config file named nextflow.config in the workflow project directory\nThe config file $HOME/.nextflow/config\nValues defined within the pipeline script itself (e.g.¬†main.nf)\n\nWhen more than one of these options for specifying configurations are used, they are merged, so that the settings in the first override the same settings appearing in the second, and so on.",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow basics"
    ]
  },
  {
    "objectID": "course_nextflow_tutorial.html#clean-up-nextflow",
    "href": "course_nextflow_tutorial.html#clean-up-nextflow",
    "title": "Nextflow tutorial",
    "section": "Clean up Nextflow:",
    "text": "Clean up Nextflow:\nPeriodically, it makes sense to clean up your nextflow working directory. You can do that manually, but the non-descriptive nature of the file system makes that difficult. Fortunately, Nextflow keeps track of your nextflow runs and their files.\npixi run nextflow log\nwill show you the executions log and runtime information of all Nextflow runs executed from the current directory(!!).\nnextflow clean\nwill clean up the project cache and the work directories. Specify the run name and option as explained here.\nAn example that cleans all runs before the named run and forces the clean up:\npixi run nextflow clean -before &lt;RUN NAME&gt; -f",
    "crumbs": [
      "Home",
      "Workflow manager",
      "Nextflow basics"
    ]
  },
  {
    "objectID": "before_command_line.html",
    "href": "before_command_line.html",
    "title": "Command line",
    "section": "",
    "text": "Unix-like operating systems are built under the model of free and open-source development and distribution. They often come with a graphical user interface (GUI) and can be run from the command line (CLI) or terminal. The CLI is a text-based interface that works exactly the same way as you would use your mouse, but you use words.\nIt is important to know how to use the terminal as all servers, and most bioinformatics tools, do not have a GUI and rely on the use of the terminal.\nFor this course, we will use a GUI for some parts, but all our interaction with the remote server will be on the command line. As such, it‚Äôs important that you know your way around the command line.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nI know that you have varying experience with the command line. The following tutorial gives an excellent overview over the basics (and not so basics). If you have already worked with the command line it will probably be enough to browse and brush up on some parts, if you haven‚Äôt this will be your chance to learn:\nSoftware carpentries tutorial on the Unix shell.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe above tutorial is designed to be taken on your local machine, where you have a home directory and a Desktop.\nYou can also follow the tutorial on the course server. However, you won‚Äôt have a Desktop there, and will have to adjust accordingly.\nIf you want to follow the course on the server you can download the tutorial data with:\ncurl https://swcarpentry.github.io/shell-novice/data/shell-lesson-data.zip -o shell-lesson-data.zip \nThen decompress with:\nunzip shell-lesson-data.zip",
    "crumbs": [
      "Home",
      "Preparations",
      "Command line"
    ]
  },
  {
    "objectID": "course_data_management.html#data-life-cycle",
    "href": "course_data_management.html#data-life-cycle",
    "title": "Data Management for Reproducible Research",
    "section": "Data Life Cycle",
    "text": "Data Life Cycle\nWhen working with any type of data, it makes sense to sit down before the project starts to think through the different life stages of the data in your project. This will help counteract some of the problems that can arise when projects grow more organically, and will help consistency within the research group, ease collaboration, and mostly your future self that will understand what past-self has been up to in the project.\n\n\n\n\n\n\nNote\n\n\n\nMore and more funding agencies expect a Data Management Plan at some point of a project application. In there, you need to document that you have thought of, and planned for, the life cycle of your data.\n\n\n\n\n\nThe Research Data Management toolkit for Life Sciences",
    "crumbs": [
      "Home",
      "Why this course?",
      "Reproducible bioinformatics"
    ]
  },
  {
    "objectID": "course_data_management.html#fair-principles",
    "href": "course_data_management.html#fair-principles",
    "title": "Data Management for Reproducible Research",
    "section": "FAIR principles",
    "text": "FAIR principles\nIn the past, research data was often generated with one question in mind. Often, they would afterwards land in some drawer and be forgoten about. Nowadays researchers acknowledge that data can also be re-used, or combined with other data, to answer different questions.\nThe FAIR principles promote efficient data discovery and reuse by providing guidelines to make digital resources:\n\n\n\nWilkinson et al.¬†(2016)\n\n\nFAIR principles, in turn, rely on good data management practices in all phases of research:\n\nResearch documentation\nData organisation\nInformation security\nEthics and legislation",
    "crumbs": [
      "Home",
      "Why this course?",
      "Reproducible bioinformatics"
    ]
  },
  {
    "objectID": "course_data_management.html#reproducible-research",
    "href": "course_data_management.html#reproducible-research",
    "title": "Data Management for Reproducible Research",
    "section": "Reproducible research",
    "text": "Reproducible research\nLucky for us, once we implement good data management practices, we will also increase the reproducibility of our analyses. Extensive documentation will increase faith in the outcome of analyses, and will help people (again, future-you) understand what has been done.\nLast, but not least, reproducible research practices make project hand-overs smoother, when the next person already understands the structure of the project, and can rely on good documentation.",
    "crumbs": [
      "Home",
      "Why this course?",
      "Reproducible bioinformatics"
    ]
  },
  {
    "objectID": "course_data_management.html#what-data-do-we-work-with",
    "href": "course_data_management.html#what-data-do-we-work-with",
    "title": "Data Management for Reproducible Research",
    "section": "What data do we work with?",
    "text": "What data do we work with?\n\nBioinformatics is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex. (Wikipedia)\n\nThis data can come from a variety of different biological processes:\n\n\n\nsource: Lizel Potgieter\n\n\nEarly on, sequencing data was not readily available, but due to decreasing costs and increased computational power biological data is now being produced in ever increasing quantities:\n\n\n\nGrowth of the Sequence Read Archive, SRA, from 2012 to 2021\n\n\nAt the same time, new technologies are being developed, and new tools that might or might not be maintained or benchmarked against existing tools. It‚Äôs the wild west out there!\n\n\n\nOverview of modern sequencing technologies and where they apply to biological processes",
    "crumbs": [
      "Home",
      "Why this course?",
      "Reproducible bioinformatics"
    ]
  },
  {
    "objectID": "course_data_management.html#working-with-data",
    "href": "course_data_management.html#working-with-data",
    "title": "Data Management for Reproducible Research",
    "section": "Working with data",
    "text": "Working with data\nOften, with a new project, one sits down with the data, tries out things and see if they worked. A lot of bioinformatics is not being afraid to try things, and reading the documentation.\nThis traditional way of working with bioinformatics data can have merits and lead to new discoveries. However, in this course we would like to introduce you to a more structured way to make sense of your data.\nLet‚Äôs have a look at a typical PhD student‚Äôs research project:\n\nThey might analyse their data, and get some results.\nAfter talking with their supervisor they might get a few other samples from a collaborator, or need to drop them from the analyses due to quality concerns.\nThey run the analyses again and get a different set of results.\nThere might be a few iterations of this process, and then the reviewers require some additional analyses‚Ä¶\n\nIn the ‚Äúend‚Äù we have something like this:\n\n\n\nWhich one of these is the latest version?\n\n\n\n\n\n\n\n\nTipBest practices file organization\n\n\n\n\nThere is a folder for the raw data, which does not get altered.\nCode is kept separate from data.\nUse a version control system (at least for code) ‚Äì e.g.¬†git.\nThere should be a README in every directory, describing the purpose of the directory and its contents.\nUse file naming schemes that makes it easy to find files and understand what they are (for humans and machines) and document them.\nUse non-proprietary formats ‚Äì .csv rather than .xlsx",
    "crumbs": [
      "Home",
      "Why this course?",
      "Reproducible bioinformatics"
    ]
  },
  {
    "objectID": "course_data_management.html#literate-programming",
    "href": "course_data_management.html#literate-programming",
    "title": "Data Management for Reproducible Research",
    "section": "Literate programming",
    "text": "Literate programming\nOur hypothetical PhD student, even if taking into account the best practice tips from above, is still likely to run the same analyses over and over whenever the input data changes. Sometimes, this might be months, or even years, after the original analysis was performed.\nLuckily, they can save their code snippets (with intuitive file names) and re-use the code from back then. This is often done with R-scripts, but can just as well be applied to bash scripts, python scripts etc.\nIn the past years, the development went even further and one can even combine code and documentation in the same document. The code is wrapped in so called chunks, or code cells, that are executable from within the document.\n\nDebugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it. Brian Kernighan\n\nBefore the course you have already worked with one such notebook - Quarto. We will continue to work with it during this course.\n\n\n\n\n\n\nTip\n\n\n\n\nDocument your methods and workflows.\nDocument where and when you downloaded data.\nDocument the versions of the software that you ran.",
    "crumbs": [
      "Home",
      "Why this course?",
      "Reproducible bioinformatics"
    ]
  },
  {
    "objectID": "course_data_management.html#version-control",
    "href": "course_data_management.html#version-control",
    "title": "Data Management for Reproducible Research",
    "section": "Version control",
    "text": "Version control\nNow that our student has reproducible documents, with reasonable names, that can execute their analyses reliably over and over again, what happens if they modify their analyses? Will they end up again with different result files and their project sink down in chaos?\nNo, because there is version control, the practice of tracking and managing changes to files.\nAgain, before the course you worked through the basics of git, and how to use it with GitHub collaboratively. We will continue using git during the course as well.",
    "crumbs": [
      "Home",
      "Why this course?",
      "Reproducible bioinformatics"
    ]
  },
  {
    "objectID": "course_data_management.html#environment-managers",
    "href": "course_data_management.html#environment-managers",
    "title": "Data Management for Reproducible Research",
    "section": "Environment managers",
    "text": "Environment managers\nUsing git, our PhD student can now share their reproducible code with their colloaborators, or between systems. They can rest assured that the different versions of the notebook are tracked and can be checked out when necessary. But what about the bioinformatic tools? Can they also be shared easily?\nDifferent computers can run on different operating systems, or can have different versions of databases installed. This can lead to conflicts between tools, or software versions and can impact code usability, or reproducibility.\nFortunately, smart people have developed environment managers such as conda, bioconda, or pixi. These tools find and install packages, so that the same package versions are being run between different computers. However, the code might still give different results on different operating systems.\nDuring this course we will be building our own environments with Pixi - you‚Äôll see how great it is not having to manually install tools anymore!",
    "crumbs": [
      "Home",
      "Why this course?",
      "Reproducible bioinformatics"
    ]
  },
  {
    "objectID": "course_data_management.html#containers-in-bioinformatics",
    "href": "course_data_management.html#containers-in-bioinformatics",
    "title": "Data Management for Reproducible Research",
    "section": "Containers in bioinformatics",
    "text": "Containers in bioinformatics\nBut what if our PhD student needs to run their code on different operating systems?\nThey can use containers, that contain everything needed to run the application, even the operating system. Containers are being exchanged as container images, which makes them lightweight. Containers do not change over time, so the results will be the same today and in a few years. Everyone gets the same container that works in the same way.\nIn this course, you will have guessed it, we will learn about containers, where to get them and how to use them.",
    "crumbs": [
      "Home",
      "Why this course?",
      "Reproducible bioinformatics"
    ]
  },
  {
    "objectID": "course_data_management.html#workflow-manager",
    "href": "course_data_management.html#workflow-manager",
    "title": "Data Management for Reproducible Research",
    "section": "Workflow manager",
    "text": "Workflow manager\nNow our PhD student can use containers, or environments, to provide a uniform environment for their version controlled, wonderfully documented and reproducible code. Fantastic! But they still have to deploy, or at least monitor, their scripts manually.\nFortunately there are workflow managers that can integrate all of the above, submit your jobs for you, and even monitor and re-submit scripts after failure. They will automatically submit jobs for you, decreasing downtime and increasing efficiency.\n\n\n\n\n\n\nTip\n\n\n\nHumans tend to do mistakes, especially when it comes to tedious or repetitive tasks. If you automate data handling, formatting etc. you are less likely to make mistakes like typos, or changing colors in images.\n\n\nIn this course, we will also cover a workflow manager, Nextflow and learn how to make our own workflow, and how to run already developed workflows.",
    "crumbs": [
      "Home",
      "Why this course?",
      "Reproducible bioinformatics"
    ]
  },
  {
    "objectID": "course_data_management.html#goal-of-the-course",
    "href": "course_data_management.html#goal-of-the-course",
    "title": "Data Management for Reproducible Research",
    "section": "Goal of the course",
    "text": "Goal of the course\nWith this, we want to give you tools that will help you plan and carry out your research. These tools will make your work more efficient and more reproducible. No matter what kind of data you use, you will take something useful from this course.\n\n\n\nOverview of modern sequencing technologies and where they apply to biological processes",
    "crumbs": [
      "Home",
      "Why this course?",
      "Reproducible bioinformatics"
    ]
  },
  {
    "objectID": "bonus_python_pixi.html",
    "href": "bonus_python_pixi.html",
    "title": "Using Python in Pixi",
    "section": "",
    "text": "You can also install Python packages into your Pixi environment. As an example we will use FastQE, a Python based tool that translates the fastq quality scores into emojis.",
    "crumbs": [
      "Home",
      "Working with environments",
      "Bonus: Python in Pixi"
    ]
  },
  {
    "objectID": "bonus_python_pixi.html#prepare-the-environment",
    "href": "bonus_python_pixi.html#prepare-the-environment",
    "title": "Using Python in Pixi",
    "section": "Prepare the environment",
    "text": "Prepare the environment\npixi init fastqe -c conda-forge\ncd fastqe/\npixi add python\npixi add --pypi fastqe\npixi add --pypi setuptools",
    "crumbs": [
      "Home",
      "Working with environments",
      "Bonus: Python in Pixi"
    ]
  },
  {
    "objectID": "bonus_python_pixi.html#run-fastqe-on-your-samples",
    "href": "bonus_python_pixi.html#run-fastqe-on-your-samples",
    "title": "Using Python in Pixi",
    "section": "run FastQE on your sample(s)",
    "text": "run FastQE on your sample(s)\npixi run srun -A project_ID -t 02:00 fastqe --html  --output sample_name.html ../sample1.fastq",
    "crumbs": [
      "Home",
      "Working with environments",
      "Bonus: Python in Pixi"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Bioinformatics",
    "section": "",
    "text": "The aim of this course is to share how to use bioinformatics tools in a reproducible and scalable way. We will use environments, containers, and established pipelines so that you can run these analyses on any operating system, as well as on systems that are not high performance computing clusters. And first and foremost: these tools and techniques can be used regardless of which type of bioinformatics you are ultimately working with.\nThe website will remain active after the course so that you have access to the material even after the course.\nWe will meet in person in Ultuna between Monday, October 6th, and Friday, October 10th, 2025. However, to get the most out of this course, we expect you to do some preparation in advance. This is to set-up and get aquainted with some of the tools we will be using.\nI want to thank Lizel Potgieter for her valuable contributions to the materials for this course!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Applied Bioinformatics",
    "section": "Course content",
    "text": "Course content\n\nbefore the course:\n\nBefore the course\n\n\nTopic\nContent\n\n\n\n\nHPC access\nCreate user accounts for the HPC resources\n\n\nVScode\nSettting up and getting aquainted with VScode\n\n\nServer access\nSSH connect to the course server\n\n\nCommand line\nUse the command line to manipulate data\n\n\nscreen\nManage persistent bash sessions\n\n\nQuarto\nInstalling and using Quarto with VScode\n\n\ngit and GitHub\nVersion control with git and GitHub\n\n\n\n\n\nduring the course:\nWe will sometimes be sharing ideas, scripts etc. during the course. For this purpose, I have set up a page on HackMD that we all can access and edit: living document at HackMD.\n\nDuring the course\n\n\nDay\nSession\n\n\n\n\nMonday\nWelcome, course setup\n\n\n\nData Management & Reproducible Research\n\n\n\nUsing git collaboratively\n\n\n\nCreate a blog with Quarto\n\n\n\nPublish your blog with GitHub actions\n\n\nTuesday\nIntroduction to environments\n\n\n\nQuality control of sequencing data\n\n\n\nBonus: Python in Pixi\n\n\n\nIntroduction to containers\n\n\n\nUsing containers on your data\n\n\nWednesday\nIntroduction to Nextflow\n\n\n\nNextflow basics\n\n\n\nNextflow RNAseq\n\n\nThursday\nnf-core\n\n\n\nTest a nf-core pipeline\n\n\n\nSet up a nf-core pipeline\n\n\nFriday\nVisualize your results: ggplot\n\n\n\nDiscussion: AI in Bioinformatics\n\n\n\nclean-up & finishing",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "course_test_nf-core.html",
    "href": "course_test_nf-core.html",
    "title": "nfcore: test pipeline",
    "section": "",
    "text": "Now that you know about Linux, containers, pixi, and Nextflow, we get to start with another really cool part of our course! In this section, we will create a pixi environment containing nf-core and nextflow. Once we‚Äôve done that, we will turn our attention to nf-core to run up a pipeline with the build in test data. This is a good way to test the setup, and to familiarize yourself with the output of the pipeline.",
    "crumbs": [
      "Home",
      "Pipelines",
      "Test nf-core"
    ]
  },
  {
    "objectID": "course_test_nf-core.html#introduction",
    "href": "course_test_nf-core.html#introduction",
    "title": "nfcore: test pipeline",
    "section": "",
    "text": "Now that you know about Linux, containers, pixi, and Nextflow, we get to start with another really cool part of our course! In this section, we will create a pixi environment containing nf-core and nextflow. Once we‚Äôve done that, we will turn our attention to nf-core to run up a pipeline with the build in test data. This is a good way to test the setup, and to familiarize yourself with the output of the pipeline.",
    "crumbs": [
      "Home",
      "Pipelines",
      "Test nf-core"
    ]
  },
  {
    "objectID": "course_test_nf-core.html#setting-up-your-pixi-environment",
    "href": "course_test_nf-core.html#setting-up-your-pixi-environment",
    "title": "nfcore: test pipeline",
    "section": "Setting Up Your Pixi Environment",
    "text": "Setting Up Your Pixi Environment\nIn our course directory execute these commands, one after the other.\nLet‚Äôs inititalise an environment for this - feel free to change the name of the environment/ directory.\npixi init nextflow_test -c conda-forge -c bioconda\nChange directory into the project you created, and list the files there:\ncd name_nextflow\nls\nAdd nf-core and Nextflow:\npixi add nextflow nf-core\n\n\n\n\n\n\nNote\n\n\n\n\n\nWhile apptainer is sticky loaded on this server, it won‚Äôt always be the case for other servers. So, if you are running within a Linux environment (and not otherwise), you can add apptainer with the add command.\n\n\n\nAnd just check that everything worked, check the version of nextflow, get the nf-core help, and run the nextflow Hello World:\npixi run nextflow -version\npixi run nf-core --help\npixi run nextflow run hello\nYour nextflow version should be something like this:\nN E X T F L O W\nversion 25.04.7 build 5955\ncreated 08-09-2025 13:29 UTC (15:29 CEST)\ncite doi:10.1038/nbt.3820\nhttp://nextflow.io",
    "crumbs": [
      "Home",
      "Pipelines",
      "Test nf-core"
    ]
  },
  {
    "objectID": "course_test_nf-core.html#configuration-profile",
    "href": "course_test_nf-core.html#configuration-profile",
    "title": "nfcore: test pipeline",
    "section": "Configuration profile",
    "text": "Configuration profile\nSince we are working on a server with a configuration profile established, but not available via nf-core, you need to download it to your working directory:\nwget https://raw.githubusercontent.com/hpc2n/intro-course/master/exercises/NEXTFLOW/INTERACTIVE/hpc2n.config\nHere is the configuration profile on HPC2N from the above link. The most important things we need to pay attention to are the max_memory, max_cpus, and max_time settings. Your jobs won‚Äôt be able to exceed these maximum values.\nTo use the config file with nextflow you need to add our compute project under project. Use single quotes, as seen in the other entries in the file.\n// Config profile for HPC2N\nparams {\n  config_profile_description = 'Cluster profile for HPC2N'\n  config_profile_contact = 'Pedro Ojeda @pojeda'\n  config_profile_url = 'https://www.hpc2n.umu.se/'\n  project = null\n  clusterOptions = null\n  max_memory = 128.GB\n  max_cpus = 28\n  max_time = 168.h\n  email = 'pedroojeda2011@gmail.com'\n}\n\nsingularity {\n  enabled = true\n}\n\nprocess {\n  executor = 'slurm'\n  clusterOptions = { \"-A $params.project ${params.clusterOptions ?: ''}\" }\n}",
    "crumbs": [
      "Home",
      "Pipelines",
      "Test nf-core"
    ]
  },
  {
    "objectID": "course_test_nf-core.html#run-the-pipeline-with-the-test-profile",
    "href": "course_test_nf-core.html#run-the-pipeline-with-the-test-profile",
    "title": "nfcore: test pipeline",
    "section": "run the pipeline with the test profile",
    "text": "run the pipeline with the test profile\nAs an example, I am running the Sarek pipeline, wich is a variant calling pipeline:\npixi run nextflow run nf-core/sarek -profile test --outdir sarek_test -c hpc2n.config\nLet‚Äôs have a look at the components of the above command:\n\npixi run: we are using our pixi environment to run the following commands\nnextflow run: run the following with nextflow\nnf-core/sarek: name/ location of the pipeline\n-profile test: use the test profile for this run - this uses the build in test data etc. On servers with a nf-core configuration file you would list the name of the profile here.\n--outdir sarek_test: name and location of the directory for the pipeline output\n-c hpc2n.config: name and location of the configuration file for the server\n\nMy test run finished within 3 and a half minutes:\n\n\n\nsarek run summary\n\n\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nHave a look at the output of the run. Do you understand what happened, and what the output is? Is this pipeline as suitable for your data as you thought?",
    "crumbs": [
      "Home",
      "Pipelines",
      "Test nf-core"
    ]
  },
  {
    "objectID": "before_quarto_and_git.html",
    "href": "before_quarto_and_git.html",
    "title": "Quarto Homepage on Github",
    "section": "",
    "text": "Creating a website in Quarto\nPublish the quarto homepage on GitHub via GitHub Actions."
  },
  {
    "objectID": "before_git_and_github.html#version-control",
    "href": "before_git_and_github.html#version-control",
    "title": "git and GitHub",
    "section": "Version control",
    "text": "Version control\nWe all have worked on data before, done analyses, talked with our PI, changed the analyses, worked a bit more‚Ä¶ and in the end we have something like this:\n\n\n\nWhich one of these is the latest version?\n\n\nVersion control, the practice of tracking and managing changes to files, can help us not descend into chaos. With a version controlled project you always know which file, and even which part of the file, is the most recent, and you can even go back to older versions if you have to.\nVersion control can be used on the local system, where both the version database and the checked out file - the one that is actively being worked on - are on the local computer. Good, but the local computer can be corrupted and then the data is compromised.\n\n\n\nLocal version control\n\n\nVersion control can also be centralized, where the version database is on a central server, and the active file can be checked out from several different computers. This is useful when working from different systems, or when working with collaborators. However, when the central servers is compromised the historical version are lost.\n\n\n\nCentralized version control\n\n\nAt last, version control can be fully distributed, with all versions of the file being on the server and different computers. Each computer checks out the file from its own version database to work on them. The databases are then synchronized between the different computers and the server. One such distributed version control system is git. It can handle everything from small to very large projects and is simple to use. GitHubis a code hosting platform for version control and collaboration, built on git.\n\n\n\nDistributed version control\n\n\nDistributed version control facilitates collaboration with others. Software like git automatically tracks differences in files, and flags conflicts between files.\nAdditionally, GitHub, the code hosting platform based on git that we are using in this course, can be used to maintain uniformity within a working group. The group can develop their own project template that people can use and populate for their own projects.",
    "crumbs": [
      "Home",
      "Preparations",
      "git and GitHub"
    ]
  },
  {
    "objectID": "before_git_and_github.html#git-and-github",
    "href": "before_git_and_github.html#git-and-github",
    "title": "git and GitHub",
    "section": "git and GitHub",
    "text": "git and GitHub\nGit is a version control software that is fully distributed - meaning that each project folder contains the full history of the project. These project folders are also called repositories and can be on several computers, or servers.\nGithub is a code hosting platform that is based on git. Here you can store, track and publish code (and code only, do NOT use github for data!). On Github you can collaborate with colleagues and work on projects together.\n\n\n\n\n\n\nNote\n\n\n\nA repository in git is the .git/ folder inside of your directory. This repository tracks all changes made to files in your project and contains your project history. Usually we refer to the git repository as the local repository.\nA repository in GitHub is where you can store your code, your files, together with their revision history. Repositories can be public or private, and might have several collaborators. Usually we refer to the Github repository as the remote repository.\n\n\nLet‚Äôs have a closer look at how git works:",
    "crumbs": [
      "Home",
      "Preparations",
      "git and GitHub"
    ]
  },
  {
    "objectID": "before_git_and_github.html#git",
    "href": "before_git_and_github.html#git",
    "title": "git and GitHub",
    "section": "git",
    "text": "git\n\nGit has three main states that your files can reside in: modified, staged, and committed:\n\n\n\nModified means that you have changed the file but have not committed it to your database yet.\nStaged means that you have marked a modified file in its current version to go into your next commit snapshot.\nCommitted means that the data is safely stored in your local database.\n\n\n\nsource: git documentation\n\nThis leads to the three main sections of a Git project: the working directory, the staging area, and the Git directory (or repository).\n\n\n\nWorking directory, staging area, and Git directory\n\n\nAnd the basic commands of git:\n\n\n\nBasic git commands\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese basic operations are all done on your local system. You have the entire history of the project on your local disk, and do not need an internet connection to work on your data with git. You can do all your commits on your local computer and later push them to a remote repostitory, like Github.\n\n\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nInstall git, and follow the Software Carpentries tutorial on Version Control with Git, chapters 1 to 13.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure you configure git with your GitHub account e-mail (either the one you signed up with, or the one provided by GitHub to hide your actual e-mail).\nI am using vim as a text editor, so I have never changed the default text editor for git. In the tutorial they give you options to change if you are used to a different editor.",
    "crumbs": [
      "Home",
      "Preparations",
      "git and GitHub"
    ]
  },
  {
    "objectID": "before_quarto.html",
    "href": "before_quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "When analysing data, it is good practice to work as reproducibleas possible. Part of that is not only developing and executing code (for example in a R session), but to save and comment the code. As always, this documentation step is crucial to understand the code, especially when sharing code with others, or getting back to it at a later point in time.\nLiterate programming combines code and documentation in the same document: the documentation is in plain text, and the code is wrapped in so called chunks, that are executable from within the document.\nThese notebooks that allow for literate programming come in different flavors, for example jupyter notebooksand marimo for Python applications, Rmarkdown for R code. Its successor, quarto can be used to integrate a variety of coding languanges. In this course, we will introduce you to quarto.",
    "crumbs": [
      "Home",
      "Preparations",
      "Quarto"
    ]
  },
  {
    "objectID": "before_quarto.html#literate-programming",
    "href": "before_quarto.html#literate-programming",
    "title": "Quarto",
    "section": "",
    "text": "When analysing data, it is good practice to work as reproducibleas possible. Part of that is not only developing and executing code (for example in a R session), but to save and comment the code. As always, this documentation step is crucial to understand the code, especially when sharing code with others, or getting back to it at a later point in time.\nLiterate programming combines code and documentation in the same document: the documentation is in plain text, and the code is wrapped in so called chunks, that are executable from within the document.\nThese notebooks that allow for literate programming come in different flavors, for example jupyter notebooksand marimo for Python applications, Rmarkdown for R code. Its successor, quarto can be used to integrate a variety of coding languanges. In this course, we will introduce you to quarto.",
    "crumbs": [
      "Home",
      "Preparations",
      "Quarto"
    ]
  },
  {
    "objectID": "before_quarto.html#quarto",
    "href": "before_quarto.html#quarto",
    "title": "Quarto",
    "section": "Quarto",
    "text": "Quarto\n\nAn open-source scientific and technical publishing system.\n\nMeaning that you can use your favorite text editor (such as VScode (o; ) to write documents in plain text markdown.\nQuarto chunks, or code cells, are executable from within the document and can be written in a variety of different languages - such as python, R, Julia, bash, Observable, and more.\nYour document can then be rendered into a variety of different output formats: html, pdf, MS Word, Markdown, and more.\nWith this you can make presentations, dashboards, homepages (like this one), reports, books, manuscripts, and more.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nInstall Quarto, and follow the tutorial for VScode.\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has a very good documentation, that you can access on their homepage -&gt; Guide, or by searching the web for ‚ÄúQuarto, feature you are looking for‚Äù.",
    "crumbs": [
      "Home",
      "Preparations",
      "Quarto"
    ]
  },
  {
    "objectID": "before_accounts.html",
    "href": "before_accounts.html",
    "title": "Create a user accounts for the server",
    "section": "",
    "text": "The following procedure takes a few days to get through because of clearance delays, so please start straight away, since every thing else in the course depends on it :)\nWe will run parts of this course on a HighPerformance Computing cluster, HPC2N. The cluster is a local resource in Ume√•, and to use it you need a user account both at the National Academic Infrastructure for Super¬≠computing in Sweden (NAISS), and at HPC2N.",
    "crumbs": [
      "Home",
      "Preparations",
      "Server accounts"
    ]
  },
  {
    "objectID": "before_accounts.html#create-a-user-account-at-suprnaiss",
    "href": "before_accounts.html#create-a-user-account-at-suprnaiss",
    "title": "Create a user accounts for the server",
    "section": "Create a user account at SUPR/NAISS",
    "text": "Create a user account at SUPR/NAISS\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nFill out & submit account request form. Read carefully and follow the provided instructions.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nUse Register with Federated Identity if you are enrolled in a Swedish university.\nIf you are not enrolled in a Swedish university or if the previous option doesn‚Äôt work for you, use instead Register without Federated Identity.\n\n\n\n\nAccept NAISS user agreement\nJust after creating the SUPR account, on the page you are taken to, there will be a section titled ‚ÄúUser Agreement‚Äù at the very top. Just under it is the button named ‚ÄúAccept NAISS User Agreement‚Äù.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nClick it.\n\n\nAlternately, if you do this at a later point. Login to SUPR. At the very top it will say ‚ÄúNAISS User Agreement Pending‚Äù.\n\n\n\n\n\n\nTip\n\n\n\n\nUse With SWAMID if you are enrolled in a Swedish university - this option is much faster!\nIf you are not enrolled in a Swedish university use instead Alternative: Offline via Paper Form. Here you will have to print out, sign and send in the form. This may take up to a week. \n\n\n\nWhen you have a SUPR account you can join our course project on HPC2N:",
    "crumbs": [
      "Home",
      "Preparations",
      "Server accounts"
    ]
  },
  {
    "objectID": "before_accounts.html#join-the-course-project-at-hpc2n",
    "href": "before_accounts.html#join-the-course-project-at-hpc2n",
    "title": "Create a user accounts for the server",
    "section": "Join the course project at HPC2N",
    "text": "Join the course project at HPC2N\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLog in to SUPR.\n\nClick ‚ÄúProjects‚Äù in the left side column.\nUnder ‚ÄúRequesting Membership in Projects‚Äù, put in my name, since I am the PI of the project you wish to join. Click ‚ÄúSearch for Project‚Äù.\nClick ‚ÄúRequest‚Äù on the MedBioInfo project.\nWhen I have accepted your membership in the project, go to SUPR again to apply for an account at HPC2N. You will receive an email when your application for membership in a project has been accepted.",
    "crumbs": [
      "Home",
      "Preparations",
      "Server accounts"
    ]
  },
  {
    "objectID": "before_accounts.html#create-a-user-account-at-hpc2n",
    "href": "before_accounts.html#create-a-user-account-at-hpc2n",
    "title": "Create a user accounts for the server",
    "section": "Create a user account at HPC2N",
    "text": "Create a user account at HPC2N\nNote: you must be a member of a project before you do this!\n\n\n\n\n\n\nImportantTo do for you\n\n\n\n\nLogin to SUPR.\nClick ‚ÄúAccounts‚Äù in the left side column.\nYou can request an account at HPC2N now. Look under the heading ‚ÄúAccount Requests‚Äù.\nClick ‚ÄúRequest account‚Äù.\nYour information will then be sent to HPC2N, and you will be taken back to a webpage where you can choose your username.\nUser accounts are usually created once a week. You will get an email from HPC2N when your account has been created.",
    "crumbs": [
      "Home",
      "Preparations",
      "Server accounts"
    ]
  },
  {
    "objectID": "before_accounts.html#questions",
    "href": "before_accounts.html#questions",
    "title": "Create a user accounts for the server",
    "section": "Questions?",
    "text": "Questions?\nContact me via Canvas!",
    "crumbs": [
      "Home",
      "Preparations",
      "Server accounts"
    ]
  },
  {
    "objectID": "course_nf-core.html",
    "href": "course_nf-core.html",
    "title": "nfcore introduction",
    "section": "",
    "text": "source: nf-core\nNf-core is a very active community around nextflow. Volunteers develop nextflow pipelines around a variety of bioinformatic data.\nHere are some flagship pipelines that have been developed by the nf-core community (we will have a look at the entire list on the nf-core homepage in a bit):\nAll nf-core pipelines are open source and the source code is available on github. The pipelines are developed by volunteers, who can have a very varied background.\nHowever, nf-core does not only develop pipelines.\nThe community also develops:\nThere is a weekly online helpdesk, and even a podcast.",
    "crumbs": [
      "Home",
      "Pipelines",
      "nf-core"
    ]
  },
  {
    "objectID": "course_nf-core.html#finding-and-evaluating-a-pipeline",
    "href": "course_nf-core.html#finding-and-evaluating-a-pipeline",
    "title": "nfcore introduction",
    "section": "Finding and evaluating a pipeline",
    "text": "Finding and evaluating a pipeline\nFrom the nf-core homepage, we can search for pipelines. We are going to demo the rnaseq pipeline. You can see that there is a lot going on in this pipeline! We will chat more about these things in class rather than including screenshots of everything.\n\n\n\nrnaseq landing page\n\n\nUnder the Usage, you will find a lot of information that describes the pipeline, including input information, the samplesheet.csv (which we will have to set up with our data later).\nUnder the Parameters tab you will find information on all of the things that we will set up in the next step.\nUnder the Output tab, you will find information on the expected output generated from the pipeline. This is useful to help you interpret what the pipeline produces.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nLook through the available pipelines and see if there is one that is interesting for you and your project.\nIn the next part we will set up a nf-core pipeline with the test profile, so then you can test the pipeline that is most interesting for you.",
    "crumbs": [
      "Home",
      "Pipelines",
      "nf-core"
    ]
  },
  {
    "objectID": "course_clean_up.html",
    "href": "course_clean_up.html",
    "title": "clean-up & finishing off",
    "section": "",
    "text": "Now you have time to finish up some of the things you did not have time for during the course.\n\nIs there another post you would like to write?\nSome Quarto thing you saw in our material/ in a fellow students homepage you‚Äôd like to try out?\nA tool you‚Äôd like to test with Pixi?\n\nOnce you are finished, please fill in the end-of-course feedback questionnaire, and send me the URL of your blog and GitHub repository (via Canvas).\nThanks for attending this course, I am looking forward to your feedback!"
  },
  {
    "objectID": "before_server.html",
    "href": "before_server.html",
    "title": "Connect to the server",
    "section": "",
    "text": "For this course, we will be working on a remote server, HPC2N. You should already have applied for a user account there. If you haven‚Äôt, do it now.\nHere, you will access the server for the first time. You will need a terminal, your user name and the temporary password you will have gotten from HPC2N.",
    "crumbs": [
      "Home",
      "Preparations",
      "Server access"
    ]
  },
  {
    "objectID": "before_server.html#access-server-via-vscode",
    "href": "before_server.html#access-server-via-vscode",
    "title": "Connect to the server",
    "section": "Access server via VScode",
    "text": "Access server via VScode\nWe can use VScode to connect to the server. For this, we need to install the remote SSH extension from within VScode.\n\n\n\n\n\n\nTip\n\n\n\nIn the side bar, click on the extensions symbol (looks like building blocks), and search for remote SSH. Click then on install in the lower right corner.\n\n\n\nadd new SSH host\n\nOpen the Command Palette (View -&gt; Command Palette).\nType Remote-SSH and select Remote-SSH: Add New SSH Host.\nType ssh username@kebnekaise.hpc2n.umu.se, where you substitute ‚Äúusername‚Äù with your HPC2N user name.\nIf VScode cannot automatically detect the trype of server you need to select Linux.\nYou will be asked to pick a SSH configuration file to update. Choose the default (top of the list .ssh/config).\n\n\n\nconnect to server\n\nType Remote-SSH and select Remote-SSH: Connect to Host.\nSelect the HPC2N server.\ntype in your password.\n\nAfter a bit of setting up you will see in the bottom left corner the verification that you are connected to the server:\n\n\n\nConnected!\n\n\n\n\n\n\n\n\nCautionAfter the first log-in: reset password\n\n\n\nIn the welcome mail you got when your HPC2N account was created there was a link to create a first, temporary password. When you have logged in using that, you need to change your password.\nFrom the HPC2N documentation:\nThis is done using the passwd command:\npasswd\nUse a good password that combines letters of different case. Do not use dictionary words. Avoid using the same password that you also use in other places.\n\nIt will first ask for your current password. The first time you login, that will be the temporary password you created with the HPC2N password reset service.\nType in that and press enter.\nThen type in the new password you want, enter, and repeat.\nYou have changed the password.\n\n\n\nYou are now on connected to the login node of the cluster.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nRead here more about HPC cluster architecture. Read until the header MPI.\n\n\nThis location is the home folder of your account.\nOpen up the terminal in VScode (View -&gt; Terminal), and you are ready for the next chapter, learning (or repeating) the basics of the command line.",
    "crumbs": [
      "Home",
      "Preparations",
      "Server access"
    ]
  },
  {
    "objectID": "course_publish_blog.html",
    "href": "course_publish_blog.html",
    "title": "Publish your blog",
    "section": "",
    "text": "To publish your blog we need to first start tracking it with git:",
    "crumbs": [
      "Home",
      "Version control & documentation",
      "GitHub actions"
    ]
  },
  {
    "objectID": "course_publish_blog.html#tracking-your-blog-with-git",
    "href": "course_publish_blog.html#tracking-your-blog-with-git",
    "title": "Publish your blog",
    "section": "Tracking your blog with git",
    "text": "Tracking your blog with git\nIn the terminal, make sure you are in the directory of your blog, then type:\ngit init\nThis will initialize your blog directory as a git repository.\n\nInitialized empty Git repository in path/to/directory\n\nNow, if you check with\ngit status\nyou will see that the repository is indeed empty. You will also see some untracked files.\n\n\n\nTerminal after checking the git status\n\n\nAdd all the files to the repository and commit the changes to git.",
    "crumbs": [
      "Home",
      "Version control & documentation",
      "GitHub actions"
    ]
  },
  {
    "objectID": "course_publish_blog.html#add-the-blog-to-github",
    "href": "course_publish_blog.html#add-the-blog-to-github",
    "title": "Publish your blog",
    "section": "add the blog to GitHub",
    "text": "add the blog to GitHub\nNow we need to add the local repository to GitHub. Log into GitHub on the homepage. Then add a new repository, give it a name and make it public. Do not add anything to it, not .gitignore, README, license etc. \nOnce initialized, GitHub will give you several suggestions of what to do with your new repository. Follow the instructions of the option: ...or push an existing repository from the command line\nBack in VScode, check in the terminal that you are in the correct directory (the one of your blog).\nYou can then add the remote origin etc with the commands given on GitHub:\ngit remote add origin &lt;specifics of your repository&gt;\ngit branch -M main\ngit push -u origin main\nCheck on Github if the content of your blog has been added. You might have to refresh your page.",
    "crumbs": [
      "Home",
      "Version control & documentation",
      "GitHub actions"
    ]
  },
  {
    "objectID": "course_publish_blog.html#publish-via-github-actions",
    "href": "course_publish_blog.html#publish-via-github-actions",
    "title": "Publish your blog",
    "section": "publish via GitHub actions",
    "text": "publish via GitHub actions\nNow that you have a local git repository that is synced to GitHub you can publish your blog using GitHub Pages, a website hosting service.\nThe default URL of your blog will be a combination of your GitHub user name and the name of the repository. If you have your own domain you can also use that at a later point in time.\nThe steps for publishing your blog with GitHub Actions are here. Follow the instructions until the section Executing code.\nNow, the publish command will be triggered automatically every time you push changes to the remote repository.",
    "crumbs": [
      "Home",
      "Version control & documentation",
      "GitHub actions"
    ]
  },
  {
    "objectID": "course_publish_blog.html#tips-and-tricks",
    "href": "course_publish_blog.html#tips-and-tricks",
    "title": "Publish your blog",
    "section": "Tips and tricks",
    "text": "Tips and tricks\nWhen working on the blog, use git (adding, committing, pushing) often. Try to keep commits to one topic - maybe one blog entry per time. This way it is easier to label the commits and backtrack what has happened in the repository.\nDo not add large files to the repository. GitHub is a code hosting platform, not a data hosting platform. If you have data in your local directory you can exlude it from the repository with the .gitignore file.",
    "crumbs": [
      "Home",
      "Version control & documentation",
      "GitHub actions"
    ]
  },
  {
    "objectID": "before_VScode.html",
    "href": "before_VScode.html",
    "title": "Visual Studio Code",
    "section": "",
    "text": "During the pre-course assignements, and the course itself, we will be using Visual Studio Code (VScode). It is available for free and runs on all major platforms. Extensions make it very versatile, and we will be using it to edit and render text, to us version control, and to connect to and work on our course server.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nInstall VScode using the instructions here.\n\n\nOnce you have installed VScode and open it, you will see the starting screen. Here is my starting screen: \nYours will look slightly different, especially in the activity bar all the way on the left, because I have already installed extensions when working with VScode. So don‚Äôt worry about the exact look.\n\n\n\n\n\n\nImportantTo do for you\n\n\n\nTo get aquainted with some of the features of VScode, please watch this 7-minute video from the Visual Studio homepage. Follow along the tutorial (as far as you can, he‚Äôs not providing the last script he‚Äôs showcasing).\nRead up more on the graphical user interface of VScode here.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you can‚Äôt see the terminal on the lower part of the screen, you can open it by clicking on the button on the upper right of the screen.\n\n\n\nClick here to open the terminal",
    "crumbs": [
      "Home",
      "Preparations",
      "Visual Studio Code"
    ]
  }
]